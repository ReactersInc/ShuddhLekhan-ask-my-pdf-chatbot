[
  {
    "section": "Unknown",
    "text": "Using Container Migration for HPC Workloads\n"
  },
  {
    "section": "Resilience",
    "text": "Mohamad Sindi\nCenter for Computational Engineering\nMassachusetts Institute of Technology\nCambridge, MA 02139, USA\nsindimo@mit.edu\nJohn R. Williams\nDepartment of Civil and Environmental Engineering\nMassachusetts Institute of Technology\nCambridge, MA 02139, USA\njrw@mit.edu\nAbstract—We share experiences in implementing a container-\nbased HPC environment that could help sustain running HPC\nworkloads on clusters. By running workloads inside containers, we\nare able to migrate them from cluster nodes anticipating hardware\nproblems, to healthy nodes while the workloads are running.\nMigration is done using the CRIU tool with no application\nmodification. No major interruption or overhead is introduced to\nthe workload. Various real HPC applications are tested. Tests are\ndone with different hardware node specs, network interconnects,\nand MPI implementations. We also benchmark the applications\non containers and compare performance to native. Results\ndemonstrate successful migration of HPC workloads inside\ncontainers with minimal interruption, while maintaining the\nintegrity of the results produced. We provide several YouTube\nvideos demonstrating the migration tests. Benchmarks also show\nthat application performance on containers is close to native. We\ndiscuss some of the challenges faced during implementation and\nsolutions adopted. To the best of our knowledge, we believe this\nwork is the first to demonstrate successful migration of real MPI-\nbased HPC workloads using CRIU and containers.\nKeywords—HPC fault tolerance, container migration, container\nperformance benchmarks, MPI\n"
  },
  {
    "section": "I. Introduction",
    "text": "According to the November 2018 world’s top 500\nsupercomputers list, 88.4% of the world’s top high performance\ncomputing (HPC) systems are based on commodity hardware\nclusters [1]. Such systems are typically designed for\nperformance rather than reliability. The Mean Time Between\nFailures (MTBF) for some of these top Petaflop systems has\nbeen reported to be several days [2], while studies estimate the\nMTBF for future Exascale systems to be less than 60 minutes\n[3]. Hence, running sustainable workloads on such systems is\nbecoming more challenging as HPC systems grow.\nIn this paper, we share experiences in implementing and\ndemonstrating a Linux container-based environment, which\ncould improve the resilience of HPC workloads running on\nclusters. The environment is meant to help sustain running HPC\nworkloads on the clusters in the event of anticipated node\nhardware issues that are common in HPC environments (e.g.\nECC memory errors, etc.). The prediction or identification of\nsuch issues and silent errors is an area that has been previously\ninvestigated in the HPC domain and is not the scope of this\npaper, however it is addressed in another paper we currently\nhave under publication review. The work presented in this paper\non the other hand is a proposed remedy method to sustain\nrunning workloads once such issues are identified on the HPC\nsystem.\nThe method used to design our environment is based on\nrunning the HPC workloads inside Linux containers. The\ncontainer technology has proven its success in the micro services\ndomain (e.g. webservers) as a scalable and lightweight\ntechnology. In our work, we adapt the container technology\ntowards HPC workloads to make use of its migration\ncapabilities, which can be thought of as a fault tolerance\nmechanism. By running inside containers, we are capable of\nmigrating the HPC workloads from nodes anticipating hardware\nproblems, to healthy spare nodes while the workload is running.\nMigration is performed using the Checkpoint-Restore in\nUserspace (CRIU) tool [4] with no application modification.\nThe container environment does not introduce any major\ninterruption or performance overhead to the running workload.\nWe also provide application benchmark results comparing the\nperformance of the container environment to the native system.\nThe container environment is tested with various real HPC\napplications that are based on the Message Passing Interface\n(MPI) standard [5]. The applications tested come from both\nacademia (open source) and the industry (closed source), and are\nwritten in various programming languages (e.g. C++, C, and\nFortran). Verifying the integrity of the results produced by the\nmigrated workloads is also addressed in this study. We test the\napplications with different hardware node types as well as\ndifferent network interconnect types (e.g. 1, 10, and 25 Gigabit\nEthernet networks). The applications are tested with various\nnumber of processing cores ranging from 4 and up to 144 cores.\nIn addition, we test the migration with three commonly used\nMPI implementations, MPICH [6], Open MPI [7], and Intel MPI\n[8].\nOur results demonstrate that we can successfully migrate\nreal HPC workloads running inside containers from one physical\nmachine to another with minimal interruption to the workload.\nThe results produced from runs with and without a migration are\nidentical with no data corruption. Our benchmark results also\nshow that running these HPC applications inside containers\nprovides a performance almost identical to the native system.\n978-1-7281-5020-8/19/$31.00 ©2019 IEEE\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:01:19 UTC from IEEE Xplore.  Restrictions appl\nIn this paper, we make the following contributions:\n•\nThe use of containers in the HPC domain as a fault\ntolerance enabling mechanism is a field that has not been\nthoroughly explored yet. To the best of our knowledge,\nwe believe this work is the first in this domain to do\nsuccessful migration of MPI-based real HPC workloads\nusing CRIU and containers.\n•\nWe share several narrated YouTube videos to\ndemonstrate the container migration of the various HPC\nworkloads tested. This includes standard computational\nworkloads, as well as workloads that produce in-situ\nvisualization during the migration. We believe such\nvideo demonstration is also a first.\n•\nRelated work benchmarking containers typically use\ngeneric HPC benchmarks (e.g. HPL [9] and NPB [10]).\nThis work benchmarks various real HPC applications.\nThe benchmarks compare the performance of containers\nto native system and quantifies the overhead involved.\n•\nAs the use of containers in HPC is a young topic, we\ndiscuss some of the challenges faced during the study\nand share the solutions adopted to overcome them.\n"
  },
  {
    "section": "Ii. Related Work",
    "text": "Earlier studies investigated using virtual machine (VM)\ntechnologies to migrate workloads as a proactive fault tolerance\nmeasure [11], [12]. However, traditional VMs can introduce an\noverhead due to the virtual emulation of devices through a\nhypervisor. Earlier work showed that network latency with VMs\ncan be more than twice in some cases compared to native [13],\n[14]. Nevertheless, VMs have been evolving with efforts to\nimprove their performance using I/O bypass methods, such as\nthe\nsingle\nroot\ninput/output\nvirtualization\n(SR-IOV)\nspecification [15]–[19]. However, such methods are limited to\nhaving a hardware/software setup that supports such features\n[20]. The performance with such methods is also still not\ncompetitive yet compared to native performance. Bypass\nmethods may also affect the capability of VM migration. For\nexample, the current Red Hat Enterprise Linux (RHEL) 7\ndocumentation states that VM migration is not supported when\nSR-IOV is enabled [21].\nOther studies looked into doing process-level migration\nwhere the actual MPI processes are targeted for migration. Study\n[22] attempted to use CRIU to perform such migration, however\nthey were not successful in migrating parallel MPI processes and\nwere able to only migrate a serial version of their application.\nAnother tool targeted to checkpoint and migrate MPI processes\nis the Distributed MultiThreaded CheckPointing (DMTCP)\npackage [23]. However, it requires the application binaries to be\nstarted through a proxy launcher, which may add an overhead\nfor the application. In addition, it does not work with graphical\napplications using X-Windows extensions like OpenGL.\nThe current emerging alternative to VMs is the container\ntechnology. Compared to the well-explored domain of VM\nmigration, container migration is a young topic that has not been\nextensively explored yet, especially in the scope of HPC. A few\nstudies however have attempted container migration in Linux\nenvironments. Study [24] implemented a prototype libvirt-based\ncustom Linux driver to enable container migration using CRIU.\nHowever, the study did not test migration with real applications,\nbut rather with an artificially induced memory load on the\nsystem. Another study used a nonstandard tool called ReVirt\n[25] to migrate Docker containers using a logging/replay\nmethod [26]. The migrated application however was a\nstandalone web application and not HPC related. A recent study\nlooked into migrating containers using CRIU [27]. The migrated\ncontainer did not have a real application in it either, but a simple\nserial Linux tool memhog [28], which just allocates memory on\na system. Several studies have also compared the performance\nof containers to those of VMs and native systems [29]–[35].\nHowever, most of these studies used generic benchmarks such\nas HPL or NPB, which are still valuable tests, but not truly\nreflective of real HPC applications.\n"
  },
  {
    "section": "III. SYSTEM DESIGN OF CONTAINER ENVIRONMENT",
    "text": "A. Cluster Setup\nOur cluster test environment was setup using the Amazon\nWeb Services (AWS) infrastructure [36]. We believe that our\ncontainer environment setup should also work with traditional\nLinux clusters that are not AWS based. By using AWS however,\nit gave us the option to setup test clusters with various specs. For\nour testing, we chose four types of node instances that varied in\nterms of specs [37]. Lower spec nodes were of type m4.2xlarge\n(4 physical cores each, 32 GB RAM, 1 Gig network). Medium\nspec nodes were of type c5.9xlarge (18 physical cores each, 72\nGB RAM, 10 Gig network). As for the higher spec nodes, we\ntested using two types, m4.16xlarge (32 physical cores each, 256\nGB RAM, 25 Gig network) and i3.metal (36 physical cores, 512\nGB RAM, 25 Gig network). The i3.metal instances are AWS’s\nnew bare metal instances.\nAll instances access a shared network storage using\nAmazon’s Elastic File System (EFS) [38]. The shared storage\nvolume is mounted on the instances through the Network File\nSystem versions 4.1 (NFSv4.1) protocol [39]. This shared\nstorage holds all the input and output data for the HPC\napplications.\nB. Software Setup\nIn this section, we will discuss the software setup of our\nenvironment in terms of the operating system and container\ntechnology used, as well as the system tools used for launching\nand maintaining the containers on the system.\nThe container technology has been gaining a lot of attention\nin the past few years due to its low overhead and near native\nperformance. Containers are more lightweight on the system\ncompared to traditional VM technologies, as they do not rely on\na hypervisor to access the underlying system resources. The\ncontainer technology also provides a convenient way to package\nan application and its dependencies for ease of deployment.\nThere are several implementations of containers. For this\nwork, we chose to use the OpenVZ containers [40]. OpenVZ\nprovided out of the box container features such as assigning IP\naddresses to containers, secure shell (ssh) connections, and NFS\nshared file system mounting, which are all crucial features to any\nHPC environment. In addition, we use the CRIU tool to facilitate\nthe container migrations. CRIU is an open source project that\n978-1-7281-5020-8/19/$31.00 ©2019 IEEE\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:01:19 UTC from IEEE Xplore.  Restrictions appl\nprovides a checkpoint and restart functionality for Linux based\napplications and processes. The tool can be used to capture the\nstates of CPU, memory, disk, and network of a process or a\nhierarchy of processes. Such functionality can also be applied to\ncontainers as they are considered Linux processes. We also use\nParallels’ “prlctl” command line tool to provision and manage\nthe containers [41]. In addition, we installed the AWS Command\nLine Interface (CLI) package [42], which eased the management\nof the instances’ networking part.\nThe OpenVZ version that we targeted for testing was 7.0.7.\nThe kernel provided by OpenVZ is a variation based on Red Hat\nEnterprise Linux (RHEL) 7.4 kernel 3.10.0-693. The OpenVZ\nvariation however has patches that support the container\nfunctionally and its management. To our understanding, the\npatches applied to the kernel are currently not mainstream in the\nLinux kernel. The CRIU version used was version 3.4.0.41-1,\nwhile the “prlctl” container management tool used was version\n7.0.148-1. The AWS CLI package used was version 1.15.61.\nIt is also important to note that the only officially supported\nmethod to install OpenVZ is through their ISO installation\nimage. This created a challenge to us as our testing environment\nis setup on the AWS platform and there was no official way to\ninstall an instance with an ISO image. Having talked with the\nOpenVZ community, it was brought to our attention that there is\na commercially supported version of OpenVZ, called Virtuozzo,\nwhich AWS currently provides instances for. Due to the\nlimitation of not being able to install via ISO image on AWS,\nwe ended up setting our environment using the equivalent\nVirtuozzo 7.0.7 image provided by AWS. Technically, the\nkernel and software versions of CRIU and the “prlctl” container\nmanagement tool for both distributions appeared identical to us.\n"
  },
  {
    "section": "C. Container Setup",
    "text": "HPC applications typically would fully utilize the\nunderlying compute node they are running on. With this in mind,\nour setup consisted of launching a single container on each node,\nwith full access to the underlying resources. By default, the\nlaunched containers do not have access restrictions in terms of\nhow many processing cores to use from the underlying machine.\nHowever when it comes to memory, we had to specify the\namount of memory we wanted to allocate for the container. We\nset the containers to use the majority of the memory available,\nwhile leaving a sufficient amount of memory for the underlying\noperating system just as a precaution measure. For example, on\nthe i3.metal instances, the containers were launched with 500\nGB of RAM out of the 512 GB available.\nWe also wrote scripts to automate creating and launching the\ncontainers with specific settings using the “prlctl” tool. This\nincluded setting the container’s operating system template,\nname, IP address, users, DNS, NFS mounts, ssh keys, default\nbash shell environment variables, default bash shell limits, and\nthe netfilter firewall settings. The container template used was\nthe CentOS 7 template. In addition, we used the “vzpkg” tool\n[43] to automate installing any extra packages we needed inside\nthe container (e.g. MPI libraries, third party packages needed by\nthe HPC applications being tested, etc.). For each of the HPC\napplications we were testing, a separate container installation\nimage was created. Each image only included the software\npackages and application binaries needed for that HPC\napplication. This kept the containers minimal in size.\nD. Network Setup\nFor every instance in the cluster, we allocated four IP\naddresses. The native instance was assigned 2 IPs, one is public\nand one is private. The container running on the instance was\nalso assigned 2 IPs, one is public and one is private. The public\nIPs are used to access the native instance and the container by\nthe ssh protocol from any remote terminal. The private IPs are\nused for local communication between the instances or\ncontainers. The IPs of the native system are associated with the\ninstance’s primary network interface, while the IPs of the\ncontainer are associated with the instance’s secondary network\ninterface. By having this setting, the IPs of the containers can be\ntreated as floating IP addresses, meaning that if a container is\nmigrated from one physical machine to another, then its IP\naddress can also be migrated along with it. The IPs were set to\nbe static as it was more convenient when doing our\nbenchmarking and container migration testing. The details of\nhow the containers’ IPs were setup can be found in this reference\n[44].\n"
  },
  {
    "section": "IV. HPC APPLICATIONS TO TEST",
    "text": "For testing, we chose six HPC applications that are all MPI\nbased and cover various domains. The applications tested were\nthe Ohio State University (OSU) Micro-Benchmarks version\n5.3.2 [45], Palabos version v2.0r0 [46], Flow version 2018.04-0\n[47], Fluidity version 4.1.15 [48], GalaxSee version MPI 0.9\n[49], and the ECLIPSE* industry-reference reservoir simulator\nby Schlumberger version 2017.2 [50]. Table 1 summarizes these\napplications.\nTABLE 1. SUMMARY OF TESTED HPC APPLICATIONS\n"
  },
  {
    "section": "V. Testing And Results",
    "text": "A. Performance Benchmarks on Containers\nThe first type of tests conducted was to benchmark the HPC\napplications on the container environment and compare the\nperformance to the native system. A native system is a cluster of\ninstances that have no containers running on them. A container-\nbased environment would be the same cluster of instances,\nhowever on each instance we launch a single container that\nencapsulates the entire underlying native system. The native\ninstances and the containers have their own distinct IP\naddresses. The mpirun launcher is then used to start the MPI\njobs. The MPI hostfile will have the appropriate IP addresses of\n978-1-7281-5020-8/19/$31.00 ©2019 IEEE\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:01:19 UTC from IEEE Xplore.  Restrictions appl\neither the native instances or the containers. We record the\nelapsed time of execution for both native and container runs.\nEach benchmark is ran four times and timings are averaged. We\nalso calculate the overhead involved with containers. All\nbenchmarks were performed using the MPICH MPI library\nversion 3.0-3.0.4-10, except for the ECLIPSE simulator where\nwe used the Intel MPI library version 5.0.2.044. Intel MPI was\nthe supported library for the closed-source binary executable of\nthe ECLIPSE simulator.\nThe first application tested was the OSU Micro-\nBenchmarks. This application is the only one out of the six we\ntested that is not a real HPC application, but rather a network\nperformance benchmark for typical MPI point-to-point and\ncollective operations. It allowed us to compare that container’s\nnetwork performance to that of the native system in terms of\nlatency and bandwidth. For the point-to-point benchmarks, we\ntested osu_latency and osu_bw. For the collective benchmarks,\nwe tested osu_allgather and osu_allreduce, which are two\ncommonly used collective MPI operations in HPC applications.\nEach benchmark was performed on a pair of instances of the\nsame type. In the case of the point-to-point benchmarks, one\nprocessor from each instance participated in the benchmark,\nwhile for the collective benchmarks all available processing\ncores on each instance were used. As our instances vary in\nnetwork specs, the tests were performed on all three network\ntypes available, which included 1, 10, and 25 Gigabit networks.\nFig. 1 shows the point-to-point benchmarks. Fig. 2 shows the\ncollective benchmarks.\nIn the case of the 1 Gigabit network, the bandwidth of the\ncontainer and native system were almost identical. With the 10\nGigabit network, we noticed a slight bandwidth reduction on the\ncontainers, while on the 25 Gigabit network we saw a more\nnoticeable reduction in bandwidth. Nevertheless, the overall\naverage bandwidth overhead was around 3.9 % with containers.\nAs for latency, there was a slight overhead with containers for\nmost of the tests. Overall, the average latency overhead was\naround 6.8 % when using the container environment. We do not\nfully understand why the 25 Gigabit network behaved a bit\ndifferently than the others, but we believe this might be related\nto the need for more appropriate network tuning for 25 Gigabit\nnetworks from the Linux side (e.g. kernel parameters, etc.).\nSuch slight overhead with containers was to be expected. We\nbelieve that the difference in performance was due to the way\nthe network routing is done. The container’s network is run in a\nhost-routed mode. In this case, the native node acts as a router\nto the packets passing through the container. Hence, the\ncontainer’s traffic will have to go through that one extra step of\nrouting, which contributes to the slight overhead observed.\nNext, we proceeded with testing the remaining HPC\napplications. The benchmarks were performed on the various\ninstance types with MPI job sizes ranging from running on 4\nprocessing cores, and up to 144 cores. The input models we used\nfor each application varied depending on the number of\nprocessing cores we were running on. For example, when testing\nthe Fluidity application, we used a moderately sized benchmark\nmodel “tephra_settling” when running on 4 cores, however\nwhen benchmarking on 144 cores, we used a significantly larger\nmodel “tides_in_the_Mediterranean_Sea”. Also note that for the\nECLIPSE simulator and Flow applications, we were only able\nto test on up to 8 processing cores. For the ECLIPSE simulator,\nthe commercial license we had was limited to only 8 parallel\nprocesses. For Flow, the test models provided with the\napplication were limited in size (e.g. Norne model was the\nlargest) and problem sizes were too small to be run on more than\n8 cores. Table 2 summarizes all input models used.\nTABLE 2. INPUT MODELS USED FOR APPLICATION TESTING\nFig. 3 shows the average elapsed times for the benchmarks\ncomparing the performance of container to native. Even though\nwe previously saw a slight overhead in latency and bandwidth\non containers with the OSU Micro-Benchmarks, the overhead\nwhen testing with real HPC application was very negligible and\nperformance was close to native (0.034% average performance\noverhead). In the worst case, the maximum overhead observed\nwas around 0.9%. Overall, the performance on the container\nenvironment was acceptable for all applications.\nB. Testing Container Migration\na)  Migration Mechanism: Our goal was to launch a\ndistributed MPI application inside containers that are hosted on\nseveral native machines, then try to successfully migrate one of\nthe containers from one native machine to another native\nmachine (i.e. spare machine) while the MPI job was running.\nFig. 4 gives an overview of the task we were trying to\naccomplish. We used the “prlctl” tool as our interface to control\nthe states of the container. The “prlctl” tool uses the CRIU\nlibrary to manipulate the container’s state. The AWS CLI tool\nis also used to migrate the container’s floating IP address\nbetween machines.\n978-1-7281-5020-8/19/$31.00 ©2019 IEEE\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:01:19 UTC from IEEE Xplore.  Restrictions appl\nFig. 1. Point-to-Point MPI benchmarks\nFig. 2. Collective MPI benchmarks\n978-1-7281-5020-8/19/$31.00 ©2019 IEEE\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:01:19 UTC from IEEE Xplore.  Restrictions appl\nFig. 3. Application benchmarks on various instance types\nFig. 4. Overview of container migration test\nOverall, the migration steps can be described as follows.\nFirst, the container is put in a “suspend” state as MPI processes\nare running inside of it. This will cause the MPI job to freeze\ntemporarily. The MPI processes on the other participating nodes\nwill still be alive, but waiting for the suspended container to\ncome back. As part of the suspension process, CRIU takes a\ndump of the container’s state and stores it locally. Next, the\ncontainer’s floating IP address gets migrated to the spare node.\nWe automated the IP migration by writing a custom script\ninfluenced by Sabat’s [51], which utilizes the AWS CLI tool for\nautomation. After that, the container is put in a “migrate” state,\nwhich will copy the dumped container’s state to the spare\nmachine. Finally, the container is put in a “resume” state on the\nspare machine. At this point, the MPI job resumes from the same\nstate before it froze and progresses. The migration and resuming\nis done as user root. We automate the entire migration process\nwith a custom script. Fig. 5 summarizes the migration steps.\nb)  Migration Time:  We also observed the time it takes\nto complete the container migration for the various applications\ntested. The migration time was influenced by the size of\napplication binaries and dependent libraries stored inside\ncontainer. In the case of Palabos, GalaxSee, and the ECLIPSE\nsimulator, those files were stored on a shared NFS storage and\nnot inside container, hence making the container’s size\nrelatively smaller. In the case of Fluidity and Flow, the\ninstallation of the applications was performed using RPM\npackages. This method ends up installing the application\nbinaries and their dependencies inside the container, thus\nincreasing the size of the container.\n978-1-7281-5020-8/19/$31.00 ©2019 IEEE\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:01:19 UTC from IEEE Xplore.  Restrictions appl\nWe also noticed that the migration time did not change much\neither we were using the 1, 10, or 25 Gigabit network. It seemed\nto us later that the bottleneck was not the network speed, but\nrather the read/write speeds of the local disks storing the\ncontainer’s dumped data. The instances by default use a General\nPurpose SSD (gpt2) local disk that has a limited throughput.\nAWS also provides higher throughput disks called Provisioned\nIOPS SSD (io1). We tested migration with both disks and the\naverage migration time improved by around 35% when using\nthe io1 disks. The average migration time with the gpt2 disks\nwas 34 seconds, while with the io1 disks it was 22 seconds.\nTable 3 shows the average container migration times for the\ndifferent applications using the two types of disks.\nTABLE 3. AVERAGE CONTAINER MIGRATION TIMES\nC. Integrity Check for Produced Results\nAs part of the container migration testing, it was crucial to\ncheck the integrity of the results produced and make sure that\nthe migration was not causing any data corruption. The different\nHPC applications tested produced several types of output files.\nFluidity produces binary files (e.g. .vtu and .pvtu files) for post\nprocessing with ParaView [52]. Palabos also produces binary\nfiles for post processing with ParaView (e.g. .vtk files), as well\nas GIF image files. The ECLIPSE simulator and Flow produce\ntext files (e.g. .log and .prt files), as well as binary output files\nfor post processing (e.g. .unsmry and .egrid files). GalaxSee\ndoes not generate output files, however it produces an in-situ\nvisualization.\nWe run each application with and without migration and do\na comparison of the results using the relevant post processing\ntools, and also compare output text files. Fig. 6 is an example\ndemonstrating the data integrity comparison check for a Palabos\nsimulation of the Rayleigh Taylor instability. The container\nmigration was triggered randomly at time step 2400 during the\nrun. We compare results produced with and without container\nmigration at time step 2400, and then at time step 3000 after the\nmigration completed and the simulation has progressed, and\nlastly at the final time step. Both results were the same. None of\napplications tested had any discrepancy issues with the results.\nFig. 6. Results integrity check for Palabos simulation\nD. Container Demo Videos\nIn this section, we share several YouTube videos\ndemonstrating the successful container migration for some of\nthe HPC applications we tested. We describe the individual tests\nand reference the links for the videos. The videos are visually\nnarrated to have a better understanding of the test scenarios.\nThe test environment is the same as illustrated in Fig. 4.\nDuring the test, the Linux “top” command is run inside each\ncontainer to monitor the state of the MPI processes. On the\nnative machines, we use the Linux “wall” command to run the\n“prlctl” tool every second to list the containers’ states. This helps\nvisually catch the transition states of the containers during\nmigration. All demos were done using instances with gpt2 disks.\nTable 4 references the links for the demo videos.\nThe first video demonstrates the container migration for\nPalabos [53]. The application is launched on two containers with\ntwo MPI processes on each. The run is for a 2D simulation of\nthe Rayleigh Taylor instability. It shows a 2D in-situ\nvisualization of the results as it runs. Container “sindi_ct1” is\nthen migrated at a random time step 2400. During migration, the\nFig. 5. Container migration steps\n978-1-7281-5020-8/19/$31.00 ©2019 IEEE\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:01:19 UTC from IEEE Xplore.  Restrictions appl\nstate of the container changes from “running” to “suspended”\nand the MPI job freeze temporarily. Once the container is\nmigrated to the spare machine, the state of the container changes\nback to “running” and the application’s time stepping and 2D\nvisualization resumes.\nThe second video demonstrates the container migration for\nthe ECLIPSE simulator [54]. The job is launched on two\ncontainers with four MPI processes each. The simulation is run\nusing the “PARALLEL_15000CELLS” model. We were\ncurious how the job behaved if the container was migrated more\nthan once. Hence, we migrate it back and forth between two\nnative machines. The job progresses successfully during both\nmigrations.\nThe third video demonstrates the container migration for\nGalaxSee [55]. It is launched on two containers with four MPI\nprocesses on each. The simulation is done for eight thousand\nstars and an in-situ visualization is displayed. We swap the two\ncontainers, sindi_ct1 and sindi_ct2, simultaneously as the job\nruns. We did this test because we were curious how the native\nmachines would behave during a simultaneous migration.\nDuring the swap, the visualization temporarily freezes then\nresumes. Having two simultaneous migrations increased the\naverage migration time to 38.5 seconds instead of 29 seconds.\nThe last demo is for the Flow application [56]. This test does\nnot involve container migration, but rather further tests the\ncontainer’s suspend and resume capabilities. The job is launched\non two containers with four MPI processes each. Next, we\nsuspend all the containers involved in the job, which causes the\njob to freeze. After that, we power off the two native machines\nhosting the two containers and then power them on again. This\nis done by issuing the “reboot” command on both machines.\nOnce the machines are up again, we resume both containers. The\njob then resumes and progresses. This demo shows that the\ncontainer’s resilience potentials are not only limited to migrating\ncontainers. The ability to suspend an entire MPI job then\nresuming it can be useful for HPC centers, especially in events\nsuch as scheduled system downtime maintenance.\nTABLE 4. VIDEO LINKS FOR CONTAINER DEMOS\nApplication\nDemo Video Link\nPalabos\nhttps://youtu.be/1v73E2Ao3Mk\nECLIPSE\nhttps://youtu.be/5tz6JP2UgTk\nGalaxSee\nhttps://youtu.be/NlT7nJ-yENc\nFlow\nhttps://youtu.be/KNTVHQnMVHU\n"
  },
  {
    "section": "VI. CHALLENGES AND SOLUTIONS",
    "text": "We faced several technical challenges throughout testing\ncontainers in an HPC environment. In this section, we go over\nsome of the challenges encountered and the solutions adopted.\nMPICH, and its derivatives, is considered the most widely\nused implementation of MPI in the world according to its\nwebsite [57]. While testing with MPICH and its derivative Intel\nMPI, we did not have technical issues in terms of launching MPI\njobs on containers, nor when it came to migrating the containers.\nOn the other hand, with Open MPI the launching of an MPI job\ninside the containers was failing. We used Open MPI version\n1.10.7, which was the current during testing. The jobs would fail\nto start with the following error “tcp_peer_send_blocking:\nsend() to > socket 9 failed: Broken pipe”. At the time of this\nstudy, the Open MPI frequently asked questions (FAQ) page\nmentions that Open MPI does not to support virtual IP interfaces\n[58]. OpenVZ uses such interfaces inside the containers (i.e.\nvenet0 and venet0:0). Nevertheless, we used a hack to the Open\nMPI source code to overcome the issue [59]. It is however a\nworkaround with no official support.\nAnother issue encountered was related to having a shared\nNFS storage mounted inside the containers. When attempting to\nmigrate a container with an active NFS mount, the migration\nprocess immediately hanged. This took place in the first step of\nthe migration process when suspending the container. After\ninvestigation, we were able to pinpoint that the issue was related\nto the CRIU library. CRIU was hanging during the execution of\nthe code in one of its files “nfs-ports-allow.sh”. Debugging that\ncode further, we found it was hanging in the \"nfs_server_ports\"\nfunction. That function tries to run the Linux “rpcinfo” tool to\nremotely query open NFS ports on the shared storage system\nmounted inside the containers, which was causing the hanging.\nWe implemented a fix to CRIU’s original code to overcome this\nissue. Instead of relying on the “rpcinfo” tool, we replaced that\ncode with a variable that references the appropriate standard\nNFS port numbers. For example, in our case the storage mounts\nwere using the NFSv4.1 protocol, so the standard corresponding\nport number was port 2049. After applying this fix to CRIU, we\nwere able to migrate the containers.\nThe last problem we had was specific to the ECLIPSE\nsimulator. Initially, the container migration was failing in the\nsuspension stage. Investigating this further, logs where showing\nthe message “remote posix locks on NFS are not supported yet”.\nApparently, CRIU currently has a limitation of not being able to\nsuspend processes having file locks [60]. The ECLIPSE\nsimulator was producing a database file “.dbprtx” with a file\nlock. Release notes of the ECLIPSE simulator showed that this\nfile is optional and that it might cause issues on shared file\nsystems that do not support file locking [61]. Fortunately, there\nwas an option to turn off the generation of this file and we were\nable to migrate the containers afterwards.\n"
  },
  {
    "section": "Vii. Conclusion And Future Work",
    "text": "In this paper we presented our experiences in using CRIU\ncontainer migration to help improve the resilience of running\nHPC workloads on clusters. We tested the container’s migration\ncapabilities using various real HPC applications. Results show\nthat we can successfully migrate containers with HPC\nworkloads between different physical machines, with minimal\ninterruption and no data corruption. We also did a broad range\nof benchmarks on containers using real HPC applications. This\nincluded benchmarking using three interconnect types and four\ndifferent machine types varying in specs. Results show that the\nperformance on containers was close to native. For future work,\nwe plan to investigate container migration in HPC environments\nhaving InfiniBand networks. We will also look into testing\nCRIU container migration with different container types such as\nSingularity and Docker.\n978-1-7281-5020-8/19/$31.00 ©2019 IEEE\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:01:19 UTC from IEEE Xplore.  Restrictions appl\nREFERENCES\n[1]\nThe TOP500 Project, “TOP500 List Statistics,” Nov-2018. [Online].\nAvailable: http://www.top500.org/statistics/list.\n[2]\nF. Cappello, A. Geist, B. Gropp, L. Kale, B. Kramer, and M. Snir,\n“Toward exascale resilience,” The International Journal of High\nPerformance Computing Applications, vol. 23, no. 4, pp. 374–388, 2009.\n[3]\nR. Riesen et al., “Redundant computing for exascale systems,” Sandia\nNational Laboratories, 2010.\n[4]\nVirtuozzo, “Checkpoint-Restore in Userspace (CRIU).” [Online].\nAvailable: https://www.criu.org.\n[5]\nD. W. Walker and J. J. Dongarra, “MPI: a standard message passing\ninterface,” Supercomputer, vol. 12, pp. 56–68, 1996.\n[6]\nP. Bridges, N. Doss, W. Gropp, E. Karrels, E. Lusk, and A. Skjellum,\n“Users’ Guide to mpich, a Portable Implementation of MPI,” Argonne\nNational Laboratory, vol. 9700, pp. 60439–4801, 1995.\n[7]\nE. Gabriel et al., “Open MPI: Goals, concept, and design of a next\ngeneration MPI implementation,” in European Parallel Virtual\nMachine/Message Passing Interface Users’ Group Meeting, 2004, pp.\n97–104.\n[8]\nIntel,\n“Intel\nMPI\nLibrary.”\n[Online].\nAvailable:\nhttps://software.intel.com/en-us/mpi-library.\n[9]\nJ. J. Dongarra, P. Luszczek, and A. Petitet, “The LINPACK benchmark:\npast, present and future,” Concurrency and Computation: practice and\nexperience, vol. 15, no. 9, pp. 803–820, 2003.\n[10] D. H. Bailey, “NAS parallel benchmarks,” Encyclopedia of Parallel\nComputing, pp. 1254–1259, 2011.\n[11] A. Polze, P. Troger, and F. Salfner, “Timely virtual machine migration\nfor pro-active fault tolerance,” in 2011 14th IEEE International\nSymposium\non\nObject/Component/Service-Oriented\nReal-Time\nDistributed Computing Workshops, 2011, pp. 234–243.\n[12] A. B. Nagarajan, F. Mueller, C. Engelmann, and S. L. Scott, “Proactive\nfault tolerance for HPC with Xen virtualization,” in Proceedings of the\n21st annual international conference on Supercomputing, 2007, pp. 23–\n32.\n[13] J. P. Walters and V. Chaudhary, “A fault-tolerant strategy for virtualized\nHPC clusters,” The Journal of Supercomputing, vol. 50, no. 3, pp. 209–\n239, 2009.\n[14] B. Davda and J. Simons, “RDMA on vSphere: Update and future\ndirections,” in Open Fabrics Workshop, 2012.\n[15] P. Kutch, “Pci-sig sr-iov primer: An introduction to sr-iov technology,”\nIntel application note, pp. 321211–002, 2011.\n[16] J. Liu, “Evaluating standard-based self-virtualizing devices: A\nperformance study on 10 GbE NICs with SR-IOV support,” in 2010 IEEE\nInternational Symposium on Parallel & Distributed Processing (IPDPS),\n2010, pp. 1–12.\n[17] J. Jose, M. Li, X. Lu, K. C. Kandalla, M. D. Arnold, and D. K. Panda,\n“SR-IOV support for virtualization on infiniband clusters: Early\nexperience,” in 2013 13th IEEE/ACM International Symposium on\nCluster, Cloud, and Grid Computing, 2013, pp. 385–392.\n[18] M. Musleh, V. Pai, J. P. Walters, A. Younge, and S. Crago, “Bridging the\nvirtualization performance gap for HPC using SR-IOV for InfiniBand,”\nin 2014 IEEE 7th International Conference on Cloud Computing, 2014,\npp. 627–635.\n[19] W. Huang, J. Liu, B. Abali, and D. K. Panda, “A case for high\nperformance computing with virtual machines,” in Proceedings of the\n20th annual international conference on Supercomputing, 2006, pp. 125–\n134.\n[20] Mellanox, “HowTo Configure SR-IOV for ConnectX-3 with KVM\n(InfiniBand).” 05-Dec-2018.\n[21] RedHat, “SR-IOV Support For Virtual Networking.” 05-Jan-2018.\n[22] A. Reber, “Process migration in a parallel environment.” Universität\nStuttgart, 23-Jun-2016.\n[23] J. Ansel, K. Arya, and G. Cooperman, “DMTCP: Transparent\ncheckpointing for cluster computations and the desktop,” in 2009 IEEE\nInternational Symposium on Parallel & Distributed Processing, 2009, pp.\n1–12.\n[24] S. Pickartz, N. Eiling, S. Lankes, L. Razik, and A. Monti, “Migrating\nLinuX containers using CRIU,” in International Conference on High\nPerformance Computing, 2016, pp. 674–684.\n[25] G. W. Dunlap, S. T. King, S. Cinar, M. A. Basrai, and P. M. Chen,\n“ReVirt: Enabling intrusion analysis through virtual-machine logging and\nreplay,” ACM SIGOPS Operating Systems Review, vol. 36, no. SI, pp.\n211–224, 2002.\n[26] C. Yu and F. Huan, “Live migration of docker containers through logging\nand replay,” in 2015 3rd International Conference on Mechatronics and\nIndustrial Informatics (ICMII 2015), 2015.\n[27] R. Stoyanov and M. J. Kollingbaum, “Efficient Live Migration of Linux\nContainers,” in High Performance Computing, vol. 11203, R. Yokota, M.\nWeiland, J. Shalf, and S. Alam, Eds. Cham: Springer International\nPublishing, 2018, pp. 184–193.\n[28] A. Kleen, “memhog: Allocation of memory with policy for testing,” 2019.\n[Online].\nAvailable:\nhttp://man7.org/linux/man-\npages/man8/memhog.8.html.\n[29] S. Soltesz, H. Pötzl, M. E. Fiuczynski, A. Bavier, and L. Peterson,\n“Container-based operating system virtualization: a scalable, high-\nperformance alternative to hypervisors,” in ACM SIGOPS Operating\nSystems Review, 2007, vol. 41, pp. 275–287.\n[30] W. Felter, A. Ferreira, R. Rajamony, and J. Rubio, “An updated\nperformance comparison of virtual machines and linux containers,” in\n2015 IEEE international symposium on performance analysis of systems\nand software (ISPASS), 2015, pp. 171–172.\n[31] M. G. Xavier, M. V. Neves, F. D. Rossi, T. C. Ferreto, T. Lange, and C.\nA. De Rose, “Performance evaluation of container-based virtualization\nfor high performance computing environments,” in 2013 21st Euromicro\nInternational Conference on Parallel, Distributed, and Network-Based\nProcessing, 2013, pp. 233–240.\n[32] C. Ruiz, E. Jeanvoine, and L. Nussbaum, “Performance evaluation of\ncontainers for HPC,” in European Conference on Parallel Processing,\n2015, pp. 813–824.\n[33] J. Zhang, X. Lu, and D. K. Panda, “High performance MPI library for\ncontainer-based HPC cloud on InfiniBand clusters,” in 2016 45th\nInternational Conference on Parallel Processing (ICPP), 2016, pp. 268–\n277.\n[34] J. Zhang, X. Lu, and D. K. Panda, “Performance characterization of\nhypervisor-and container-based virtualization for HPC on SR-IOV\nenabled InfiniBand clusters,” in 2016 IEEE International Parallel and\nDistributed Processing Symposium Workshops (IPDPSW), 2016, pp.\n1777–1784.\n[35] J. Zhang, X. Lu, and D. K. Panda, “Is Singularity-based Container\nTechnology Ready for Running MPI Applications on HPC Clouds?,” in\nProceedings of the10th International Conference on Utility and Cloud\nComputing, 2017, pp. 151–160.\n[36] Amazon, “Amazon Web Services (AWS),” 2019. [Online]. Available:\nhttps://aws.amazon.com.\n[37] Amazon Web Services, “Amazon EC2 Instance Types,” 2019. [Online].\nAvailable: https://aws.amazon.com/ec2/instance-types.\n[38] Amazon Web Services, “Amazon Elastic File System (EFS),” 2019.\n[Online]. Available: https://aws.amazon.com/efs.\n[39] S. Shepler, M. Eisler, and D. Noveck, “Network file system (NFS) version\n4 minor version 1 protocol,” 2010.\n[40] M. Furman, OpenVZ essentials. Packt Publishing Ltd, 2014.\n[41] OpenVZ, “OpenVZ Command Line Reference: prlctl.” [Online].\nAvailable:\nhttps://docs.openvz.org/openvz_command_line_reference.webhelp/_prlc\ntl.html. [Accessed: 01-Apr-2019].\n[42] Amazon, “AWS Command Line Interface (CLI).” [Online]. Available:\nhttps://aws.amazon.com/cli. [Accessed: 01-Apr-2019].\n[43] OpenVZ, “OpenVZ Command Line Reference: vzpkg.” [Online].\nAvailable:\nhttps://docs.openvz.org/openvz_command_line_reference.webhelp/_vzp\nkg.html. [Accessed: 01-Apr-2019].\n[44] Virtuozzo, “Using Virtuozzo in the Amazon EC2.” [Online]. Available:\nhttps://docs.virtuozzo.com/wiki/Using_Virtuozzo_in_the_Amazon_EC2\n978-1-7281-5020-8/19/$31.00 ©2019 IEEE\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:01:19 UTC from IEEE Xplore.  Restrictions appl\n#Configure_the_external_IP_address_for_the_container. [Accessed: 10-\nJan-2018].\n[45] Ohio State University, “OSU Micro-Benchmarks,” 2018. [Online].\nAvailable: http://mvapich.cse.ohio-state.edu/benchmarks. [Accessed: 01-\nApr-2018].\n[46] J. Latt, “Palabos, Parallel Lattice Boltzmann Solver,” FlowKit, Lausanne,\nSwitzerland, 2009.\n[47] Open Porous Media (OPM) Initiative, “Flow: Fully-Implicit Black-Oil\nSimulator,”\n2018.\n[Online].\nAvailable:\nhttps://opm-\nproject.org/?page_id=19. [Accessed: 27-Mar-2018].\n[48] Imperial College London AMCG, “Fluidity manual v4.1.12.” 22-Apr-\n2015.\n[49] D. Joiner, “GalaxSee-MPI:  Gravitation N-Body Simulation.” [Online].\nAvailable:\nhttps://www.shodor.org/refdesk/Resources/Tutorials/MPIExamples/Gala\nxSee.php. [Accessed: 04-Mar-2018].\n[50] Schlumberger, “ECLIPSE Industry-Reference Reservoir Simulator, *\nMark\nof\nSchlumberger,”\n2017.\n[Online].\nAvailable:\nhttps://www.software.slb.com/products/eclipse.\n[51] T. Sabat, “AWS CLI Script to Assign a Secondary IP,” 09-Jun-2015.\n[Online].\nAvailable:\nhttps://codepen.io/tsabat/post/aws-cli-script-to-\nassign-a-secondary-ip.\n[52] J. Ahrens, B. Geveci, and C. Law, “Paraview: An end-user tool for large\ndata visualization,” The visualization handbook, vol. 717, 2005.\n[53] M. Sindi, “Container Migration Demo for MPI-based HPC Workloads:\nPalabos Application,” Massachusetts Institute of Technology, 2019.\n[Online]. Available: https://youtu.be/1v73E2Ao3Mk.\n[54] M. Sindi, “Container Migration Demo for MPI-based HPC Workloads:\nThe ECLIPSE Simulator by Schlumberger,” Massachusetts Institute of\nTechnology, 2019. [Online]. Available: https://youtu.be/5tz6JP2UgTk.\n[55] M. Sindi, “Container Migration Demo for MPI-based HPC Workloads:\nGalaxSee Application,” Massachusetts Institute of Technology, 2019.\n[Online]. Available: https://youtu.be/NlT7nJ-yENc.\n[56] M. Sindi, “Container Demo for MPI-based HPC Workloads:\nSuspendResume Entire MPI Job - Flow Application,” Massachusetts\nInstitute\nof\nTechnology,\n2019.\n[Online].\nAvailable:\nhttps://youtu.be/KNTVHQnMVHU.\n[57] W. Gropp, L. Ewing, N. Doss, and A. Skjellum, “MPICH High-\nPerformance Portable MPI.” [Online]. Available: https://www.mpich.org.\n[Accessed: 21-Mar-2017].\n[58] Open MPI, “Open MPI FAQ: Does Open MPI support virtual IP\ninterfaces?,” 22-Jan-2019. [Online]. Available: https://www.open-\nmpi.org/faq/?category=tcp#ip-virtual-ip-interfaces. [Accessed: 03-Apr-\n2019].\n[59] J. Squyres, “Open MPI Hack to Work with OpenVZ Containers,” 24-Jun-\n2016.\n[Online].\nAvailable:\nhttps://www.mail-\narchive.com/users@lists.open-mpi.org/msg29585.html. [Accessed: 04-\nDec-2017].\n[60] CRIU, “What cannot be checkpointed.” 13-Jul-2017.\n[61] Schlumberger, “ECLIPSE Industry-Reference Reservoir Simulator\n(Version 2017.2) - Release Notes.” 2017.\n978-1-7281-5020-8/19/$31.00 ©2019 IEEE\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:01:19 UTC from IEEE Xplore.  Restrictions appl\n"
  }
]