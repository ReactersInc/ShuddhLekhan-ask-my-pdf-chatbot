[
  {
    "section": "Unknown",
    "text": "Visualizing an Exascale Data Center Digital Twin:\nConsiderations, Challenges and Opportunities\nMatthias Maiterth*\nOak Ridge National Laboratory\nWes Brewer\nOak Ridge National Laboratory\nDane De Wet\nOak Ridge National Laboratory\nScott Greenwood\nOak Ridge National Laboratory\nVineet Kumar\nOak Ridge National Laboratory\nJesse Hines\nOak Ridge National Laboratory\nSedrick Bouknight\nOak Ridge National Laboratory\nZhe Wang\nOak Ridge National Laboratory\nTim Dykes\nHewlett Packard Enterprise\nFeiyi Wang\nOak Ridge National Laboratory\nFigure 1: Prototype of the Frontier digital twin using Unreal Engine 5. All child components are visible: The first row, shows 12\ncompute racks and four cooling distribution units1. The racks show, among others: cable trays, rectifiers1, and compute nodes,\nwith all CPUs, GPUs1 and DIMMs visible. (1: Colored according to power consumption.) Total actors: 152,906.\nABSTRACT\nDigital twins are an excellent tool to model, visualize, and simulate\ncomplex systems, to understand and optimize their operation. In\nthis work, we present the technical challenges of real-time visual-\nization of a digital twin of the Frontier supercomputer.\nWe show the initial prototype and current state of the twin and\nhighlight technical design challenges of visualizing such a large\nHigh Performance Computing (HPC) system. The goal is to un-\nderstand the use of augmented reality as a primary way to extract\ninformation and collaborate on digital twins of complex systems.\nThis leverages the spatio-temporal aspect of a 3D representation\nof a digital twin, with the ability to view historical and real-time\ntelemetry, triggering simulations of a system state and viewing the\nresults, which can be augmented via dashboards for details. Finally,\nwe discuss considerations and opportunities for augmented reality\nof digital twins of large-scale, parallel computers.\nIndex Terms: Digital Twin, Data Center, Information Representa-\ntion, Massively Parallel Systems, Operational Data Analytics, Sim-\nulation, Augmented Reality\n1\nINTRODUCTION\nA digital twin can be defined as an (1) evolving digital represen-\ntation of the (2) historical, current and future behavior (3) of a\nphysical object or process (4) that helps to optimize its perfor-\nmance [17, 11]. Simulations for digital twins in scientific domains,\nfrom applied mathematics to the life-sciences, are often run on High\nPerformance Computing (HPC) systems [12] [21] [24]. Yet, using\ndigital twins to model HPC systems themselves is relatively new.\nHPC centers have been collecting metrics of their operations\nfor decades, which generally served to trigger system alerts and\ndo post-mortem data-analysis. Digital twins promise a holistic un-\nderstanding of HPC systems combining telemetry with simulation,\n*e-mail: maiterthm@ornl.gov\nProject home: https://exadigit.github.io.\nenabling insights to improve overall system efficiency.\nFor a digital twin, its visualization is the main point for human\ninteraction. It ties all disaggregate information together and allows\nus to correlate the temporospatial information generated.\nRelated works have shown the impact of the visual representa-\ntion of compute clusters; however, these are limited to smaller sys-\ntems [3] or only focus on one aspect of digital twins [23], or do\nnot incorporate telemetry of infrastructure [22] or simulation. At\nthe same time, to the knowledge of the authors, no other work dis-\ncusses the major challenges of visualizing large-scale data centers\nat high fidelity with real-time interaction, with the ability to trigger\nsimulations visualized in the same environment.\nIn this work, we present: A brief overview of Frontier, a state-of-\nthe-art supercomputer with more than 60 million parts, generating\none million data-points each second; the current state of the visual-\nization prototype for the exascale data center digital twin; and a dis-\ncussion on encountered challenges and opportunities for the visual-\nization of digital twins of large-scale parallel compute systems. The\noverarching ExaDigiT digital twin framework is presented in [4].\n2\nBACKGROUND\nThe Oak Ridge Leadership Computing Facility (OLCF) started in-\nstallation of the Frontier Supercomputer by the end of 2021, and\nachieved 1.1 ExaFlops of performance by the submission to the\nTOP500 list of June 2022 [18]. To create a digital twin and its\nvisualization, a description of the system and an understanding of\nthe telemetry data is needed, which is shown in the following.\n2.1\nSystem Description\nThe Frontier HPC system consists of 9,472 AMD compute nodes,\nwith a total of 9,472 Central Processing Units (CPUs) plus 37,888\nGraphics Processing Units (GPUs). In June 2023, 37,632 of these\nGPUs were used for the second TOP500 submission, achieving\n1.194 ExaFlops at 22.7 MW [19].\nThe system is comprised of seven rows of HPE Cray EX cabi-\nnets. Three Cray EX cabinets (384 compute nodes) are supported\nby one Cooling Distribution Unit (CDU) for cooling. Each com-\npute cabinet consists of eight compute chassis, plus the network-\ning switches, for the Slingshot Dragonfly network. Each chassis\nitself has eight Bard Peak blades, each housing two compute nodes.\n21\n2024 IEEE Visualization and Visual Analytics (VIS)\nU.S. Government work not protected by U.S. copyright\nDOI 10.1109/VIS55277.2024.00012\n2024 IEEE Visualization and Visual Analytics (VIS) | USGov | DOI: 10.1109/VIS55277.2024.00012\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 03:31:30 UTC from IEEE Xplore.  Restrictions appl\nThe nodes themselves consist of one AMD EPYC™7A53 “Trento”\nCPU with four AMD Instinct™MI250X GPUs. Each individual\nMI250X GPU is composed of two chiplets, called Graphics Com-\npute Dies (GCDs). Outside of the compute room, the central energy\nplant is responsible for pumping coolant through the primary cool-\ning loop and distributing it to the CDUs as part of the secondary\ncooling loop. The CDUs contains the heat exchangers to supply\neach cabinet with cooling. Switchboards, bus-bars, rectifiers, and\nSuper Intermediate VOltage Converters (SIVOCs) within the com-\npute room are responsible for distributing and stepping down volt-\nages for consumption at the nodes.\nThis high level overview alone shows that having a good under-\nstanding of such complex system, and how its parts interact, is not\nan easy task. Overall there are ∼60,000,000 individual parts in the\nsystem that could be modeled. The question is which to include,\nmodel, and show; Which depends on their state, data of interest,\nand potential simulations to augment insights gained.\n2.2\nTelemetry\nThe Oak Ridge Leadership Facility houses two flagship HPC sys-\ntems, while the installed infrastructure supports additional facili-\nties. The fact that there is not a single central system collecting\ndata is due to the sensible split of responsibilities according to fa-\ncility, compute, scheduling, etc. A majority of data sources of rele-\nvance for the digital twin are aggregated in the Integrated Telemetry\nDatabase (ITDB), but the consolidation is an ongoing process. At\nthe same time, several systems of relevance, e.g., to the central en-\nergy plant, are explicitly encapsulated. The collection of telemetry\nis a process not only required by a digital twin, but already under-\ntaken by Operational Data Analytics (ODA) [1].\nIn general, there are three data retention strategies for the teleme-\ntry generated, which influences how they can be used within a dig-\nital twin: Data is either a) at full resolution and stored long term, b)\navailable at full resolution but reduced in resolution for long-term\nstorage, or c) only available in real-time or for a short time win-\ndow and over-written in a sliding window fashion. Retention and\nlong-term storage depend on the impact and usage of each sensor\nvalue.\nOverall Frontier alone generates one million data points each\nsecond. This is a stark increase from the previous system, Sum-\nmit, which generated ‘only’ 400,000 data points each second.\n3\nCONSIDERATIONS\nFOR\nTHE UE5 FRONTIER DIGITAL\nTWIN\nThe goal of the digital twin is for users and system engineers to\ninteract with the virtual representation of the system and to under-\nstand the implications of the system setup, including telemetry re-\nplay and triggering simulations of its sub-components.\nInitial tests using web frameworks served as an example to eval-\nuate streaming of data into an Augmented Reality (AR)/Virtual Re-\nality (VR)-Scene. This was done using A-Frame.io [14, 15], which\nis great for prototyping but not suitable for large scenes. To re-\nsolve this, the prototype was re-implemented in Unreal Engine 5\n(UE5) [8] for AR, as it has been successfully adopted for other\nprojects in the organization [20, 9], using HoloLens2, as well as\ndesktop. A snapshot of the current state is shown in Fig. 1.\n3.1\nCurrent Prototype\nSystem hierarchy:\nThe general setup of the system is built us-\ning UE5-blueprints and follows the hierarchical setup of the system\nas outlined above. The Frontier system blueprint consists of rows\nof cabinets, as seen in Fig. 2. These are themselves either com-\npute racks or CDU blueprints. The compute rack blueprint contains\neight chassis, where each chassis is further subdivided into its net-\nwork equipment, cooling manifolds, PDUs and rectifiers. Addition-\nally, each chassis contains eight blades, each with two nodes, where\nFigure 2: Desktop View: Side View and Translucent top down.\nFigure 3: Left to right: Cooling manifold, two rectifiers, eight blades\n(containing two nodes each), six additional rectifiers, eight blades\nand the PDUs on the right. Each node consists of one CPU, four\nGPUs, eight DIMMs each. Telemetry in red for rectifiers and GPUs.\nnodes contain blueprints for CPU, GPUs and Dual In-line Memory\nModules (DIMMs). In case a component of the system is contained\nwithin another, the modeled UE5 actor is attached to a parent actor\nforming a hierarchy of actors. The detailed view of a single chassis,\nshowing rectifiers, GPUs, CPUs, and DIMMs is shown in Fig. 3.\nComponent Representation:\nEven with the more advanced\nengine, the number of individual components is very large. With\nthe current state of the implementation, the fully populated scene\nhas over 150,000 UE5 actors. (The setup follows the blueprint hi-\nerarchy as described above, plus a few additional parts such as rec-\ntifiers, etc.) The actors can be spawned on demand, either in the\neditor or while running the scene, to only show the subsystems of\ninterest. Since each component can show different information and\ncan be spawned on demand, for visual clarity, typical techniques\nsuch as merging of meshes or instanced components are not feasi-\nble. This would prohibit AR interaction and disallow streaming of\ndata to only subsets of a system and the change of the system in a\nnon-static way. This is a double-edged sword: the resulting large\nactor counts are not good for performance, but allow to reduce vi-\nsual clutter and interactive focus on areas of interest. In the default\nlevel, the user only sees the 104 cabinets (of which 74 are com-\npute cabinets of frontier) and can interact and show internals and\nadditional detail on demand, alleviating the large actor count.\nFig. 1 and Fig. 2 show all components spawned in different\nviews.\nDisplaying all components with all details and dynamic\ntelemetry is not feasible, due to performance limitations. In an in-\nteractive view only the subsystems of interest are spawned, as seen\nin Fig. 4, showing the system projected onto a meeting table, or in\nFig. 5, where the two twins are shown in place.\nData Representation:\nEach component has a blueprint with\na material and variables for the metrics of interest. The metrics\nare used to programmatically change the material color based on\nthe requested metric (from low to high, teal to red). Additionally,\n22\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 03:31:30 UTC from IEEE Xplore.  Restrictions appl\nFigure 4: AR Table-top: First (right) and third (left) person view.\nFigure 5: Digital and physical twin in-place: First (left) and third (right)\nperson view. The user is displaying telemetry and about to trigger the\nsimulation in AR. The QR-Code serves as spatial anchor.\nthe system is programmed in a way where the Frontier system has\na representation of the hierarchical data and each spawned child-\nactor has functionality to distribute and display its data as a color\nvia the material of the actor. The telemetry data is either stored lo-\ncally or can be requested from the telemetry service which is then\ndistributed to the correct location. This can be used to continuously\nstream data or project a recorded dataset into the scene. For this,\nwe use our telemetry service and can request data of interest. In the\ncase shown in the figures, we query power for the CDUs, the recti-\nfiers, and the GPUs. Data structures, data ingestion and distribution\nis implemented in parts in blueprints but mostly C++.\n3.2\nData Ingestion and Visualization\nFig. 6 shows the data flow from source to visualization.\nAf-\nter querying, selecting, and preprocessing, the remote data is for-\nwarded to the local workstation. This allows to query data from the\nbeginning of data collection within under five seconds via REST-\nAPI of the telemetry service. ‘Live’ data via the telemetry service\nis available within two minutes [1] of collection. The final pre-\nprocessing step is performed locally such that the visualization can\nbe displayed in the 3D-Scene.\nFigure 6: Data Flow from source to visualization.\nFigure 7: Future Work: Dragonfly Network, with inter-group (left) and\nintra-group (middle) network. And central energy plant (right)\nDepending on the current actor hierarchy in the scene, the\ntelemetry is used to populate a hierarchical shadow structure, which\nis then used to distribute the data to the respective actor. This is\nachieved by the implementation of a scatter or broadcast mecha-\nnism available to each blueprint, by dynamically distributing and\nrouting of the information in at most nlogn steps. This allows us\nto handle missing data, while only requiring distribution to compo-\nnents present in the current scene. The design allows us to incre-\nmentally add subsystems based on the state and focus of the scene.\nAdditionally, this enables us to easily port the design to other HPC\ntwins. A static distribution using offsets in a data table has been\navoided due to reduced flexibility and portability reasons.\nThe data-set used in the figures of this work is the power data\nrecorded for the TOP500 submission of June 2023 [19, 2]. In gen-\neral, the data can be queried on demand by the client.\n3.3\nIntegration of simulation for what-if scenarios\nAs mentioned in the introduction, “[a digital twin is a digital] rep-\nresentation of the (2) historical, current and future behavior [...]”\nof a system [17]. Therefore, not only is the display of telemetry\ndata of interest, but also the ability to simulate subsystems behav-\nior based on a system state. The level of fidelity (both spatially and\ntemporally) for each of such interdependent simulation tools largely\ndepend on how they are coupled with each other. Depending on the\nthe simulation, this can be run in in-situ, alongside the digital twin,\nor be fed into the 3D-scene after the simulation completion. This\ndepends largely on the model complexity, real-time capability and\nhow the individual simulations are coupled.\nOur current model is able to run a thermo-fluidic model to simu-\nlate the cooling system, triggered via a hand menu. This is achieved\nusing the Functional Mock-up Interface (FMI) standard interface\nused to integrate packaged ODE-based system models (Functional\nMock-up Units (FMUs)). The FMU model accepts real-time in-\nputs and generates outputs which are key to coupling to other mod-\nels within the digital twin modeling framework. The system-level\nmodel offers a good balance between advanced predictive capabil-\nity and simulation time in comparison to Computational Fluid Dy-\nnamics (CFD) models which offer greater spatial and temporal res-\nolution but are currently not suitable for real-time dynamic simula-\ntions. In the work presented, a Modelica [5] model was developed\nand the simulation of the liquid cooling loop from cooling tower\nto CDU is integrated and can be triggered and results visualized via\nFMU plugin [10], developed for the TRANSFORM library [9]. The\nresults of the simulation are displayed in the mocked internals of the\nCDUs (see Fig. 5, left). The thermo-fluid simulation includes the\ncooling loop from cooling tower to CDU [4] and is in the process\nof being fully integrated with the visualization. Currently, we map\nsimulated flow rates, pressures and temperatures to the CDUs when\ntriggering the simulation from telemetry within the visualization.\n3.4\nNext steps: Addition of the network and integration\nof central energy plant\nHPC systems rely on fast interconnects to achieve high performance\nat system scale, therefore, modeling and understanding the net-\n23\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 03:31:30 UTC from IEEE Xplore.  Restrictions appl\nwork is essential. We have started prototyping the network and can\nshow a full Dragonfly topology, with 74 ∗73/2 Inter-Group plus\n74 ∗(64 ∗63)/2 Intra-Group links, resulting in a total of 151,885\nlinks. This large amount of links is implemented via the Niagara\nparticle system in UE5 [7]. (Note: The real system has fewer links,\nas only 32 switches per rack are installed, where 64 is the racks\nmaximum.) The next step is to replay and display telemetry for each\nlink. Fig. 7 shows that all static links can be displayed, and even be\nstreamed to the HoloLens2 without performance issues. Selectively\ndisplaying based on link-level telemetry is work in progress.\nAdditionally, we have added the CAD models for the central en-\nergy plant in the visualization, where integration of simulation and\ntelemetry are ongoing work (see Fig. 7 right).\n4\nCHALLENGES\nIn the following, we discuss the major challenges encountered:\nLarge Scale Systems:\nThe system discussed is at extreme\nscale. This goes both for individual components, but is true for\neach: compute, infrastructure, and data. At the same time, the inter-\nplay of the different systems spans multiple domains (computation,\nelectric, cooling). Most of the compute systems can be grouped\nhierarchically, such as the layout described in the setup above. Ev-\nerything else has a complex interaction with the compute system\nand forms an overlay structure: the networking of the system; the\nelectrical system; the cooling system. Mapping active workloads\nand jobs to the compute system and understanding its impact on the\naforementioned subsystems poses a significant challenge, where a\nvisual system can help understand theses interactions.\nBig Data:\nAs seen in the telemetry section, there are several\nteams required to maintain and operate the telemetry services to\nuse the data coming from the system, in which ODA [16] is of\nprincipal importance. This is truly big data, as becomes clear by\nputting information of our telemetry system to the four Vs associ-\nated with big data [1]: Volume (expected 20PB); Velocity (currently\n300 MB/s of telemetry in transit); Variety (Types of sensors, data-\ntypes, sampling frequency retention strategy etc.); Veracity (miss-\ning data and faulty sensors). By projecting the collected data onto\na digital twin, a better understanding of the system is formed. Yet,\nthe real value-add is achieved by using this data to run simulations\nand gain additional insights, by doing what-if analysis.\nVisualization:\nThe current visualization only captures a part of\nthe system. However, the total number of actors with only this num-\nber of components present in the system is over 150,000 already.\nSince we use simple cuboids the rendering and updating of data\nin the scene is still feasible and performance on a modern desktop\nsystem is acceptable. When streaming the scene to a HoloLens 2\ndevice from the same desktop, not all actors can be displayed, expe-\nriencing limitations at ∼100,000 actors. Even if we could stream\nall this data, there is more data of interest, thus we always have\nto make trade-offs and select areas of interest and usability, while\nproviding overview and detail.\n5\nOPPORTUNITIES\nThe combination of telemetry and simulation displayed in the same\n3D scene allows for unique capabilities for better understanding\ncomplex system behavior.\nSpatial and Temporal Correlation of Information:\nEach\ncomponent within the system has a physical location. Data visu-\nalization of telemetry usually does not present the data according\nto its location. Digital twins present a unique opportunity to bring\ntelemetry and sensors into a spatial context and help to understand\nthe impact on connected systems. After the spatial correlation, the\ntemporal correlation of the data is the next step. The distribution\nof telemetry observed in different subsystems can be replayed in a\nsynchronized fashion. Depending on the fidelity of the simulations,\ninitial conditions can be chosen based on the telemetry of the com-\nplementary subsystems and replayed based on the temporal context.\nAdaptive Layout of Information:\nLessons learned from infor-\nmation visualization should be vigorously applied to avoid informa-\ntion overload and visual clutter. This can be especially detrimental\nin AR and VR environments. At the same time, by operating inside\nof a game engine, the layout is not tied to the physical layout of\nthe system. In Computer Aided Design (CAD) the exploded view\nof a part is an example for this. For an HPC digital twin, present-\ning the information in a planar view as seen in a dashboard and\nthen transforming the individual components back to their physical\nlayout can allow bridging the chasm between data and information\nvisualization and 3D visualization of physical systems [13, 6].\nUnderstanding Complex System Interaction:\nWithin the\ndigital twin modeling framework, one of the keys to understanding\ncomplex system interaction is to ensure that each of the constituent\nmodels can exchange dynamic data and generate system simulation\nwhich is useful to the end-user or customer. The flavor of the digital\ntwin depends on the end-user – for planning and the design phase,\nfor operations, or after decommissioning for retrospective use. Our\nuse-cases are: In the facility design/planning phase, optimization of\nthe cooling system can be used to drive the datacenter Power Usage\nEffectiveness (PUE) closer to 1.0. For facility operations, which is\nthe typical use-case for a digital twin, the digital twin is useful for\nthe facility operator to visualize the performance of the digital twin\nalongside the real system, allowing to localize potential deviation.\nThe digital twin can also include reliability models informing the\noperator of potential component failures, such as mechanical com-\nponents (cooling system), electrical components (power supply),\nand GPUs/CPUs. The what-if-scenario use case relies heavily on\nthe simulation aspect of the digital twin and is descibed in [4].\nInteractive Level of Detail (LoD):\nWe identified interaction\nas an LoD mechanism to be an excellent way of coping with lim-\nited performance and at the same time visual clutter. Fully utilizing\ninteractive LoD is an opportunity, yet to find good ways of gen-\neral interaction is not always trivial in complex systems. The value\nof the digital twins is also of collaborative nature, therefore under-\nstanding how interactive LoD can be done with network replication\nin mind is a large opportunity that we want to leverage.\nCombining AR with web-based Dashboards:\nData analysis\nin three dimensions is not trivial, where two dimensions can have\nsignificant advantages when comparing individual data points, and\ntimelines. As we operate in an AR environment we can bring web-\nbased dashboards already developed for ODA into the scene, and\nsupplement the digital twin with simulation-specific dashboards.\nThis also improves the value of the dashboards, as it is generally\nnot trivial to correlate their information to the spatial domain. This\nbridges a huge gap that only digital twins with real-time analysis\ncapability can achieve.\n6\nCONCLUSION & FUTURE WORK\nWe presented the current state of the visualization for the Frontier\ndigital twin. We showed the hierarchically spawn-able structure of\nour twin and the capability to replay data from the ODA teleme-\ntry servers and to trigger thermo-fluid simulations, displaying its\nresults in the live AR environment. With the focus on interactive\nAR systems, resource limitations for representing such a large-scale\nsystem had to be carefully considered, resulting in a solution with\nscalability and interactive LoD in mind. We are in the process of\ncapturing additional use-cases and are working with the community\nto make it modular and relevant for other compute systems.\nThe visualization of a digital twin greatly augments the under-\nstanding of a complex system, such as the Frontier supercomputer,\nand allows us to interact with them for greater understanding.\n24\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 03:31:30 UTC from IEEE Xplore.  Restrictions appl\nACKNOWLEDGMENTS\nThis research used resources of the Oak Ridge Leadership Comput-\ning Facility, which is a DOE Office of Science User Facility sup-\nported under Contract DE-AC05-00OR22725.\nThe author did not use any content generated by artificial intelli-\ngence (AI) in this article.\nREFERENCES\n[1] R. Adamson, T. Osborne, C. Lester, and R. Palumbo. STREAM: A\nScalable Federated HPC Telemetry Platform, 5 2023. 2, 3, 4\n[2] S. Atchley and M. Maiterth. OLCF Frontier Supercomputer 2023-04-\n29 HPL Power Data used for Top500/Green500 Submission, 5 2023.\ndoi: 10.13139/OLCF/1975494 3\n[3] B. Bergeron, M. Hubbell, D. Sequeira, W. Williams, W. Arcand,\nD. Bestor, C. Byun, V. Gadepally, M. Houle, M. Jones, A. Klien,\nP. Michaleas, L. Milechin, J. Mullen, A. Prout, A. Reuther, A. Rosa,\nS. Samsi, C. Yee, and J. Kepner. 3d real-time supercomputer moni-\ntoring. In 2021 IEEE High Performance Extreme Computing Confer-\nence, HPEC 2021, pp. 1–7. IEEE, Waltham, MA, USA, 9 2021. doi:\n10.1109/HPEC49654.2021.9622787 1\n[4] W. Brewer, M. Maiterth, V. Kumar, R. Wojda, S. Bouknight, J. Hines,\nW. Shin, J. Webb, S. Greenwood, W. Williams, D. Grant, and F. Wang.\nA digital twin framework for liquid-cooled supercomputers as demon-\nstrated at exascale. In Proceedings of the International Conference\nfor High Performance Computing, Networking, Storage and Analysis,\nSC ’24. Association for Computing Machinery, New York, NY, USA,\n11 2024. (accepted). 1, 3, 4\n[5] F. Casella, M. Otter, K. Proelss, C. Richter, and H. Tummescheit. The\nModelica fluid and media library for modeling of incompressible and\ncompressible thermo-fluid pipe networks. In Proceedings of the 5th\nInternational Modelica Conference, pp. 631–640. The Modelica As-\nsociation, Vienna, Austria, 2006. 3\n[6] A. Cockburn, A. Karlson, and B. B. Bederson.\nA review of\noverview+detail, zooming, and focus+context interfaces. ACM Com-\nput. Surv., 41(1), jan 2009. doi: 10.1145/1456650.1456652 4\n[7] Epic Games. Niagara Reference, 11 2022. 4\n[8] Epic Games. Unreal Engine 5.1, 11 2022. 2\n[9] M. S. Greenwood, B. R. Betzler, A. L. Qualls, J. Yoo, and C. Rabiti.\nDemonstration of the advanced dynamic system modeling tool trans-\nform in a molten salt reactor application via a model of the molten salt\ndemonstration reactor. Nuclear Technology, 206(3):478–504, 2020.\ndoi: 10.1080/00295450.2019.1627124 2, 3\n[10] S. Greenwood. Unreal Engine - FMI Plugin (UEFMI), 2019. 3\n[11] W. Hu, T. Zhang, X. Deng, Z. Liu, and J. Tan. Digital twin: A state-\nof-the-art review of its enabling technologies, applications and chal-\nlenges. Journal of Intelligent Manufacturing and Special Equipment,\n2(1):1–34, 2021. doi: 10.1108/JIMSE-12-2020-010 1\n[12] M. Kasztelnik, P. Nowakowski, J. Meizner, M. Malawski, A. Nowak,\nK. Gadek, K. Zajac, A. A. L. Mattina, and M. Bubak. Digital Twin\nSimulation Development and Execution on HPC Infrastructures. In\nJ. Mikyˇska, C. de Mulatier, M. Paszynski, V. V. Krzhizhanovskaya,\nJ. J. Dongarra, and P. M. Sloot, eds., Computational Science – ICCS\n2023, pp. 18–32. Springer Nature Switzerland, Cham, 2023. doi: 10.\n1007/978-3-031-36021-3 2 1\n[13] J. Kehrer and H. Hauser. Visualization and visual analysis of multi-\nfaceted scientific data: A survey. IEEE Transactions on Visualization\nand Computer Graphics, 19(3):495–513, 2013. doi: 10.1109/TVCG.2012\n.110 4\n[14] D. Marcos, D. McCurdy, K. Ngo, and A-frame contributors. A-frame\n– a web framework for building 3D/AR/VR experiences, 01 2023. 2\n[15] D. Marcos, D. McCurdy, K. Ngo, and A-frame contributors. aframe\ngit, 01 2023. 2\n[16] A. Netti, W. Shin, M. Ott, T. Wilde, and N. Bates. A conceptual frame-\nwork for HPC operational data analytics. In 2021 IEEE International\nConference on Cluster Computing (CLUSTER), pp. 596–603. IEEE,\nPortland, OR, USA, 2021. doi: 10.1109/Cluster48925.2021.00086 4\n[17] A. Parrott and L. Warshaw. Industry 4.0 and the DT, 2017. 1, 3\n[18] PROMETEUS Professor Meuer Technologieberatung und Services\nGmbH. Top500 supercomputer sites — june 2022, 05 2022. 1\n[19] PROMETEUS Professor Meuer Technologieberatung und Services\nGmbH. Top500 supercomputer sites — june 2023, 06 2023. 1, 3\n[20] M. B. Smith, D. E. Peplow, N. Nelson, N. Thompson, M. S. Green-\nwood, and USDOE. Viper - method for visualizing and interacting\nwith ionizing radiation data using augmented reality, 4 2023. doi: 10.\n11578/dc.20230406.2 2\n[21] B. G. Sumpter and V. Meunier. Digital twins in materials and chemical\nsciences. Carbon Trends, 13(C), 12 2023. doi: 10.1016/j.cartre.2023.\n100297 1\n[22] R. Wang, Z. Cao, X. Zhou, Y. Wen, and R. Tan. Phyllis: Physics-\ninformed lifelong reinforcement learning for data center cooling con-\ntrol. In Proceedings of the 14th ACM International Conference on Fu-\nture Energy Systems, e-Energy ’23, p. 114–126. Association for Com-\nputing Machinery, New York, NY, USA, 2023. doi: 10.1145/3575813.\n3595189 1\n[23] R. Wang, X. Zhou, L. Dong, Y. Wen, R. Tan, L. Chen, G. Wang, and\nF. Zeng. Kalibre: Knowledge-based neural surrogate model calibra-\ntion for data center digital twins. In Proceedings of the 7th ACM Inter-\nnational Conference on Systems for Energy-Efficient Buildings, Cities,\nand Transportation, BuildSys ’20, p. 200–209. Association for Com-\nputing Machinery, New York, NY, USA, 2020. doi: 10.1145/3408308.\n3427982 1\n[24] I. Zacharoudiou, J. McCullough, and P. Coveney. Development and\nperformance of a HemeLB GPU code for human-scale blood flow sim-\nulation. Computer Physics Communications, 282:108548, 2023. doi:\n10.1016/j.cpc.2022.108548 1\n25\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 03:31:30 UTC from IEEE Xplore.  Restrictions appl\n"
  }
]