[
  {
    "section": "Unknown",
    "keywords": [
      "container migration",
      "HPC",
      "workloads"
    ],
    "key_phrases": [
      "container migration",
      "HPC workloads",
      "running HPC workloads"
    ],
    "key_points": [
      "The paper discusses using container migration for HPC workloads.",
      "Workloads can be migrated from nodes anticipating hardware problems to healthy nodes.",
      "Migration is done using CRIU with no application modification."
    ]
  },
  {
    "section": "Resilience",
    "keywords": [
      "container migration",
      "HPC",
      "CRIU"
    ],
    "key_phrases": [
      "container-based HPC environment",
      "container migration",
      "real HPC applications"
    ],
    "key_points": [
      "The authors share experiences implementing a container-based HPC environment for resilience.",
      "Workloads are migrated using CRIU without application modification.",
      "Tests were conducted with various hardware, network interconnects, and MPI implementations.",
      "Results demonstrate successful migration with minimal interruption and maintained data integrity.",
      "Application performance in containers is close to native performance.",
      "This work is the first to demonstrate successful migration of real MPI-based HPC workloads using CRIU and containers."
    ]
  },
  {
    "section": "I. Introduction",
    "keywords": [
      "HPC",
      "containers",
      "migration"
    ],
    "key_phrases": [
      "high performance computing",
      "container migration",
      "fault tolerance mechanism"
    ],
    "key_points": [
      "Most top supercomputers are based on commodity hardware with relatively low reliability (MTBF).",
      "The paper explores using Linux containers to improve the resilience of HPC workloads.",
      "Container migration, facilitated by CRIU, is used as a fault tolerance mechanism.",
      "The environment aims to sustain workloads when hardware issues are identified.",
      "The study benchmarks container performance against native systems using real HPC applications.",
      "The authors contribute the first successful migration of MPI-based HPC workloads using CRIU and containers.",
      "Video demonstrations of the migration tests are provided."
    ]
  },
  {
    "section": "Ii. Related Work",
    "keywords": [
      "VM",
      "containers",
      "migration"
    ],
    "key_phrases": [
      "virtual machine migration",
      "container migration",
      "process-level migration"
    ],
    "key_points": [
      "VM technologies have been used for workload migration but introduce overhead.",
      "Process-level migration attempts with CRIU were previously unsuccessful for parallel MPI processes.",
      "Container technology is a newer alternative to VMs with lower overhead.",
      "Container migration is a relatively unexplored area, especially in HPC.",
      "Previous container migration studies used artificial loads or non-HPC applications.",
      "Existing benchmarks often use generic HPC benchmarks (HPL, NPB) instead of real applications."
    ]
  },
  {
    "section": "III. SYSTEM DESIGN OF CONTAINER ENVIRONMENT",
    "keywords": [
      "AWS",
      "OpenVZ",
      "CRIU"
    ],
    "key_phrases": [
      "cluster setup",
      "software setup",
      "container environment"
    ],
    "key_points": [
      "The cluster test environment was set up using Amazon Web Services (AWS).",
      "Four types of node instances with varying specs were used for testing.",
      "Shared storage was provided by Amazon's Elastic File System (EFS).",
      "OpenVZ containers were chosen for their features and ease of use.",
      "CRIU was used for checkpointing and restarting containers.",
      "Parallels' prlctl tool was used for container provisioning and management.",
      "The environment was based on a modified RHEL 7.4 kernel with OpenVZ patches.",
      "Due to AWS limitations, Virtuozzo 7.0.7 was used, which is functionally equivalent to OpenVZ."
    ]
  },
  {
    "section": "C. Container Setup",
    "keywords": [
      "container",
      "HPC",
      "resources"
    ],
    "key_phrases": [
      "full access",
      "memory allocation",
      "prlctl tool"
    ],
    "key_points": [
      "Each node runs a single container with full access to underlying resources.",
      "Containers have unrestricted core access by default.",
      "Memory allocation is limited to leave sufficient resources for the OS (e.g., 500GB RAM out of 512GB).",
      "The 'prlctl' tool is used to automate container creation and configuration (OS template, IP address, users, DNS, NFS mounts, SSH keys, environment variables, firewall).",
      "CentOS 7 is used as the container template.",
      "The 'vzpkg' tool is used to install necessary packages inside the container (MPI libraries, third-party dependencies).",
      "Separate container images are created for each HPC application to minimize size."
    ]
  },
  {
    "section": "D. Network Setup",
    "keywords": [
      "IP address",
      "container",
      "network"
    ],
    "key_phrases": [
      "public IP",
      "private IP",
      "floating IP addresses"
    ],
    "key_points": [
      "Each instance and container are assigned two IP addresses: one public and one private.",
      "Public IPs are used for remote access via SSH.",
      "Private IPs are used for local communication.",
      "Container IPs are associated with a secondary network interface, allowing them to act as floating IPs during migration.",
      "Static IPs are used for benchmarking and migration testing.",
      "Details on IP setup can be found in reference [44]."
    ]
  },
  {
    "section": "IV. HPC APPLICATIONS TO TEST",
    "keywords": [
      "HPC",
      "applications",
      "MPI"
    ],
    "key_phrases": [
      "OSU Micro-Benchmarks",
      "reservoir simulator",
      "parallel applications"
    ],
    "key_points": [
      "Six MPI-based HPC applications were chosen for testing.",
      "The applications cover various domains.",
      "Applications tested include: OSU Micro-Benchmarks, Palabos, Flow, Fluidity, GalaxSee, and ECLIPSE.",
      "Table 1 summarizes the applications."
    ]
  },
  {
    "section": "V. Testing And Results",
    "keywords": [
      "performance",
      "migration",
      "containers"
    ],
    "key_phrases": [
      "performance overhead",
      "container migration",
      "MPI jobs"
    ],
    "key_points": [
      "Performance was benchmarked on containers and compared to native systems.",
      "MPI jobs were launched using 'mpirun' with appropriate IP addresses.",
      "Benchmarks were run four times and averaged.",
      "MPICH and Intel MPI libraries were used.",
      "OSU Micro-Benchmarks were used to assess network performance (latency and bandwidth).",
      "Container bandwidth showed a slight reduction on 10 and 25 Gigabit networks (3.9% average overhead).",
      "Container latency showed a slight overhead (6.8% average).",
      "The overhead is attributed to host-routed network mode.",
      "Overall performance overhead on real HPC applications was negligible (0.034% average, max 0.9%).",
      "Container migration was tested by suspending a container during an MPI job and migrating it to a spare machine.",
      "The 'prlctl' tool and CRIU library were used for migration.",
      "AWS CLI was used to migrate the floating IP address.",
      "Migration time was influenced by container size and disk speed.",
      "Using Provisioned IOPS SSD (io1) disks improved migration time by 35% (22 seconds vs. 34 seconds with gpt2).",
      "Data integrity was verified after migration using post-processing tools and file comparisons.",
      "Demo videos of container migration are available (links in Table 4)."
    ]
  },
  {
    "section": "VI. CHALLENGES AND SOLUTIONS",
    "keywords": [
      "OpenMPI",
      "CRIU",
      "migration"
    ],
    "key_phrases": [
      "virtual IP interfaces",
      "NFS mounts",
      "file locks"
    ],
    "key_points": [
      "Open MPI version 1.10.7 initially failed to launch jobs due to lack of support for virtual IP interfaces.",
      "A hack to the Open MPI source code was implemented as a workaround.",
      "Migrating containers with active NFS mounts caused CRIU to hang due to issues with 'rpcinfo' tool.",
      "The CRIU code was modified to use standard NFS port numbers instead of 'rpcinfo'.",
      "Container migration failed with ECLIPSE due to file locks on NFS.",
      "The ECLIPSE simulator was configured to disable the generation of the file causing the lock."
    ]
  },
  {
    "section": "VII. Conclusion And Future Work",
    "keywords": [
      "migration",
      "containers",
      "HPC"
    ],
    "key_phrases": [
      "container migration",
      "HPC workloads",
      "future work"
    ],
    "key_points": [
      "CRIU container migration successfully improved the resilience of HPC workloads.",
      "Container migration was tested with various HPC applications with minimal interruption and no data corruption.",
      "Container performance was close to native.",
      "Future work includes investigating container migration with InfiniBand networks.",
      "Future work includes testing CRIU with different container types (Singularity, Docker)."
    ]
  }
]