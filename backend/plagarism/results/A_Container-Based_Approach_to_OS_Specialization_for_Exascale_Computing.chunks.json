[
  {
    "section": "Unknown",
    "text": "A Container-Based Approach to OS Specialization for Exascale Computing\nJudicael A. Zounmevo∗, Swann Perarnau∗, Kamil Iskra∗, Kazutomo Yoshii∗,\nRoberto Gioiosa†, Brian C. Van Essen‡, Maya B. Gokhale‡, Edgar A. Leon‡\n∗Argonne National Laboratory {jzounmevo@, perarnau@mcs., iskra@mcs., kazutomo@}anl.gov\n†Paciﬁc Northwest National Laboratory. Roberto.Gioiosa@pnnl.gov\n‡Lawrence Livermore National Laboratory. {vanessen1, maya, leon}@llnl.gov\nAbstract—Future exascale systems will impose several con-\nﬂicting challenges on the operating system (OS) running on\nthe compute nodes of such machines. On the one hand, the\ntargeted extreme scale requires the kind of high resource usage\nefﬁciency that is best provided by lightweight OSes. At the\nsame time, substantial changes in hardware are expected for\nexascale systems. Compute nodes are expected to host a mix of\ngeneral-purpose and special-purpose processors or accelerators\ntailored for serial, parallel, compute-intensive, or I/O-intensive\nworkloads. Similarly, the deeper and more complex memory\nhierarchy will expose multiple coherence domains and NUMA\nnodes in addition to incorporating nonvolatile RAM. That\nexpected workload and hardware heterogeneity and complexity\nis not compatible with the simplicity that characterizes high\nperformance lightweight kernels. In this work, we describe the\nArgo Exascale node OS, which is our approach to providing\nin a single kernel the required OS environments for the two\naforementioned conﬂicting goals. We resort to multiple OS\nspecializations on top of a single Linux kernel coupled with\nmultiple containers.\nKeywords-OS Specialization; Lean; Container; Cgroups\nI. INTRODUCTION\nDisruptive new computing technology has already be-\ngun to change the scientiﬁc computing landscape. Hybrid\nCPUs, manycore systems, and low-power system-on-a-chip\ndesigns are being used in today’s most powerful high-\nperformance computing (HPC) systems. As these technology\nshifts continue and exascale machines close in, the Argo\nresearch project aims to provide an operating system and\nruntime (OS/R) designed to support extreme-scale scientiﬁc\ncomputations. It aims to efﬁciently leverage new chip and\ninterconnect technologies while addressing the new modali-\nties, programming environments, and workﬂows expected at\nexascale.\nAt the heart of the project are four key innovations:\ndynamic reconﬁguring of node resources in response to\nworkload changes, allowance for massive concurrency, a hi-\nerarchical framework for power and fault management, and\na cross-layer communication protocol that allows resource\nmanagers and optimizers to communicate and control the\nplatform. These innovations will result in an open-source\nprototype system that runs on several architectures. It is\nexpected to form the basis of production exascale systems\ndeployed in the 2018–2020 timeframe.\nThe design is based on a hierarchical approach. A global\nview enables Argo to control resources such as power or\ninterconnect bandwidth across the entire system, respond\nto system faults, or tune application performance. A local\nview is essential for scalability, enabling compute nodes to\nmanage and optimize massive intranode thread and task par-\nallelism and adapt to new memory technologies. In addition,\nArgo introduces the idea of “enclaves,” a set of resources\ndedicated to a particular service and capable of introspection\nand autonomic response. Enclaves will be able to change the\nsystem conﬁguration of nodes and the allocation of power\nto different nodes or to migrate data or computations from\none node to another. They will be used to demonstrate\nthe support of different levels of fault tolerance—a key\nconcern of exascale systems—with some enclaves handling\nnode failures by means of global restart and other enclaves\nsupporting ﬁner-level recovery.\nWe describe here the early stages of an ongoing effort, as\npart of Argo, to evolve the Linux kernel into an OS suitable\nfor exascale nodes. A major step in designing any HPC\nOS involves reducing the interference between the OS and\nthe HPC job. Consequently, we are exposing the hardware\nresources directly to the HPC runtime, which is a user-level\nsoftware layer. However, the increase in resource complexity\nexpected for the next-generation supercomputers creates a\nneed for some hardware management that is best left to the\nOS. The resource complexity comes from the heterogeneous\nset of compute cores and accelerators, coupled with deeper\nand more complex memory hierarchies with multiple co-\nherence and NUMA domains to cope with both the CPU\nheterogeneity and the massive intranode parallelism. Our de-\nsign is based on the concept of OS Specialization—splitting\nthe node OS into several autonomous components each\nmanaging its own subset of CPU cores, a memory region,\netc. We introduce the concepts of ServiceOS which is a fully-\nﬂedged Linux environment meant for node management\nand legacy application execution, and Compute Containers\nwhich realize lean OS environments meant for running HPC\napplications with little interference from the OS.\nThe rest of the paper is organized as follows. Section II\nprovides motivation for putting forth OS specialization in\nthe Argo NodeOS. Section III describes our approach to\nOS specialization using containers, as well as how Compute\nContainers can achieve various levels of leanness over the\nsame fully-ﬂedged Linux kernel used by the ServiceOS.\nSection IV discusses related work and Section V concludes.\n"
  },
  {
    "section": "2015 IEEE International Conference on Cloud Engineering",
    "text": "978-1-4799-8218-9/15 $31.00 © 2015 IEEE\nDOI 10.1109/IC2E.2015.78\n359\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:07:31 UTC from IEEE Xplore.  Restrictions appl\nHPC job\nHPC runtime\nOS\nDevice drivers\nHardware\n(a) Traditional HPC\nOS\nwith\nlimited\nOS-bypass\nHPC job\nHPC runtime\nOS\nDevice drivers\nHardware\n(b) Lean HPC OS\nwith substantial OS-\nbypass\nFigure 1.\nTraditional HPC OS vs. lean OS with most resources exposed\nto the HPC runtime\n"
  },
  {
    "section": "II. ESTABLISHING THE NEED FOR OS SPECIALIZATION",
    "text": "A. Lean OS\nRecent 3.x Linux kernels have more than 15 millions\nlines of code (LoC). With only about 60,000 LoC, the IBM\nCompute Node Kernel (CNK) [1] of Mira, a Blue Gene/Q\nsupercomputer at Argonne National Laboratory, contrasts\nwith Linux by being barely more than a thin layer between\nthe hardware and the HPC job. The CNK LoC number\nincludes the kernel, the management network stack, all the\nsupported system calls, and limited ﬁle system facilities.\nCNK is tailored to achieving the minimum possible interfer-\nence between the HPC job and its hardware usage; in doing\nso, it provides the maximum possible hardware efﬁciency\nto the HPC job. As shown for collective operations [2],\nlow OS interference can make a noticeable difference in\nHPC job performance at extreme scales; in that respect, the\nlightweight nature of kernels such as CNK is a compelling\ncharacteristic for an exascale OS.\nContemporary HPC stacks already selectively bypass the\nOS for direct access to certain devices such as RDMA-\nenabled network cards (Fig. 1(a)). The Argo project goes\nfurther by exposing most hardware resources to the HPC\nruntime which executes as part of the application (Fig. 1(b)).\nOn top of fulﬁlling the minimal interference goal, ofﬂoading\nthe hardware resource management of the application from\nthe OS to the HPC runtime is justiﬁed by the runtime being\nmore knowledgeable about the needs of the HPC application\nthan the OS. The OS as seen by the HPC application running\nin the Argo environment is said to be lean.\nB. Provisioning for Legacy Applications\nA look at the list of the 500 most powerful supercomputers\n(Top500) [3], as of November 2014, shows that such systems\nare overwhelmingly using Linux. In fact, Fig. 2 shows that\nmore than half of the Top500 systems have consistently been\nusing Linux for the past decade, and the share has kept\ngrowing. This observation implies that there is a substantial\namount of HPC code that assumes the existence of familiar\nprogramming APIs such as POSIX, as well as most of the\nsystem calls that Linux programmers take for granted. A\npurely lean OS such as CNK is not a ﬂexible environment\nthat would easily host the massive amount of existing legacy\ncode. For instance, CNK does not support process forking;\nin fact, the 63 system calls offered by CNK represent only\na very limited subset of what is offered by any Linux kernel\nof the last 10 years. CNK does not provide any complex or\n-Heterogeneous sets of compute cores\n-Massive intra-node parallelism\nDeep and complex memory hierarchy\nSmall\nMassively\nParallel\ncores\nSmall\nMassively\nParallel\ncores\nSmall\nMassively\nParallel\ncores\nSmall\nMassively\nParallel\ncores\nSmall\nMassively\nParallel\ncores\nSmall\nMassively\nParallel\ncores\nSmall\nMassively\nParallel\ncores\nSmall\nMassively\nParallel\ncores\nSmall\nmassively\nparallel\ncores\nSmall\nMassively\nParallel\ncores\nBig serial\ncores\nBig serial\ncores\nBig serial\ncores\nAccelerators\nOther special-purpose\ncores\nNUMA\nnode\nCoherence\ndomain\nNUMA\nnode\nAccelerator\nmemory 0\nAccelerator\nmemory 1\nAccelerator\nmemory n\n...\nNVRAM\nNVRAM\nOther\nNUMA\nnode\nCoherence\ndomain\nNUMA\nnode\nNUMA\nnode\nCoherence\ndomain\nNUMA\nnode\nNUMA\nnode\nCoherence\ndomain\nNUMA\nnode\nNUMA\nnode\nCoherence\ndomain\nNUMA\nnode\nNUMA\nnode\nCoherence\ndomain\nNUMA\nnode\nFigure 3.\nResource heterogeneity and complexity in exascale nodes\ndynamic virtual memory to physical memory mapping. Sim-\nilarly, it does not provide all the thread preemption scenarios\nof a vanilla Linux kernel. In particular, the absence of time\nquantum-like sharing can prevent the well-known approach\nto nonblocking operations that is fulﬁlled by hidden agents\nbacked by a middleware-level thread that runs in addition to\nthe application threads [4].\nWhile the overall Argo NodeOS seeks the leanness of a\nlightweight kernel, there is provision for supporting legacy\napplications that need a full Linux environment. An aspect\nof the OS specialization is to provide side-by-side in the\nsame node:\n• an OS environment for HPC applications that require a\nfully-ﬂedged Linux kernel,\n• an OS environment that is lean. As much as possible,\nthat environment would allow the HPC runtime to get\ndirect access to the hardware resources with little or no\nOS interference.\nC. Provisioning for Heterogeneous Resources\nSubstantial changes in hardware are expected for exascale\nsystems. The deeper and more complex memory hierarchy\nwill expose multiple coherence domains and NUMA nodes\nin addition to incorporating NVRAM. The envisioned mul-\ntiple coherence and NUMA domains are the consequence\nof the expected increase in CPU core density and processor\nheterogeneity at the node level. That resource complexity\n(Fig. 3) requires a hardware management rigor that is best\nleft to a fully-ﬂedged kernel such as Linux. Hardware drivers\nare not always open-source, and hardware vendors are reluc-\ntant to provide drivers for niche operating systems. However,\nhardware drivers, especially for HPC-directed devices, are\nroutinely provided for Linux because it is a well-established\nOS in HPC [3]. As a consequence, the Argo NodeOS\nrequires a fully-ﬂedged Linux kernel as its underlying OS\nbut also as one of its OS specializations.\nD. Provisioning for Different Compute Needs\nNob all HPC jobs have the same needs, and their un-\nderlying processes do not exhibit the same behavior. HPC\njobs could differ by communication patterns, I/O require-\nments, storage needs, computation-intensiveness, etc. For\ninstance, a job that performs large numbers of small message\n360\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:07:31 UTC from IEEE Xplore.  Restrictions appl\nFigure 2.\nPercentage of the Top500 supercomputers using Linux\nexchanges would be more sensitive to the latency or the\nleanness of the network stack than one that does heavier data\ntransfers. While some jobs require massive external storage\nfor the sake of reading input or generating output, other\nexpect to start and complete entirely in memory. Some HPC\napplications could require the ability to oversubscribe the\nnode with more threads than CPU cores so as to efﬁciently\nachieve an optimal balance between blocking operations\nand asynchronous needs. Taking that disparity of needs into\naccount, there is no single set of lean OS characteristics\nthat could fulﬁll all the possible compute needs. Actually,\nany such single set of characteristics is bound to lead to\nover-provisioning of features that could hardly keep the OS\nlean. As a result, there is not just a need for distinction\nbetween a fully-ﬂedged Linux environment and a leaner OS\nenvironment providing low-interference HPC; there is also\na need for specializing between various lean compute envi-\nronments. Thus, the OS specialization in the Argo project\nallows OS features to be enabled or disabled according to\nthe desired level of leanness expressed by the HPC job being\nrun.\nDistinct compute needs exist because different jobs are\ndissimilar, but distinct compute needs can exist for a single,\ncomposite job as well. While it is uncommon in HPC to col-\nlocate two or more distinct jobs on the same compute node,\nand most jobs have the same compute requirements for all\ntheir processes on the same node, there is a growing trend of\nmodular HPC jobs where a compute aspect collaborates with\na post-processing aspect in real time. An example of such\npost-processing could be co-visualization. As the compute\nand the co-visualization aspects do not have the same needs,\nin spite of being part of the same job, their respective optimal\ncompute environments could differ widely. The same job can\ntherefore require its processes to be distributed over several\ndistinct OS specializations, even within the same node.\nIII. OS SPECIALIZATION VIA COMPUTE CONTAINERS\nAs shown in Fig. 4, the NodeOS is made of a unique\nServiceOS and of one or more Compute Containers. The\nServiceOS—which is the default OS specialization—boots\nthe node, initializes the hardware resources, and provides\nmost of the legacy API and services expected by an HPC\njob that assumes a fully-ﬂedged Linux environment. An HPC\napplication with lean OS requirements does not execute over\nthe ServiceOS; instead, it executes over one or multiple\nCompute Containers which are OS specializations distinct\nfrom the ServiceOS.\nA single-kernel approach is used to achieve the special-\nization. The ServiceOS is the default host OS, and the\nCompute Containers are the guest entities based on Linux\ncgroups. Compute Containers get their hardware allocation\nin bulk from the ServiceOS and then expose these resources,\nwith little interference, to the user-space HPC runtime.\nFor instance, scheduling policies, CPU core allocation, and\nmemory management are to a large extent delegated to the\nruntime. The ServiceOS hosts a resource manager which ma-\nnipulates cgroups and resource controllers. The ServiceOS\nalso deals with storage management, job management, and\nsystem call forwarding for select calls that are by default\ndisabled in the Compute Containers.\nCompute Containers have exclusive ownership of certain\nresources that they are granted by the ServiceOS; even the\nServiceOS refrains from using these resources during the\nlifetime of the owning Compute Containers. A Compute\nContainer can be as fully-ﬂedged as the ServiceOS since\nthe same kernel is shared between the two kinds of OS\nspecializations. However, by default, a Compute Container\nis created with many features disabled, so as to achieve a\ncertain default level of leanness (Fig. 5). Further features can\nbe selectively enabled or disabled by providing parameters\nto the resource manager either at the creation time of a\n361\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:07:31 UTC from IEEE Xplore.  Restrictions appl\nApplication\nVendor RTS\nConcurrency RTS\nLegacy API\nServiceOS\nResource manager\nJob Manager\nStorage manager\nCall forwarding\nAccelerator\nLow-power\nCPU core\nCPU core optimized\nfor serial execution\nRTS = Runtime system\nLegend\nComputeContainer ComputeContainer ComputeContainer\nFigure 4.\nOS specialization\nComputeContainer\nServiceOS\nComputeContainer\nComputeContainer\n...\nFully-fledged\nLean\nFeaturea\nFeatureb\nFeaturez\n...\nFeaturec\nFeaturea\nFeatureb\nFeaturez\n...\nFeaturec\nFeaturea\nFeatureb\nFeaturez\n...\nFeaturec\nFigure 5.\nDifferentiated behavior between ServiceOS and Compute\nContainers\nCompute Container or during its lifetime.\nThe concept of a container on Linux usually comes with\nboundaries similar to virtual machine isolation. Compute\nContainer, as put forth by the Argo NodeOS, does not put\nany emphasis on namespace isolation. Processes running\non distinct Compute Containers share the same PID space,\nﬁle system, and network domains. In fact, the NodeOS\nproactively needs Compute Containers to be permeable,\nso the same job using multiple Compute Containers can\nseamlessly have its processes communicate without crossing\nany virtual node boundary. The emphasis in this work is\nput on resource partitioning and differentiated behaviors by\nmeans of feature activation/deactivation.\nA. Provisioning for Compute Container Feature Selection\nThe mechanism depicted in Fig. 5 is already partially\nfeasible on a vanilla Linux kernel. For instance, by disabling\nload balancing in the cpuset resource controller for a cgroup,\nit is possible to reduce the OS interference over the subset\nof CPU cores dedicated to a Compute Container. When the\nServiceOS is kept away from the Compute Container CPU\ncores via the isolcpus kernel parameter, the deactivation\nof load-balancing noticeably reduces OS interference and\nleaves the CPU core management to the HPC runtime.\nThese available features were already an important steps in\nrealizing a certain level of Compute Container leanness. The\nlevel of feature activation/deactivation offered by a vanilla\nLinux kernel is insufﬁcient though, and we are developing\nadditional mechanisms to further Compute Container Spe-\ncialization.\nA new scheduling class is under development for Compute\nContainers. It is optimized for small per-core process and\nthread counts. It disables load balancing and replaces process\npreemption with cooperative scheduling, thus providing a\nconsiderably more predictable performance to workloads\noptimized for a single software thread per hardware thread,\nas is common in HPC.\nThe memory registration required by Remote Direct\nMemory Access (RDMA) is known to be costly [5]. How-\never, if a process can have its virtual address space pre-\nregistered, memory registration would become virtually free\nat run time and would lead to an overall performance\nboost for the RDMA operations that are common in su-\npercomputing inter-node communication. For a process to\nbeneﬁt from such static mapping, it must own the physical\nmemory frames exclusively. However, with the existing\ncpuset resource controller, the physical memory ownership\ngranularity is the NUMA node. Thus, in a uniform memory\naccess (UMA) architecture such as the Xeon Phi, where\nthe whole memory is considered a single memory node, it\nbecomes impossible for a collocated Compute Container to\nachieve the aforementioned exclusive memory ownership.\nWe are adding a new resource controller that breaks down\nphysical memory into smaller units that can be granted\nexclusively to Compute Containers (Fig. 6). The units are\nsized in multiples of a page size. For a UMA architecture\nsuch as Xeon Phi, Fig. 6(a) shows how the new feature\ncan allow memory partitioning. The equivalent of the same\npartitioning for NUMA architectures is shown in Fig. 6(b).\nFor NUMA architectures, the ﬁner-grained memory units\nof a Compute Container do not have to be contiguously\nallocated from the same NUMA node. For instance, in\nFig. 6(b), ComputeContainer1 does not have to get all its\nﬁner-grained memory units from NUMA node 2, especially\nif it hosts a process that needs to use interleaved memory\nallocation policy.\nFor certain threads or processes, the runtime can have very\nlow tolerance for preemption; and would therefore expect to\nbe hosted, as much as possible, in a Compute Container\nthat prevents the scheduling of OS-related kernel threads.\nWe are providing an OS specialization knob that can disable\nselect per-CPU core kernel threads on the cores used by any\nsuch Compute Container. This OS specialization feature is\nappropriate when the functionality provided by a disabled\nper-CPU core kernel threads is not required for the set of\nfunctionalities needed for a lean Compute Container.\n"
  },
  {
    "section": "Iv. Related Work",
    "text": "While the idea of isolating processes from each other is\nnot new (see chroot or jails from BSD), containers are\nbecoming a popular solution in the commercial space as a\nlightweight virtualization technology. Docker [6] or LXC [7]\nprovide a framework to specify, launch, and control an\nenvironment isolated from the base system through various\n362\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:07:31 UTC from IEEE Xplore.  Restrictions appl\nPartitioning into\nfiner-grained\nmemory units\nUnique\nunpartitionable\nphysical\nMemory\n(e.g. Xeon Phi)\nComputeContainer1\nComputeContainer2\nComputeContainer0\nComputeContainers can have guaranteed page frames\nStatic virtual-physical mapping => Free memory pinning for faster RDMA\nPhysical\nmemory\nunit\nComputeContainer0\nComputeContainer1\nComputeContainer2\n(a) Finer memory unit (UMA)\nPartitioning into\nfiner-grained\nmemory units\nNUMA\ndomain\nHardware\nNUMA\nnode 0\nHardware\nNUMA\nnode 2\nNUMA\ndomain\nHardware\nNUMA\nnode 1\nHardware\nNUMA\nnode 3\nComputeContainer1\nComputeContainer2\nComputeContainer0\nComputeContainers can have guaranteed page frames\nStatic virtual-physical mapping => Free memory pinning for faster RDMA\nComputeContainer0\nComputeContainer1\nComputeContainer2\nPhysical\nmemory\nunit\n(b) Finer memory unit (NUMA)\nFigure 6.\nBreaking physical memory nodes in ﬁner logical nodes that can exclusively be owned by Compute Containers\nnamespaces and to manage resources between those envi-\nronments. However, these technologies aim at different goals\nthan us. Their containers are focused on isolation—a process\ncannot access resources outside its container—and resource\nsharing management—how much memory or CPU to give to\na given container—while we focus on exclusive partitioning\nof heterogeneous resources and OS specialization inside con-\ntainers. Furthermore, in our context, all processes running\nin a single node are part of a single application. Thus, the\njob does not beneﬁt from these processes being hosted in\nisolated containers where communication must cross virtual\nnode boundaries instead of using straighforward efﬁcient\nmeans such as shared memory.\nCoreOS [8] is a minimalist OS based on the Linux kernel,\nusing Docker at its core to run each application inside a\ncontainer and providing additional software to control a\ncluster deployment. As HPC systems usually depend on\narchitecture- and vendor-speciﬁc base system images, the\nArgo project’s intent is to build loosely integrated compo-\nnents (Fig. 4) that can later be deployed and conﬁgured on\na large variety of machines instead of a tightly integrated\nsolution.\nIn recent years, several research projects have studied rad-\nical changes to the OS stack to adapt to large, heterogeneous\nor homogeneous resources on single nodes. Corey [9] put\nforth a single kernel that delegates speciﬁc kernel functions\nto speciﬁc cores, and allowing applications to specify when\nkernel data structures should be shared and across which\npart of the system. Multikernels are also being investigated\nand put forth by many others. Barrelﬁsh [10] for instance\nis implemented as a distributed system, with one kernel\ninstance per core and efﬁcient communication between the\ndifferent kernel instances. Another instance of multikernel\napproach is presented by Baumann et al. [11], with the\ngoal, among other concerns, of showing that multiple OSes\nover multiple partitions of the node hardware resources\ncan offer a means of enforcing intra-node scalability by\nlearning from inter-node distributed systems. Mint [12] and\nPopcorn [13] are two other multikernels which are meant\nfor managing parallel intra-node resources with perfectly\nautonomous kernels. Popcron Linux allows the independent\nkernels to provide a seamless feeling of single system image\nwhere processes can easily cross kernel boundaries. Finally,\nTwin-Linux [14] appears as an approach to OS specializa-\ntion via multiple fully-ﬂedged kernels meant for different\nworkloads. We argue here for a middle-ground approach,\nwith Compute Containers using dedicated resources, having\nspecial conﬁgurations, and some kernel functions forwarded\nto the ServiceOS. Furthermore, by using the Linux kernel\nas a basis, we ensure compatibility with vendor-controlled\n363\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:07:31 UTC from IEEE Xplore.  Restrictions appl\narchitectures. One crucial difference between the aforemen-\ntioned multikernels and the Argo NodeOS is the need to\nhost the same job over multiple OS specializations at low\ncost.\n"
  },
  {
    "section": "V. Conclusion",
    "text": "The extreme scale of next generation supercomputers\nwill noticeably beneﬁt from reduced OS interference in the\nHPC job execution. Furthermore, hardware resource micro-\nmanagement is best ofﬂoaded from the OS to the HPC run-\ntime, for the latter is more informed of the needs of the HPC\njob. These two goals are reminiscent of lightweight kernels\nsuch as the Blue Gene CNK. However, the complex and\nheterogeneous nature of the next generation supercomputing\nsystems and the need to support the massive amount of\nlegacy applications that assume fully-ﬂedged POSIX APIs\nand a variety of system services establish the relevance of\nan existing and well-adopted OS like Linux. The NodeOS\naspect of the Argo project is an ongoing effort to simul-\ntaneously provide the leanness of a lightweight kernel for\nextreme performance and the richness required to support\nlegacy applications and disparate hardware. We are leverag-\ning the cgroups and resource controller interface of Linux to\nimplement OS specialization on top of a single kernel. New\nresource controllers are being added to complement those\nthat are provided by the mainline Linux kernel.\nACKNOWLEDGEMENTS\nThis work was supported by the Ofﬁce of Advanced\nScientiﬁc Computer Research, Ofﬁce of Science, U.S. De-\npartment of Energy, under Contract DE-AC02-06CH11357.\nREFERENCES\n[1] T. Budnik, B. Knudson, M. Megerian, S. Miller, M. Mundy,\nand W. Stockdell, “Blue Gene/Q resource management archi-\ntecture,” in Many-Task Computing on Grids and Supercom-\nputers (MTAGS), 2010 IEEE Workshop on, Nov. 2010.\n[2] P. Beckman, K. Iskra, K. Yoshii, and S. Coghlan, “The inﬂu-\nence of operating systems on the performance of collective\noperations at extreme scale,” in Cluster Computing, 2006\nIEEE International Conference on, Sep. 2006, pp. 1–12.\n[3] “Top500,” http://www.top500.org/.\n[4] T. Hoeﬂer and A. Lumsdaine, “Message progression in par-\nallel computing – to thread or not to thread?” in Proceedings\nof the 2008 IEEE International Conference on Cluster Com-\nputing (Cluster), 2008, pp. 213–222.\n[5] F. Mietke, R. Rex, R. Baumgartl, T. Mehlan, T. Hoeﬂer, and\nW. Rehm, “Analysis of the memory registration process in\nthe Mellanox InﬁniBand software stack,” in Euro-Par 2006\nParallel Processing, ser. Lecture Notes in Computer Science,\nW. Nagel, W. Walter, and W. Lehner, Eds.\nSpringer Berlin\nHeidelberg, 2006, vol. 4128, pp. 124–133.\n[6] “Docker,” http://www.docker.com/.\n[7] “LXC: Linux container tools,” http://www.linuxcontainers.\norg/.\n[8] “CoreOS: Linux for massive server deployments,” http://\nwww.coreos.com/.\n[9] S. Boyd-Wickizer, H. Chen, R. Chen, Y. Mao, F. Kaashoek,\nR. Morris, A. Pesterev, L. Stein, M. Wu, Y. Dai, Y. Zhang,\nand Z. Zhang, “Corey: An operating system for many cores,”\nin Proceedings of the 8th USENIX Conference on Operating\nSystems Design and Implementation (OSDI ’08), 2008, pp.\n43–57.\n[10] A. Sch¨upbach, S. Peter, A. Baumann, T. Roscoe, P. Barham,\nT. Harris, and R. Isaacs, “Embracing diversity in the Bar-\nrelﬁsh manycore operating system,” in Proceedings of the\nWorkshop on Managed Many-Core Systems, 2008.\n[11] A. Baumann, P. Barham, P.-E. Dagand, T. Harris, R. Isaacs,\nS. Peter, T. Roscoe, A. Sch¨upbach, and A. Singhania, “The\nMultikernel: A new OS architecture for scalable multicore\nsystems,” in Proceedings of the 22nd ACM SIGOPS Sympo-\nsium on Operating Systems Principles (SOSP ’09), 2009, pp.\n29–44.\n[12] Y. Nomura, R. Senzaki, D. Nakahara, H. Ushio, T. Kataoka,\nand H. Taniguchi, “Mint: Booting multiple Linux kernels on a\nmulticore processor,” in Broadband and Wireless Computing,\nCommunication and Applications (BWCCA), 2011 Interna-\ntional Conference on, Oct. 2011, pp. 555–560.\n[13] A. Barbalace, B. Ravindran, and D. Katz, “Popcorn: A\nreplicated-kernel OS based on Linux,” in Proceedings of the\nLinux Symposium, 2014, pp. 123–138.\n[14] J. Adhiraj, P. Swapnil, N. Mandar, R. Swapnil, and P. Kiran,\n“Twin-Linux: Running independent Linux kernels simultane-\nously on separate cores of a multicore system,” in Proceedings\nof the Linux Symposium, 2010, pp. 101–107.\n364\nthorized licensed use limited to: CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING - CDAC - PUNE. Downloaded on June 17,2025 at 04:07:31 UTC from IEEE Xplore.  Restrictions appl\n"
  }
]