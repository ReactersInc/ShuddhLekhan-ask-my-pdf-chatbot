[
  {
    "section": "Unknown",
    "text": "TACC: A Secure Accelerator Enclave for AI Workloads\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\n"
  },
  {
    "section": "Jianping Zhu, Rui Hou∗, Dan Meng",
    "text": "State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences;\nSchool of Cyber Security, University of Chinese Academy of Sciences;\n{zhujianping,hourui,mengdan}@iie.ac.cn\n"
  },
  {
    "section": "Abstract",
    "text": "We present a Secure Accelerator Enclave design, which in-\ncludes heterogeneous accelerator running AI workloads into\nthe protection scope of Trusted Execution Environment,\ncalled TACC (Trusted Accelerator). TACC supports dynamic\nuser switching and context clearing of accelerator enclave\nfrom the microarchitecture level; The physical isolation of in-\npackage memory (3D chip package) and off-package memory\nis used to realize the full stack (from hardware to software)\nisolation of enclave internal running memory and external\nciphertext memory; It is also equipped with independent\nhardware AES-GCM module (including DMA engine) to be\nresponsible for the interaction between internal and exter-\nnal memory. On a FPGA development board containing Xil-\ninx xc7z100-ffg900-2 chip, we implemented two versions of\nTACC prototypes: FAT (144 multipliers and 48 blockRAMs)\nand SLIM (36 multipliers and 12 blockRAMs). We deployed\nand ran the RepVGG inference neural networks on them re-\nspectively under different batch sizes. The average overhead\nof our security mechanism is no more than 1.76%.\nACM Reference Format:\nJianping Zhu, Rui Hou∗, Dan Meng. 2022. TACC: A Secure Ac-\ncelerator Enclave for AI Workloads. In The 15th ACM Interna-\ntional Systems and Storage Conference (SYSTOR ’22), June 13–15,\n2022, Haifa, Israel. ACM, New York, NY, USA, 14 pages. https:\n//doi.org/10.1145/3534056.3534943\n"
  },
  {
    "section": "1 Introduction",
    "text": "In recent years, the field of architecture has presented a situ-\nation where CPU, GPU, and accelerator are three pillars, and\nthe accelerator architecture for AI workloads will play an in-\ncreasingly important role. As small as the on-chip integrated\nheterogeneous architecture of the mobile phone chip (for\nexample, the Apple A15 Bionic chip [1] is composed of 6-core\n∗Corresponding author: Rui Hou (hourui@iie.ac.cn)\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\n© 2022 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-9380-5/22/06.\nhttps://doi.org/10.1145/3534056.3534943\nCPU + 5-core GPU + 16-core Neural Engine), as large as the\ndistributed heterogeneous computing of the cloud platfor-\nm/data center Architecture (for example, Google Cloud [21]\nsupports three types of computing resources: CPU clusters,\nGPU clusters and TPU clusters). Trusted Execution Environ-\nment (TEE) only based on CPU architecture can no longer\nmeet the security requirements of the heterogeneous ar-\nchitecture. The industry urgently needs to cover the entire\nheterogeneous architecture with the protection scope of the\nsecure enclave.\nAlthough there have been some related studies of hetero-\ngeneous isolated execution environments that have explored\nthe \"CPU + GPU\" combination mode. However, these works\nhave not yet involved the research on the trusted reconstruc-\ntion of the emerging accelerator architecture itself.\nFor example, Graviton [58] builds GPU TEE by modifying\nthe command processor in the GPU chip, and requires the\nSGX [40] enclave on the CPU to host the user application\nand GPU runtime. HIX [28] extends the protection scope of\nSGX enclave to commodity GPUs by modifying components\nsuch as PCIe root complex controller and MMU page table\nwalker in the CPU chip. Telekine [25] is based on a GPU TEE\nsimilar to Graviton, and moves the user application and GPU\nruntime from the cloud platform to the client to improve\nsecurity. All above works require the heavy GPU software\nstack (TensorFlow/Pytorch + CUDA runtime + GPU driver)\nto be ported to the CPU enclave, but we have not yet found an\nopen source case of successful porting. HETEE [67] is based\non off-the-shelf CPU and GPU chips, and uses a physically\nsealed chassis to construct a rack-scale heterogeneous TEE.\nAlthough HETEE does not need to rely on the CPU enclave\nor port the GPU runtime, its user switching needs to rely on\nthe firmware of the commercial GPU to be able to clear the\ncontext by rebooting.\nIn this paper, we start from the unique (on-chip buffer open\nto programmers and compilers) memory access hierarchy of\naccelerators running AI workloads (such as convolutional\nneural networks), and design a Secure Accelerator Enclave\narchitecture called TACC. TACC adds an independent hard-\nware DSC (Device Security Controller) to the accelerator,\nwhich is responsible for the context clearing of the accelera-\ntor’s on-chip buffer array, as well as the allocation, recycling\nand destruction of the plaintext memory in the enclave. This\npaper also uses the physical isolation of the chip’s 3D package\n58\nThis work is licensed under a Creative Commons Attribution International 4.0 License.\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nJ. Zhu et al.\nto achieve full-stack isolation (from hardware to software)\nbetween the plaintext running memory inside the TACC\nenclave and the external ciphertext memory. And we equip\nTACC with independent hardware AES module (including\nDMA engine) responsible for the interaction between inter-\nnal and external memory.\nThis paper constructs a heterogeneous TEE in the \"CPU\n+ Accelerator\" mode, which has the following advantages\ncompared with the previous \"CPU + GPU\" mode:\nFirst, the explicit memory access hierarchical structure of\nthe accelerator architecture is conducive to the upper-layer\nsoftware intent to directly reach the underlying hardware.\nThe use of on-chip buffer array rather than just following\nthe cache hierarchy in the traditional CPU architecture and\nopen it to programmers and compilers is a milestone in-\nnovation for emerging accelerators running AI workloads.\n(Such as, On-chip buffers in DianNao series [12, 13, 37],\nCambricon-X [63], DEEPHi [49], and Google TPU [31]; On-\nchip Scratchpad Memory in Cambricon [39] replaces the\nhuge and energy-consuming Vector Register-file in GPU and\nalso functions as data cache.) More importantly, the physical\naddress space of the on-chip buffer array is visible to pro-\ngrammers and compilers, allowing TACC to perform simple\nand intuitive security boundary management, which is more\nsecure than the transparent and complex cache hierarchy in\ntraditional CPUs/GPUs. (The transparent and complex vir-\ntualizated memory access hierarchy on the CPU has many\nside-channel hidden dangers. For example, the transparent\ncache hierarchy is shared by different user programs and\nthe LLC (Last Level Cache) is shared by multiple physical\ncores [22, 27, 38, 62, 64].) Second, the accelerator can be cus-\ntomized more freely to obtain a lightweight software stack,\nwhich makes it easy to port to CPU enclave (such as SGX)\nfor hosting, and maintain a relatively small TCB (Trusted\nComputing Base). Finally, accelerators for AI workloads have\nmore urgent security requirements in frontier applications\nsuch as Image Recognition and Target Detection. Therefore,\nthe protection scope of our TACC can more accurately focus\non security-sensitive applications.\nContributions of this paper are summarized as follows:\n•A Secure Accelerator Enclave architecture named TACC.\nWe add an independent hardware module called DSC (De-\nvice Security Controller) into accelerator for context clearing.\nOnly DSC can access the device Root of Trust (Endorsement\nKey), and randomly derive the RSA Attestation Key and\nAES Symmetric Key of different user enclaves. DSC ensures\nthat when the accelerator dynamically switches users to cre-\nate and destroy enclaves, the on-chip buffers are physically\ncleared by writing zeros. And every time the device is pow-\nered on again, the DSC secure boot flow clears all on-chip\nbuffers and in-package memory by default.\n•Isolated memory management mechanisms for TACC’s in-\npackage memory and external memory.\nThe in-package memory is used as the plaintext running\nmemory of the enclave, and the external memory is used as\nthe ciphertext communication memory between the accel-\nerator device and the host. The communication memory is\nvisible to the insecure world and managed by the traditional\nhost OS kernel through the CPU’s IOMMU (Input/Output\nMemory Management Unit). While the plaintext running\nmemory is only visible inside the enclave. The TACCmalloc/-\nTACCfree security primitive instructions that we specially\ndesigned for the allocation and recycling of the running\nmemory. (The DSC is responsible for the execution. Before\nallocating a new piece of memory to the enclave, the area is\ncleared, and the memory space is also cleared after reclaim-\ning). The management of these two memories is separated\nfrom programming and compilation to hardware execution\n(the communication memory is accessed by the host CPU\nthrough the Xilinx xdma engine, while the running memory\ncan only be read and written by the TACC core). The inter-\naction between the two memories can only be forwarded\nthrough the hardware AES-GCM module (with packet in-\ntegrity and ID check). The AES-GCM module receives the\nkeys from the DSC and is only controlled by the DSC.\n•TACC prototypes on FPGA and run inference neural networks.\nWe implement two versions of the TACC accelerator pro-\ntotypes on the FPGA development board (including Xilinx\nxc7z100-ffg900-2 chip, 1GB PL side memory half used for the\nciphertext area, the other half for the plaintext area, and PCIe\n2.0 x8 device interface). The FAT-core execution array con-\ntains 144 multipliers and the on-chip buffer array contains 48\nblockRAMs totaling 1600 KB; the SLIM-core execution array\ncontains 36 multipliers and the on-chip buffer array con-\ntains 12 blockRAMs totaling 1552 KB. We deploy and run the\nRepVGG-A0 [17] inference neural network on them, and ob-\ntain performance comparison of the TACC prototypes with\nand without hardware encryption and decryption under dif-\nferent batch sizes. The average overhead of our AES128GCM\nmodule in the evaluation cases does not exceed 1.76%.\nRoadmap. The rest of this paper is organized as follows:\nSec. 2 is the background, which mainly includes the differ-\nences between accelerator and CPU/GPU memory access\nhierarchies, and the comparison of TACC and existing TEEs;\nSec. 3 provides the TACC design including the threat model;\nSec. 4 describes the TACC prototype implementation; Sec. 5\nreports the performance evaluations; Sec. 6 elaborates our\nsecurity analysis; Sec. 7 surveys the related works and Sec. 8\nconcludes the paper.\n59\nTACC: A Secure Accelerator Enclave for AI Workloads\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\n"
  },
  {
    "section": "Background",
    "text": "2.1\nThree design principles of TEE\nConfidentiality: Secrets are invisible to the outside. The\nuser’s sensitive data inside the enclave, as well as its calcula-\ntion process, are not allowed to be observed or perceived by\nuntrusted third parties (including privileged OS). And the\ncommunications between user and enclave are encrypted.\nIntegrity: The content is not allowed to be tampered with\nby others. The content of the interaction between the user\nand the enclave, as well as the data and processing within the\nenclave, are not allowed to be modified by untrusted third\nparties. Or it can detect tampered communication contents.\nAuthentication: The user and the enclave can authenticate\neach other and build secure channels. The trusted platform\nhas secure boot and remote attestation mechanisms, which\ncan prove to the user that the desired code and data are\ncorrectly deployed in the enclave.\n"
  },
  {
    "section": "2.2",
    "text": "Memory hierarchy differences between\nAccelerator and CPU/GPU\nPredecessors made great efforts to build TEE on the CPU,\nand obtained security primitive support from the underlying\nhardware by modifying the CPU micro-architecture compo-\nnents (such as TLB, page table walker, MMU, etc.). We believe\nthat when building a heterogeneous TEE whose main goal is\nto protect AI workloads, it is necessary to add security prim-\nitive support from the underlying hardware to the emerging\naccelerator micro-architecture. The processor’s memory ac-\ncess hierarchy is the key underlying hardware for secure data\ninteraction and trusted computing. Therefore, this paper will\nadd an independent hardware Device Security Controller to\nthe accelerator’s unique memory access hierarchy to pro-\nvide security primitive support. Here we briefly compare the\nmemory access hierarchy differences between popular AI\naccelerators and traditional CPUs/GPUs:\nPer-enclave page tables in the CPU enclave design. The\nper-enclave page table lookup mechanism in Sanctum [15]\nphysically separates the memory access management inside\nand outside the enclave, thereby protecting the confidential-\nity and integrity of the internal memory space. And, CPU\nTEEs (both Sanctum and SGX [14]) have designed complex\nsecurity context saving and restoring mechanisms for flexi-\nble environment switching (maintaining high utilization of\nhardware resources). However, for the efficiency of inter-\nnal and external interaction, CPU TEEs may allow part of\nmemory hierarchy (LLC, shared TLB, etc.) directly shared\nbetween internal and external, which leaves the enclave a\nvariety of side channels [6–9, 54, 59]. In addition, if the CPU\nTEE implements unencrypted address bus to access off-chip\nDRAM, and there are also off-chip side channels [34]. The\nresearch on the CPU TEE security enhancements is contin-\nuously advancing in academia, such as Keystone [35] and\nPenglai [18]. Although the user application and TACC run-\ntime of our solution need to be hosted inside the CPU enclave,\nthe construction and security research of the CPU TEE itself\nis beyond the scope of this paper.\nTLB\nGPC\nL2 $\nPer-GPC\nmicrocontroller\nPTWs\nFar\npage-\nfaults\nMSHRs\nGMMU\nShared\nTLB\nGPU\nCommand Processor\n(Channel engine)\nSM\ncoalescer\nGDDR/HBM\nDDR4\nCPU\nUnified Virtual Addressing Memory\nGPU\npage-\nfaults\nruntime/\ndriver\nupdates\nupdates\nUntrusted OS\ninterrupts\nUser APP\nUser runtime\ncalls\nMMIO\nXdma\nFigure 1: A brief GPU unified memory management.\nGPU memory hierarchy is managed by its device dri-\nver resident in untrusted host OS. Due to the characteris-\ntics of many-core and massive multi-threading, GPU has a\nmore complicated memory hierarchy than CPU. And as mod-\nern GPUs evolve in the direction of Unified Virtual Address-\ning Memory, both performance and programming conve-\nnience has been significantly improved [46, 48, 50, 52, 53, 65],\nbut at the same time it has also deepened the dependence of\nGPU memory management on untrusted OS. Fig. 1 shows a\nbrief resource management block diagram of discrete GPU.\nIn NVIDIA Volta, L1 cache is virtually addressed (equiva-\nlent to scratchpad memory), while L2 cache is physically\naddressed. The GPU uses two-level TLBs when accessing\nthe L2 cache [29]. Because the GPU is a slave device, the\nGPU memory and register status are managed by the device\ndriver/runtime registered and running in the host OS kernel\nspace. Almost all modern GPUs implement unified memory,\nand the memory of the CPU and GPU are paged and recy-\ncled by the OS, and interact through the PCIe bus. After the\nmapping is established, the GPU driver use the MMIO path\nto send context requests such as the channel descriptors,\npage directories, and page tables of the current kernel to the\nCommand Processor for context update. After the context\nis updated, the xdma copy engine on the device performs\npage-granular physical memory copy, and the IOMMU on\nthe CPU is responsible for address checking. If page-faults\nare encountered during kernel execution, and the missing\npage is not in the GPU local memory (far page-faults), the\nhost OS is interrupted to request paging services.\nWhen the host OS is untrustworthy, the GPU’s memory\nand state control registers will not only be leaked and tam-\npered with [30, 43, 47, 60, 66], they can even be used to attack\n60\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nJ. Zhu et al.\nthe memory space of other users on the host[68]. In addition,\nthe interactive communication between host CPU and device\nGPU also has the risk of side channel leakage [36]. NVIDIA\nGPU performs context switching, which actually requires a\ncluster of microcontrollers [20]: Each GPC is configured by\nan on-chip microcontroller. And a hub microcontroller (Com-\nmand Processor), which broadcasts the operations on all the\nGPC-dedicated microcontrollers. In addition, the bus on the\nNVIDIA GPU board has a complex topology, and there are\nalso a variety of board-level management microcontrollers,\nand the control system composed is called Falcon [44, 45].\nAll this microcontrollers are running firmware (microcode),\nwhich also need to be loaded into their internal dedicated\nmemory by the GPU driver in advance. NVIDIA has been\nverifying GPU firmware signatures since 2014 and denying\nunofficial firmware access to memory, which has largely\nalleviated GPU microcode attacks [68].\n...\nPE\nAdd  tree\nAccumulator\nMatrix\nmultipliers\nActivator\nScheduler\nIB\nSB\nNBin\nNBout\nBuffer  Controller  (with  DMAs)\nLocal  memory\n"
  },
  {
    "section": "Accelerator Core",
    "text": "Local  MMU  ?\nMemory  Controller\nIO  xdma\nFigure 2: Popular Accelerator memory hierarchy.\nPopular Accelerator memory access hierarchy. We re-\nfer to popular accelerator designs (such as DianNao series [12,\n13, 37], Cambricon [39, 63], TPU [31]), and summarize a brief\naccelerator memory hierarchy in Fig. 2. Accelerators usu-\nally do not have complex multi-level caches, TLBs, instead\nthey use on-chip buffers to cache input and output data. Di-\nanNao series accelerators buffer the weight parameters of\nneural networks in SB (Synapses Buffers), and buffer the\ninput/output feature values of a layer of network in NBin/N-\nBout (Neurons Buffers). Scheduler is responsible for fetch\nand decode instruction stream from IB (Instruction Buffer)\nto control the Buffer Controller to organize input data into\nmultiple tiles and dispatch them to PEs (Process Elements)\nin parallel, thereby controlling the concurrent execution of\nPE-array. The output of PEs is also collected by Buffer Con-\ntroller and written back to Buffers. After the data in the\ncurrent Buffers are processed, the accelerator writes a batch\nof results back to local memory and reads the next batch of\ninput data (actually using the double-buffering mechanism,\nmemory accesses and PE calculations can be overlapped).\nAfter the local memory collects all the results required by\nthe current task, the accelerator notifies the host CPU to take\nthe results, and then waits for the host to send a new task.\nAlthough, some local MMU designs for accelerators have\nrecently appeared [24, 26], providing a unified memory space\nsimilar to GPUs, supporting address space virtualization and\non-demand paging on accelerators, so as to achieve program-\nming convenience and improve resource utilization. But they,\nlike GPUs, rely on OS support. In this paper, it is assumed\nthat the OS is not trustworthy, so the internal memory man-\nagement of the TACC enclave needs to be isolated from the\nuntrusted OS (and other software). See Section 3 for details.\nEach enclave of TACC has exclusive physical Accelerator\ncore, and each physical core can only cooperate with the\nDSC context clearing mechanism to time-share the same\nresources, and is a trade-off between security and resource\nutilization. TACC does not implement complex caches and\nTLBs, instead it implements on-chip buffers all direct ad-\ndressing, and is open to programmers and compilers, thus\nallowing the upper-level software to simply and intuitively\ncontrol the secure physical boundary.\nPrevious works (such as Graviton [58], HIX [28], Telekine\n[25]) explores the heterogeneous TEEs of \"CPU + GPU\",\nwhich require porting the GPU software stack to the SGX\nenclave to address the security risks of GPU: GPU mem-\nory hierarchy is managed by its device driver resident in\nuntrusted host OS. However, none of these works have yet\ninvolved the research on trusted reconstruction of the emerg-\ning accelerator micro-architecture itself, and the memory\nmanagement of existing accelerators is also controlled by\nthe untrusted host OS like GPUs. The TACC in this paper\nexplores the heterogeneous TEE of \"CPU + Accelerator\", and\nis the first accelerator enclave design that attempts to sup-\nport security context switching from the micro-architecture\nlevel. The TACC design utilizes the physical isolation of the\ninternal memory of the chip 3D package and the memory\noutside the package, and realizes the isolation of the inter-\nnal plaintext running memory and the external ciphertext\nmemory of the accelerator enclave, and is equipped with an\nindependent hardware AES-GCM module responsible for the\ninteraction between the internal and external memory. This\nensures that the memory access pattern of the accelerator\ncore inside the enclave is not visible to the external untrusted\nhost OS, thus ensuring confidentiality and integrity. In or-\nder to more systematically present the differences between\nTACC and previous works, we have counted the protection\nscope comparison between TACC and the main related TEEs,\nas shown in Table 1.\n61\nTACC: A Secure Accelerator Enclave for AI Workloads\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nTable 1: Comparison of TACC and related work TEEs.\nTEE protection scope\nCPU\nGPU\nAccelerator\nIsolated management for chip\nin-package & external memories\nSGX [40], TrustZone [2],\n✔\n✕\n✕\n✕\nSanctum [15], Keystone [35], Penglai [18]\nGraviton [58], HIX [28], Telekine [25]\n✔\n✔\n✕\n✕\nHETEE [67]\n✔\n✔\n✔\n✕\nOur TACC\n✔\n✕\n✔\n✔\n"
  },
  {
    "section": "2.3",
    "text": "3D chip packaging technology\nWith the advancement of 3D IC packaging technology [10,\n16, 19, 32, 33], the integration of a considerable scale of in-\npackage memory on heterogeneous processor chips in the\nforeseeable future will become the mainstream. In-package\nmemory is different from off-package memory that uses\ncopper-based traces on the PCB to connect to the CPU. The\nmemory outside the package is easy to snoop and tamper\nwith [34]. But it is difficult for an attacker to open the chip\npackage and snoop on the silicon interconnection between\nthe accelerator and the stacked memory.\nFor AI computing, memory access has increasingly be-\ncome a performance bottleneck, so how to improve the ef-\nficiency of memory access becomes more and more impor-\ntant. In order to alleviate this problem, the current cloud AI\nchip using HBM DRAM + large-capacity on-chip SRAM (on-\nchip buffers) has become popular (for example, the domain-\nspecific NVIDIA GPU [19]). In this paper, TACC uses the\nphysical isolation of in-package memory and off-package\nmemory, to effectively isolate the internal memory manage-\nment of the enclave from the untrusted host OS.\n"
  },
  {
    "section": "TACC DESIGN",
    "text": "3.1\nOverview\nFig. 3 is our TACC architecture overview. Inside the TACC\nchip, we adopt the popular Accelerator Core in Fig. 2. We\nadd several critical modules for building secure enclave to\nthe memory access hierarchy of the accelerator: DSC (device\nsecurity control), Address Checker, and AES-GCM engine\n(with DMAs), etc. The gray shaded area in the above figure\nis the untrusted space (mainly controlled by the untrusted\nhost OS), and the transparent box is the secure world. We\ncan see that three trusted nodes (Remote User, CPU enclave\n(such as SGX enclave), and TACC encalve) share the same\nset of AES keys to implement a three-party encrypted com-\nmunication channels. (The DSC actually include functions\nsuch as Secure Boot, Remote Attestation, and three-party\nKey Negotiation. These functions require the support of ded-\nicated modules and their on-chip buffers: RSA/ECC engine\n(asymmetric cryptography) + RNG (random number genera-\ntor) + KDF (key derivator) + DH engine (Diffie-Hellman key\nexchange protocol), not shown in Fig. 3.)\nUntrusted OS\nXdma\ndriver\nOblivious  Relay\nCPU enclave\nUser APP\nTACC\nruntime\ncalls\nRemote User\ncalls\nDSC\nOn-chip Mem 1\nOff-chip Mem 0\nHost memory\nkeys\nSet / Clear\nMem Controller 1\nMem Controller 0\nAddr\nChecker\nTACC\nHost CPU\nAccelerator Core\nCommand\nbuffer\nAES-GCM\nXdma\nSet / Clear\n⑥\n①\n②\n③\n④\n⑤\nFigure 3: TACC architecture overview.\nUnlike the GPU unified memory management in Fig. 1,\nTACC cuts off the MMIO path for the host CPU to access the\naccelerator (we do not need to modify the CPU hardware,\ninstead TACC chip does not physically support the MMIO\npath). Command channels and Task channels are both in\nthe form of encrypted packets and are transmitted by xdma\non TACC through PCIe interface. A pure xdma driver (de-\ncoupled from the TACC runtime that supports the internal\ncalculation of the accelerator) is installed in our host OS\nto serve the Oblivious Relay. Therefore, untrusted OS (and\nother software) can only access off-chip Mem 0 (ciphertext\ncommunication memory) of TACC through the xdma engine.\nOn-chip Mem 1 (plaintext running memory) is only visible\nto the CPU enclave bound to the current TACC and the user\nAPP in it (assuming that the host CPU supports SGX enclave,\nand assuming that the side channel vulnerability of SGX\nitself has been patched).\n"
  },
  {
    "section": "3.2",
    "text": "Threat model of this paper\nThe strong adversary of this paper is the OS (and other root\nprivileged code) running on the host CPU. The attacker will\ntry to use the shared IOMMU mapped device xdma and\nshared PCIe Root Complex to snoop or even tamper with\nthe memory and registers space of accelerator cards that\nconnected to the CPU. Since the space exposed by our TACC\nto the host is only the ciphertext memory area outside the\nchip package, and the plaintext running memory area in\nthe package is not visible to the host. Therefore, TACC can\nprevent these attacks and ensure that the data and calculation\nprocesses inside the TACC enclave can not be observed or\ntampered with.\n62\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nJ. Zhu et al.\nEvasion attacks at test time and poisoning attacks at train-\ning time [3–5, 41, 42] are important threats to AI workloads,\nand they are closely related to machine learning algorithms.\nTACC puts the AI network inside the enclave, it can pre-\nvent black-box probing by third-party attackers, but if the\nnetwork code provided by the enclave user itself has errors\nor the user input samples are already noisy, TACC cannot\ndefend against these attacks. Attacks against the defects of\nthe AI algorithm itself (such as Adversarial Samples and\nAdversarial Noises attacks against the robustness of neural\nnetworks [11, 51, 55, 56, 61]) are not within our scope. TACC\nis aimed at the security of the PaaS (Platform as a Service)\nscenario, and assumes that the samples and model codes\nand parameters provided by the user are credible by default.\nTACC protects the privacy and integrity of user data and the\ncalculation process. That is, no opportunity for third-party\nmalware to add sample noise, replace samples, and tamper\nwith model codes and parameters. Therefore, evasion attacks,\npoisoning attacks and more general adversarial attacks are\northogonal to TACC design. Assume that the adversary can-\nnot break the CPU enclave hosting the TACC runtime and\nuser app (such as the side channels and controlled channels\nof SGX [9, 34, 59] are outside our scope).\nBecause the ciphertext communication packets between\nthe user and the TACC enclave needs to be relayed and\nforwarded by an untrusted host OS. Although the adversary\ncannot obtain the confidential content, and the tampered\nciphertext packet will not pass the TACC verification, it can\nrefuse to forward and block the channel. Therefore, Denial of\nService attacks are outside the scope of this paper. Assuming\nthat the chip package is credible, the internal memory cannot\nbe snooped by an adversary who physically touches the chip\npackage pins. In addition, physical leakage methods such as\nelectromagnetic and temperature for the chip are not within\nour scope.\n"
  },
  {
    "section": "3.3",
    "text": "4 types of secure channels\nNext we introduce 4 different types of communication chan-\nnels supported by TACC: Remote/Local-Command, and Task-\nCode/Data.\nRemote-Command channel: The Remote-Command\nchannel is used by the Remote User to send TACC enclave\ncreation and destruction requests. It contains five steps: ①, ②,\n③, ④, and ⑤in Fig. 3. We assume that the Remote User has\nestablished a trust relationship with the host CPU enclave\nbefore requesting to create the TACC enclave. We provide\nusers with TACCenclaveCreate() and TACCenclaveDestory()\ntwo security primitive functions to create and release the\naccelerator secure enclave required by the user. The Create\nfunction actually corresponds to multiple back and forth\nCommand and Response packages at the bottom layer, used\nto authenticate the TACC hardware identity and the mea-\nsurement MAC code of the TACC runtime in the current\nCPU enclave. Only certified devices and runtime stacks are\nallowed to establish TACC enclaves, and then negotiate a\nthree-party shared communication AES keys to construct\na secure channel. Then the Remote User can transmit the\ncode and data to the \"CPU+TACC\" joint enclave. Every time\na TACC enclave is created or destroyed, DSC will trigger\nthe Accelerator Core to clear all its buffers (write values\nof 0), and trigger the Address Checker module to clear the\nmemory usage of the current task to ensure that no sensitive\ninformation remains.\nLocal-Command channel: The Local-Command chan-\nnel is used to allocate and reclaim the internal running mem-\nory of TACC in the User APP program in the CPU enclave. It\ncontains the four steps: ②, ③, ④, and ⑤in Fig. 3. We provide\nthe User APP with TACCmalloc and TACCfree two security\nprimitive instructions, so that the programmer can directly\nmanage the TACC on-chip memory used by the current task.\nA TACC task (similar to the GPU program kernel) code may\ncontain a series of TACCmalloc instructions (packaged into\none or a few command packages). It will be buffered in the\nTACC on-chip Command Buffer, and then DSC will extract\nthe memory requirement from it, allocate a free on-chip\nmemory chunk and bind it to the current enclave ID (each\nCPU enclave exclusively occupies one TACC physical Ac-\ncelerator Core), and record it into the current task memory\noccupancy table in the Address Checker module, for sub-\nsequent access permission checks of the Accelerator Core.\nThe TACCfree command executes unbinding and deleting\nrecords in the same way.\nTACC currently only supports static allocation of mem-\nory chunks, that is, all memory chunks required by a task\nmust be allocated in advance before calculation can begin.\nTemporary changes to the task memory occupancy table are\nnot allowed during the calculation process. All subsequent\nmemory access requests executed by the Accelerator Core\nmust be checked by the Address Checker. Only when the\nindexed (using high bits in the address) occupation table\nentry match the current enclave ID can it be allowed to pass,\notherwise an exception will be generated. Different from the\nexception of traditional accelerators to interrupt untrusted\nhost OS, Address Checker packages the exception informa-\ntion into a Response package, which is encrypted by AES\nand transmitted back to the CPU enclave.\nTask-Code channel: The Task-Code channel is used to\ntransmit the code instruction stream executed by the Accel-\nerator Core. It contains five steps: ①, ②, ③, ④, and ⑥in Fig. 3.\nTask code will be fetched and buffered in IB, see Fig. 2.\nTask-Data channel: The Task-Data channel is used to\ntransmit the data to be processed by the Accelerator Core. It\ncontains the four steps: ①, ③, ④, and ⑥in Fig. 3 (note that\n63\nTACC: A Secure Accelerator Enclave for AI Workloads\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nstep ②can be skipped). The neural network weight parame-\nters in task data will be loaded to SB, and the input feature\nmaps and result feature maps will be cached in NBin and\nNBout, see Fig. 2. Generally, after the weights are deployed\nonce, it will be used for a period of time without updating.\nFor AI inference applications, even the neural network model\ncode is relatively fixed (TACC code once deployed can be\nreused repeatedly by its own user). The remote user may\nfrequently send different batches of input (for example, the\nimage recognition neural network will process a large num-\nber of different images). At this time, the encrypted Task-data\npackages that the Remote User transmits to the host CPU\ncan skip the CPU enclave step ②. Directly relay to TACC for\nprocessing, which can reduce the number of decryption and\nencryption of large quantities of data.\nThe packages transmitted by all channels are encrypted,\nso the untrusted host OS cannot perceive the sensitive infor-\nmation. In addition, we also need to ensure that the size of\ndifferent packages and the sending and receiving time inter-\nvals are data-oblivious (that is, it is not related to the sensitive\ncontent and execution process inside the enclave, so as to\navoid snooping attacks on the communication channels.)\n"
  },
  {
    "section": "3.4",
    "text": "DSC and Address Checker\nTACC maintains per-Accelerator Core memory occupancy\ntable of the current task in the Address Checker (the DSC is\nresponsible for set/clear the table entries), to implement the\nmemory access permission check of the Accelerator Core.\nUnlike the virtual addressing and complex memory paging\nmechanism in traditional CPU/GPU (normal page: 4 KB,\nlarge page: 2 MB). Instead, TACC uses direct physical ad-\ndressing and divides the on-chip memory into larger chunks.\nFor example, if the on-chip memory has 4 GB space (32-bit\nphysical address is required), we can equally divide it into\n128 memory chunks, each with 32 MB. In this case, TACC\nonly needs to implement 128 entries in the task memory\noccupancy table, as shown in Fig. 4.\n"
  },
  {
    "section": "Enclave ID",
    "text": "Task memory occupancy table\nphysical address\n...\nEnclave ID\n1\n1\n"
  },
  {
    "section": "Enclave ID",
    "text": "0\n128 x 16-bit\n...\n32-bit\n7-bit  Index\n25-bit:\nchunk size = 32 MB\nFigure\n4:\nTask\nmemory\noccupancy\ntable\nper-\nAccelerator Core.\nWhen the DSC executes a TACCmalloc instruction, it first\nfinds the entry corresponding to the required memory chunk\naccording to the index, and checks whether its occupied flag\nis in the Idle state. Only memory chunks in the Idle state are\nallowed to be allocated, the current enclave ID is recorded\nin the table entry, and then occupied flag bit is set to 1. If\nthe flag bit is already in the Occupied state, an allocation\nexception Resonse package is generated, which is encrypted\nand returned to the CPU enclave. Similarly, when a TACCfree\ninstruction is executed, the occupied flag is set to 0, and the\nenclave ID in the table entry is cleared. At the same time,\nthe Address Checker needs to be triggered to overwrite the\ncorresponding physical memory chunk with values of 0, to\nensure that no sensitive information remains.\nWhen all the table items required by the current task are\nset, DSC can trigger the Accelerator Core to perform calcu-\nlations. All memory access requests sent from the Acceler-\nator Core must go through the Address Checker to check\npermissions. The check logic finds the corresponding entry\naccording to the high bits in the 32-bit physical address as\nthe index, and checks whether the occupied flag bit is correct\nand whether the enclave ID matches the current enclave. If\nthe permissions do not match, a Response package for the\nmemory exception is generated, and encrypted and returned\nto the CPU enclave. When the Remote User finishes using\nand destroys the enclave, all buffers in the corresponding\nAccelerator Core are cleared, and all table items and physical\nmemory chunks occupied by it need to be cleared.\n"
  },
  {
    "section": "3.5",
    "text": "AES-GCM engine\nTACC currently implements a 128-bit GCM mode AES hard-\nware encryption and decryption module. The AES-GCM\nmodule completes the integrity check while decrypting the\ninput packages, and generates the integrity MAC check code\naccompanying the ciphertext packages while encrypting the\noutput packets. The unique package ID in the decrypted\npacket is used to prevent replay attacks and rollback attacks,\nand the bound CPU enclave ID is used to check whether the\npacket belongs to the current TACC enclave. Packets that do\nnot comply with the check will be discarded. After checking\nthe package, according to the package type code in it, the\nAES-GCM module will forward the Command package to\nthe Command buffer for memory configuration; or forward\nthe Task package to On-chip Mem 1 for Accelerator Core\ncalculation.\nIn our TACC architecture, xdma engine for host commu-\nnication only using ciphertext transfer queues to transmit\nthe encrypted packages. And the AES-GCM module (with\nDMA engine) is the only way that perform the interaction\nbetween inside and outside memories. Therefore, as long\nas AES-GCM does not forward data packets, the internal\nspace is physically separated from the external non-secure\nworld, thereby isolating TACC from the malicious software\naccesses on untrusted host OS.\n64\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nJ. Zhu et al.\n"
  },
  {
    "section": "IMPLEMENTATION",
    "text": "4.1\nPrototype implementation platform\nAX7Z100 is a FPGA development board similar to the Xil-\ninx ZC706, which has a Xilinx xc7z100-ffg900-2 chip on\nboard, and a PCIe x8 end-point interface can be connected\nto the host PCIe slots. Although the 1 GB PL side memory\nof AX7Z100 is all outside the chip package, we use it half for\nciphertext area, and another half for plaintext (pretending\nthat we have in-package memory).\n"
  },
  {
    "section": "4.2",
    "text": "Case study: deploying RepVGG-A0 inference\nneural network\nRepVGG [17] is a powerful architecture of Convolutional\nNeural Network, which has a VGG-like inference-time body\ncomposed only a stack of 3x3 convolution and ReLU. While, it\ncan achieve the accuracy and speed of multi-branch complex\nnetworks. For simplicity (considering the limited resources\non the FPGA), we choose RepVGG-A0, which has the least\nnetwork parameters in a series of RepVGG configurations,\nas the AI workload to evaluate our prototype TACC on the\nFPGA. Even so, the RepVGG-A0 network still has consider-\nable network size and recognition accuracy (it can classify\n1000 classes of images in the ImageNet-2012 test set, with\na Top-1 accuracy of 72.41%). It has a total of 24 layers: 22\nConvolutional layers (each followed by a ReLU), 1 Global\nAverage Pooling layer, and 1 Fully Connected classifier layer.\nIt has a total of about 8.30 M parameters. Even with 16-bit\nfixed-point processing, these parameters require about 16\nMB of storage space.\nSince the RepVGG-A0 network only uses 3x3 convolution\nkernels, in order to improve the utilization of the matrix\nmultipliers in PE and simplicity , we set the number of mul-\ntipliers of the matrix to a multiple of 9, as shown in Fig. 2.\nAnd, we have implemented a SLIM core with 4*9 = 36 mul-\ntipliers, and a FAT core with 16*9 = 144 multipliers. Two\nversions of Accelerator Cores will be used in subsequent per-\nformance evaluations. The size and number of buffers such\nas SB, NBin, and NBout are consistent with the number of\n3x3 matrixes. That is to say, there are 4 buffers of each type\nfor SLIM core that can be accessed simultaneously (a total\nof 12 blockRAMs, total size: 4*4 KB + 4*192 KB + 4*192 KB\n= 1552 KB), For FAT core, there are 16 buffers of each type\nthat can be accessed simultaneously (48 blockRAMs in total,\nbecause the number of buffers has increased by 4 times, but\nthe capacity of the blockRAMs on the FPGA is limited, so we\nhave to reduce the size of each buffer, to a total of 16 *4 KB +\n16*48 KB + 16*48 KB = 1600 KB). The LUT, FF, DSP resource\noccupancy of the two versions of TACC are summarized\nin Table 2. Currently, both versions of the TACC prototype\nonly implement a single Accelerator Core (and per-Core one\nPE). The overhead of increased security-related hardware re-\nsources mainly includes DSC + AES-GCM + Address Checker.\nIf ASIC design is adopted in the future, multiple Accelerator\nCores can be deployed to reduce the relative overhead of\nsecurity mechanism hardware.\nTable 2: FAT / SLIM TACC prototype FPGA utilization.\nLUTs\nFFs\nDSPs\nFAT core\n120712\n18883\n341\nSLIM core\n31398\n10745\n134\nMig Mem Controller\n10817\n8630\n0\nXdma\n25132\n26508\n0\nDSC+AES+Addr Checker\n69756\n142454\n0\n"
  },
  {
    "section": "4.3",
    "text": "TACC instructions\nAs mentioned above, the Security-related TACCmalloc/TAC-\nCfree instructions and TACCenclaveCreate/TACCenclaveDe-\nstroy commands are executed by DSC. In addition, all other\ninstructions of TACC are Task instructions, which are ex-\necuted by the Accelerator Core. We refer to the design of\nthe Cambricon instruction set [39], but simplify and modify\nit. We divide TACC task instructions into two types: mem-\nory access and calculation. Since we implemented multiple\nbuffers that can be accessed simultaneously for SB, NBin and\nNBout in Accelerator Core, but not all instructions need to\naccess all buffers. Therefore, on the basis of Cambricon’s in-\nstruction format, we add field indicating the buffers mask for\nselective access to multiple buffers. And, since the physical\naddress of on-chip buffers is directly open to programmers\nand compilers, mask allows user to flexibly choose to concur-\nrently access all buffers, or selectively access a buffer. Our\nTACC Task instructions are introduced as follows:\n•Memory access instructions\nFig. 5 shows the memory access instructions formats sup-\nported by TACC, including tensor load (TLOAD), tensor store\n(TSTORE), tensor copy (TCOPY), and tensor clear (TCLEAR).\nAmong them, TLOAD is used to load a tensor from memory\nto the specified buffer. TSTORE is just the opposite, writ-\ning the tensor in the specified buffer back to the specified\nmemory area. TCOPY is used to directly copy tensor content\nbetween different on-chip buffers, to reduce the number of\nmemory accesses. In addition, the TCLEAR instruction is\nused to write all 0 values to the target buffer area to achieve\nclear up. The addressing mode of memory is \"base address +\nimmediate offset\". The \"mask\" is used to select which buffers\nto access (for example, 0x0000ffff means that the 1 value\nof 16 bits corresponding to 16 buffers concurrent access).\nThe addressing mode in the buffer is \"start address ~ start\naddress + size - 1\". The size is variable, if the desired tensor\nsize exceeds the buffer capacity, programmer or compiler is\nrequired to perform tiling.\n65\nTACC: A Secure Accelerator Enclave for AI Workloads\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nTLOAD      Dest_addr Dest_size  Dest_mask  Src_base            Src_offset\nTSTORE     Src_addr    Src_size     Src_mask   Dest_base         Dest_offset\nTCOPY      Dest_addr       size           mask         Src_addr\nTCLEAR    Dest_addr  Dest_size  Dest_mask\nImmediate\n8                   6                 6                 6                  6                            32\nopcode\nReg0\nReg1\nReg2\nReg3\nopcode\nReg0\nReg1\nReg2\nReg3\nopcode\nReg0\nReg1\nReg2\n8                   6                 6                 6                  6                            32\n8                   6                 6                 6                                          38\nFigure 5: Memory access instructions.\n•Calculation instructions\nFig. 6 shows the calculation instructions formats sup-\nported by TACC. Currently, only convolutional layer cal-\nculations, fully connected classifier layer calculations, and\nglobal average pooling calculations are supported. For con-\nvolution and fully connected layers, different opcodes can\ndistinguish whether to accumulate and whether to activate\nwith ReLU, which corresponds to the PE in Fig. 2. For pooling\ncalculations, additional execution units are required, which\nare not shown in the figure. The calculation instructions are\ncollectively referred to as tensor execute (TEXEC). Among\nthem, Nin_addr, Nin_size specify the input neural matrix\nrequired by the execution unit, and Nout_addr, Nout_size\nspecify the output neural matrix. Win_addr specifies the in-\nput weight matrix, the size of which is Nin_size * Nout_size.\nopcode\nReg0\nReg1\nTEXEC        Nout_addr  Nout_size     mask      Win_addr  Nin_addr   Nin_size\nReg2\nReg3\n8                   6                 6                 6                  6                  6                6                20\nReg4\nReg5\nFigure 6: Calculation instructions.\nAll Task instructions are fetched and decoded by the Sched-\nuler in the Accelerator Core in Fig. 2 (strictly executed in-\norder, without out-of-order or speculation). Then, the mem-\nory access instruction is executed by the Buffer Controller.\nAccording to the start address and size specified in the in-\nstruction, the built-in DMAs are used to read the specified\nmemory to the specified buffers area, or write the specified\nbuffers area back to the specified memory area. The calcu-\nlation instruction first loads the required matrix data from\nthe buffers into the input register array of the execution unit\nby the Buffer Controller. Then functional unit such as PE is\nresponsible for execution. Finally, the results in the output\nregisters are also saved by the Buffer Controller into the\nspecified buffers area.\nMore types of instructions and supporting functional units\ncan be expanded to improve the versatility of Accelerator\nCore, in the future. (TACC currently does not implement\nthe Jump, Condition Branch, Logical Comparison and other\ninstructions as in the Cambricon). In addition, the number\nof pipeline stages can be increased to improve the operating\nfrequency (the current prototype of TACC on FPGA is exe-\ncuted at 50MHz). Add more buffers capacity and concurrent\nports, to use more double-buffering mechanisms to improve\nparallelization. These optimizations are currently beyond\nthe scope of this paper.\n"
  },
  {
    "section": "4.4",
    "text": "Program model\nFig. 7 shows the pseudo code of the TACC programming\nmodel. Unlike the Unified Memory programming model of\nGPU, TACC does not support direct pointer sharing between\nhost CPU and device, and TACC device on-chip memory\nmust be managed explicitly. That is, all memory chunks to\nbe accessed by the current task must be manually allocated\nby the programmer, and then the task kernel code can access\nthem, and finally the memory chunks need to be manually\nfreed by the programmer. If the capacity of memory chunks\nused by a task kernel exceeds the capacity of TACC on-\nchip memory, programmers also need to manually swap\nmemory chunks to off-chip memory (memory chunks must\nbe encrypted and decrypted when swapping out and in).\n// TACC application snippet inside enclave      (pseudo code)\nint * d_neuron_in;\nint * d_synapse_in;\nint * d_neuron_out;\n// Explicit memory chunks allocation.\nTACCmalloc(& d_neuron_in, ChunkIndex000);\nTACCmalloc(& d_synapse_in, ChunkIndex001);\nTACCmalloc(& d_neuron_out, ChunkIndex002);\n// Task-Code for Accelerator Core.\nTACC_task_kernel(d_neuron_out, d_neuron_in, d_synapse_in);\n// Memory chunks recycling.\nTACCfree(d_neuron_in);\nTACCfree(d_synapse_in);\nTACCfree(d_neuron_out);\nFigure 7: TACC application snippet inside enclave.\nFig. 8 shows a TACC sample enclave interface definition.\nTACC requires at least two types of ECALLs functions to\ntransfer Remote Commands packages (TACCenclaveCre-\nate/Destory requests) and Task Code (C code) packages into\nthe CPU enclave. Similarly, two types of OCALLs functions\nare needed to transfer Local Commands (TACCenclaveCre-\nate/Destory & TACCmalloc/free instructions) and Task Code\n(binary) packages out from the CPU enclave. Of course, all\ndata packets in and out of the CPU enclave need to be de-\ncrypted and encrypted.\nFig. 9 shows the Oblivious-Relay pseudo-code, using the\nxdma driver in the untrusted OS, to read and write the off-\nchip memory (ie ciphertext area) of the device. Oblivious-\nRelay can forward the ciphertext Local Command packages\nand Task Code packages from the CPU enclave to the cipher-\ntext transmission queues of the TACC device. And for large\nbatches of ciphertext Task Data packages from the Remote\nUser, they do not need to go through the CPU enclave, but\nare directly relayed to the TACC device, thereby reducing\nthe number of encryption and decryption.\n66\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nJ. Zhu et al.\n// myTACC_sample_enclave.edl      (pseudo code)\nenclave {\ntrusted {\n// ECALLs transfer Remote Command &Code into CPU enclave.\n// Remote-Command for TACCenclaveCreate/Destory.\npublic void myEcallFunc0([in,size=alen] int *abuf, size_t alen);\n// Task-Code packages (C code) before compilation.\npublic void myEcallFunc1([in,size=blen] int *bbuf, size_t blen);\n};\nuntrusted {\n// OCALLs transfer Local Command &Binary from CPU enclave to TACC.\n// Local-Command(TACCenclaveCreate/Destory &TACCmalloc/free).\nvoid myOcallFunc2([in,size=clen] int *cbuf, size_t clen);\n// Task-Code(binary code for Accelerator Core).\nvoid myOcallFunc3([in,size=dlen] int *dbuf, size_t dlen);\n};\n};\nFigure 8: TACC sample enclave interfaces.\n// Oblivious-Relay      (pseudo code)\n// Relay cipher-text input commands, codes and data packets.\nLaunchCipherPacket(relay_buf,relay_size,startaddr,XdmaHostToCard);\n// Relay cipher-text results/responses packets.\nGetCipherPacket(result_buf,result_size,startaddr,XdmaCardToHost);\nFigure 9: Oblivious relay.\n4.5\nBinary code generation\nCurrently, the binary code of TACC is compiled and gen-\nerated manually by us. The control flow state machine is\nwritten according to RepVGG-A0 (supports reconfigurable\nnetwork parameters), but if the network model changed, we\nhave to regenerate the binary control flow code. Building a\ncomplete binary code generator with the automated cross-\ncompiler and TACC runtime library is our important future\nwork. Therefore, we have not really port the TACC runtime\nto the SGX enclave. Similarly, we leave develop and port\nthe TACC runtime to the CPU enclave as future work. Al-\nthough the current TACC prototype system cannot evaluate\nthe software overhead that enters and exits the CPU enclave\nvery well, the evaluation of the hardware design of TACC\ndevice on the FPGA platform still has a certain reference\nsignificance.\n"
  },
  {
    "section": "Evaluation",
    "text": "In the following, we will show the impact of security-related\nhardware (such as AES-GCM, DSC, and Address Checker)\non the execution performance of the Accelerator Core.\nFor comparison, in the absence of AES-GCM, DSC, and\nAddress Checker, we provide the LaunchPlianPacket( ) and\nGetPlainPacket( ) functions for Oblivious-Relay on the host,\nwhich can directly relay plaintext data packets to TACC\non-chip memory, and thus get baseline performance. Com-\npared with the performance obtained by the ciphertext relay\nfunctions LaunchCipherPacket( ) and GetCipherPacket( ),\nthe increased overhead of security-related hardware can be\nobtained. The main focus of this paper is to build a secure\nenclave on the accelerator chip, and the ultimate pursuit of\nhigh performance and high energy efficiency of the accelera-\ntor itself is beyond the scope of this paper. Nevertheless, on\nthe FPGA platform, at 50Hz, compared to the performance\nof the pure Accelerator Core, we get very low encryption\nand decryption overhead, as shown in Fig. 10 and Fig. 11.\nAs mentioned earlier, our workload is RepVGG-A0. We\nselect input images (with a size of 3*224*224 values) from\nthe ImageNet-2012 test set to form the different inputs of\nour TACC prototype. We take the average of 10 sets of tests\nfor FAT core and SLIM core respectively to draw the graphs.\nFig. 10 shows the latency overhead of the TACC security\nmechanism when batch size = 1, ie one image. In the case\nwhere the network weights need to be temporarily loaded,\nand the case where the weights are already deployed, the\nlatency overhead differs by about 40x. This is because the\nencrypted network weights (RepVGG-A0 requires a total\nof about 8M parameters) are about 56x compared with one\niamge 3*224*224=147K values. Even if temporary deploy-\nment weights are required, the maximum overhead of TACC\ndoes not exceed 1.76%.\nFigure 10: Latency overhead of TACC.\nFigure 11: Throughput overhead of TACC.\nFig. 11 shows the throughput overhead statistics of the\nTACC security mechanism when the batch size is 1, 8, 16,\n32, and 64 images (Neural network weights are already de-\nployed). Among them, the average overhead of the FAT core\nis about 3.5x that of the SLIM core. This is because the num-\nber of multipliers and on-chip buffers of the FAT core are\n4x that of the SLIM core. In addition, it can be clearly seen\nthat batch size = 16 is a turning point in the trend, that is,\nwhen the size of the encrypted input data package exceeds\n16*147K*2 bytes = 4.59 MB, the overhead that TACC’s secu-\nrity mechanism can introduce is already minimize smoothly.\nAt this time, FAT core and SLIM core are only 0.035% and\n0.01% overhead, respectively.\n67\nTACC: A Secure Accelerator Enclave for AI Workloads\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nPeople may think that the overall small overhead intro-\nduced by TACC is due to the slower baseline performance\nof the Accelerator Core we implemented. If the number of\nexecution units in the Accelerator Core is expanded or a\nmulti-core accelerator is implemented, the encryption and\ndecryption overhead will increase. But it should be noted\nthat while we expand accelerator resources, we can also ex-\npand multiple AES engines concurrently to achieve higher\nencryption and decryption bandwidth. Therefore, it is fore-\nseeable that the TACC security mechanism maintains an\noverhead of no more than 1.76% in the case of expansion and\nupgrade of both sides in the future.\n"
  },
  {
    "section": "Security Analysis",
    "text": "First, due to the our separate memory management mech-\nanisms inside and outside the TACC chip, only the outside\nciphertext area is visible to the privileged adversary (such\nas host OS), and the TACC on-chip memory the adversary\ncannot access it. Secondly, TACC does not support host CPU\nto access device registers and on-chip memory and buffers\nthrough the MMIO path. And the exceptions in the internal\ncalculation process of TACC will not interrupt the untrusted\nOS. Instead, TACC will package the exception information\ninto Response packets and encrypt it and send it back to\nthe CPU enclave/Remote User. Finally, all communications\nbetween the TACC enclave, CPU enclave and Remote User\nare encrypted, and the symmetric secret key negotiated by\nthe three parties is shared by only the three of them. In ad-\ndition, the time interval and data packet size of the sending\nand receiving and relaying of ciphertext data packets are\nall data-oblivious. Therefore, our TACC system can protect\nagainst the adversary’s attack described in our threat model.\n"
  },
  {
    "section": "Related Work",
    "text": "Early TEE researches mainly focused on CPU, such as SGX [40],\nTrustZone [2]. In recent years, there have also been solutions\nto build GPU TEE by extending CPU TEE, such as Gravi-\nton [58] and HIX [28]. The data-oblivious streams provided\nby Telekine [25] strengthen the security of interaction be-\ntween remote users and GPU TEE. In addition, there are\nsolutions that do not rely on existing CPU TEE. HETEE [67]\nstrengthens security by adding a hardware-independent Se-\ncurity Controller to physically separate the management of\nthe secure channels from the workloads calculation inside the\nenclave, but it reduces the utilization of distributed hetero-\ngeneous computing resources. However, none of the above\nstudies involved the trusted reconstruction of the emerging\naccelerator architecture itself.\nProtecting only the security-sensitive part of the AI pipeline\ninto the enclave is indeed less overhead. However, this prac-\ntice may not affect confidentiality, but it does not guarantee\nintegrity. Non-sensitive network layer parameters and struc-\ntures outside the enclave may be tampered with by attackers,\nresulting in misclassification. Ternary Model Partitioning\n[23] only protects those network layers where the intermedi-\nate representations of the feature map would leak user input\ninformation. Slalom [57] proposes to outsource the linear\nlayers’ computation of DNNs to GPUs outside the enclave.\nAnd their computing part inside the SGX enclave cannot\nbenefit from hardware accelerators. While, our TACC design\nprovides a heterogeneous enclave that protects hardware AI\naccelerator.\n"
  },
  {
    "section": "Conclusion",
    "text": "This paper provides a secure accelerator enclave design called\nTACC. TACC isolates the internal memory management\nmechanism of the accelerator from the untrusted host OS\nto protect against privileged software attacks. We designed\ndedicated TACCmalloc/TACCfree instructions for the allo-\ncation and recycling of on-chip memory, and designed ded-\nicated TACCenclaveCreate/Destroy command for Remote\nUser, which is used to realize online user switching and con-\ntext clearing of TACC enclave. Based on the FPGA platform,\nwe have implemented two versions of TACC prototype, FAT\ncore and SLIM core, and evaluated that the maximum over-\nhead of the TACC hardware security mechanism does not\nexceed 1.76%. As far as we know, TACC is the first such\ndesign for the emerging accelerator architecture. We hope\nto arouse more attention from the community on secure\naccelerator enclaves.\n"
  },
  {
    "section": "ACKNOWLEDGMENTS",
    "text": "We thank the shepherd Prof. Christian Wressnegger and\nthe anonymous reviewers for their insightful comments.\nThis work was supported by the Chinese National Science\nFoundation for Distinguished Young Scholars under grant\nNo.62125208.\n"
  },
  {
    "section": "References",
    "text": "[1] Apple.\n2021.\nA15\nbionic\nchip\nin\niphone-13-pro,\nhttps://www.apple.com/iphone-13-pro/specs/.\n[2] ARM. 2021.\nArchitecting a more Secure world with isolation\nand\nvirtualization,\nhttps://community.arm.com/arm-community-\nblogs/b/architectures-and-processors-blog/posts/architecting-more-\nsecure-world-with-isolation-and-virtualization.\n[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim\nŠrndić, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion\nAttacks against Machine Learning at Test Time. In Proceedings of\nthe 2013th European Conference on Machine Learning and Knowledge\nDiscovery in Databases - Volume Part III (Prague, Czech Republic)\n(ECMLPKDD’13). Springer-Verlag, Berlin, Heidelberg, 387–402. https:\n//doi.org/10.1007/978-3-642-40994-3_25\n68\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nJ. Zhu et al.\n[4] Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger,\nIgino Corona, Giorgio Giacinto, and Fabio Roli. 2014. Poisoning Be-\nhavioral Malware Clustering. In Proceedings of the 2014 Workshop on\nArtificial Intelligent and Security Workshop (Scottsdale, Arizona, USA)\n(AISec ’14). Association for Computing Machinery, New York, NY, USA,\n27–36. https://doi.org/10.1145/2666652.2666666\n[5] Battista Biggio and Fabio Roli. 2018. Wild Patterns: Ten Years After the\nRise of Adversarial Machine Learning. Pattern Recognition 84 (2018),\n317–331.\n[6] Ferdinand Brasser, Urs Müller, Alexandra Dmitrienko, Kari Kostiainen,\nSrdjan Capkun, and Ahmad-Reza Sadeghi. 2017. Software Grand\nExposure: SGX Cache Attacks Are Practical. In 11th USENIX Workshop\non Offensive Technologies (WOOT 17). USENIX Association, Vancouver,\nBC. https://www.usenix.org/conference/woot17/workshop-program/\npresentation/brasser\n[7] Jo Van Bulck, Marina Minkin, Ofir Weisse, Daniel Genkin, Baris Kasikci,\nFrank Piessens, Mark Silberstein, Thomas F. Wenisch, Yuval Yarom,\nand Raoul Strackx. 2018. Foreshadow: Extracting the Keys to the\nIntel SGX Kingdom with Transient Out-of-Order Execution. In 27th\nUSENIX Security Symposium (USENIX Security 18). USENIX Associa-\ntion, Baltimore, MD, 991–1008. https://www.usenix.org/conference/\nusenixsecurity18/presentation/bulck\n[8] Jo Van Bulck, Nico Weichbrodt, Rüdiger Kapitza, Frank Piessens, and\nRaoul Strackx. 2017. Telling Your Secrets without Page Faults: Stealthy\nPage Table-Based Attacks on Enclaved Execution. In 26th USENIX Secu-\nrity Symposium (USENIX Security 17). USENIX Association, Vancouver,\nBC, 1041–1056. https://www.usenix.org/conference/usenixsecurity17/\ntechnical-sessions/presentation/van-bulck\n[9] Guoxing Chen, Sanchuan Chen, Yuan Xiao, Yinqian Zhang, Zhiqiang\nLin, and Ten H. Lai. 2019. SgxPectre: Stealing Intel Secrets from SGX\nEnclaves Via Speculative Execution. In 2019 IEEE European Symposium\non Security and Privacy (EuroS P). 142–157. https://doi.org/10.1109/\nEuroSP.2019.00020\n[10] Ming-Fa Chen, Fang-Cheng Chen, Wen-Chih Chiou, and Doug C.H.\nYu. 2019. System on Integrated Chips (SoIC(TM) for 3D Heterogeneous\nIntegration. In 2019 IEEE 69th Electronic Components and Technology\nConference (ECTC). 594–599. https://doi.org/10.1109/ECTC.2019.00095\n[11] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.\n2017. ZOO: Zeroth Order Optimization Based Black-Box Attacks to Deep\nNeural Networks without Training Substitute Models. Association for\nComputing Machinery, New York, NY, USA, 15–26. https://doi.org/\n10.1145/3128572.3140448\n[12] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu,\nYunji Chen, and Olivier Temam. 2014. DianNao: A Small-Footprint\nHigh-Throughput Accelerator for Ubiquitous Machine-Learning. In\nProceedings of the 19th international conference on Architectural support\nfor programming languages and operating systems (Salt Lake City, Utah,\nUSA) (ASPLOS ’14). Association for Computing Machinery, New York,\nNY, USA, 269–284. https://doi.org/10.1145/2541940.2541967\n[13] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang,\nLing Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, and Olivier Temam.\n2014. DaDianNao: A Machine-Learning Supercomputer. In Proceedings\nof the 47th Annual IEEE/ACM International Symposium on Microar-\nchitecture (Cambridge, United Kingdom) (MICRO-47). IEEE Computer\nSociety, USA, 609–622. https://doi.org/10.1109/MICRO.2014.58\n[14] Victor Costan and Srinivas Devadas. 2016. Intel SGX Explained. Cryp-\ntology ePrint Archive, Report 2016/086. https://ia.cr/2016/086.\n[15] Victor Costan, Ilia Lebedev, and Srinivas Devadas. 2016.\nSanc-\ntum: Minimal Hardware Extensions for Strong Software Isola-\ntion. https://www.usenix.org/conference/usenixsecurity16/technical-\nsessions/presentation/costan. In 25th USENIX Security Symposium\n(USENIX Security 16). USENIX Association, Austin, TX, 857–874.\n[16] Manish Deo. 2021. Enabling Next-Generation Platforms Using Intel’s\n3D System-in-Package Technology. Intel, White Paper, 2021-11-19.\nhttps://www.intel.com/content/dam/www/programmable/us/en/\npdfs/literature/wp/wp-01251-enabling-nextgen-with-3d-\\system-in-\npackage.pdf.\n[17] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang\nDing, and Jian Sun. 2021. RepVGG: Making VGG-Style ConvNets Great\nAgain. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR). 13733–13742.\n[18] Erhu Feng, Xu Lu, Dong Du, Bicheng Yang, Xueqiang Jiang, Yubin Xia,\nBinyu Zang, and Haibo Chen. 2021. Scalable Memory Protection in the\nPENGLAI Enclave. In 15th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 21). USENIX Association, 275–294.\nhttps://www.usenix.org/conference/osdi21/presentation/feng\n[19] Yaosheng Fu, Evgeny Bolotin, Niladrish Chatterjee, David W. Nel-\nlans, and Stephen W. Keckler. 2021. GPU Domain Specialization via\nComposable On-Package Architecture. CoRR abs/2104.02188 (2021).\narXiv:2104.02188 https://arxiv.org/abs/2104.02188\n[20] Yusuke Fujii, Takuya Azumi, Nobuhiko Nishio, Shinpei Kato, and\nMasato Edahiro. 2013. Data Transfer Matters for GPU Computing.\nIn 2013 International Conference on Parallel and Distributed Systems.\n275–282. https://doi.org/10.1109/ICPADS.2013.47\n[21] Google.\n2021.\nCloud\nTensor\nProcessing\nUnits\n(TPUs),\nhttps://cloud.google.com/tpu/.\n[22] Ben Gras, Kaveh Razavi, Erik Bosman, Herbert Bos, and Cristiano\nGiuffrida. 2017. ASLR on the Line: Practical Cache Attacks on the\nMMU. In NDSS Symposium 2017. 1–15. https://doi.org/10.14722/ndss.\n2017.23271\n[23] Zhongshu Gu, Heqing Huang, Jialong Zhang, Dong Su, Hani Jamjoom,\nAnkita Lamba, Dimitrios Pendarakis, and Ian Molloy. 2018. Confi-\ndential Inference via Ternary Model Partitioning. https://doi.org/10.\n48550/ARXIV.1807.00969\n[24] Yuchen Hao, Zhenman Fang, Glenn Reinman, and Jason Cong. 2017.\nSupporting Address Translation for Accelerator-Centric Architectures.\nIn 2017 IEEE International Symposium on High Performance Computer\nArchitecture (HPCA). 37–48. https://doi.org/10.1109/HPCA.2017.19\n[25] Tyler Hunt, Zhipeng Jia, Vance Miller, Ariel Szekely, Yige Hu, Christo-\npher J. Rossbach, and Emmett Witchel. 2020. Telekine: Secure Com-\nputing with Cloud GPUs. In 17th USENIX Symposium on Networked\nSystems Design and Implementation (NSDI 20). USENIX Association,\nSanta Clara, CA, 817–833. https://www.usenix.org/conference/nsdi20/\npresentation/hunt\n[26] Bongjoon Hyun, Youngeun Kwon, Yujeong Choi, John Kim, and Min-\nsoo Rhu. 2020. NeuMMU: Architectural Support for Efficient Ad-\ndress Translations in Neural Processing Units. In Proceedings of the\nTwenty-Fifth International Conference on Architectural Support for Pro-\ngramming Languages and Operating Systems (Lausanne, Switzerland)\n(ASPLOS ’20). Association for Computing Machinery, New York, NY,\nUSA, 1109–1124. https://doi.org/10.1145/3373376.3378494\n[27] Gorka Irazoqui, Mehmet Sinan Inci, Thomas Eisenbarth, and Berk\nSunar. 2015. Lucky 13 Strikes Back. In Proceedings of the 10th ACM\nSymposium on Information, Computer and Communications Security\n(Singapore, Republic of Singapore) (ASIA CCS ’15). Association for\nComputing Machinery, New York, NY, USA, 85–96. https://doi.org/\n10.1145/2714576.2714625\n[28] Insu Jang, Adrian Tang, Taehoon Kim, Simha Sethumadhavan, and\nJaehyuk Huh. 2019. Heterogeneous Isolated Execution for Commodity\nGPUs. In Proceedings of the Twenty-Fourth International Conference on\nArchitectural Support for Programming Languages and Operating Sys-\ntems (Providence, RI, USA) (ASPLOS ’19). Association for Computing\nMachinery, New York, NY, USA, 455–468.\nhttps://doi.org/10.1145/\n3297858.3304021\n69\nTACC: A Secure Accelerator Enclave for AI Workloads\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\n[29] Zhe Jia, Marco Maggioni, Benjamin Staiger, and Daniele Paolo\nScarpazza. 2018. Dissecting the NVIDIA Volta GPU Architecture via\nMicrobenchmarking. CoRR abs/1804.06826 (2018). arXiv:1804.06826\nhttp://arxiv.org/abs/1804.06826\n[30] Zhen Hang Jiang, Yunsi Fei, and David Kaeli. 2016. A complete key\nrecovery timing attack on a GPU. In 2016 IEEE International Symposium\non High Performance Computer Architecture (HPCA). 394–405. https:\n//doi.org/10.1109/HPCA.2016.7446081\n[31] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gau-\nrav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Bo-\nden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris\nClark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben\nGelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gul-\nland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu,\nRobert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski,\nAlexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch,\nNaveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le,\nChris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacK-\nean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagara-\njan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark\nOmernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt\nRoss, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov,\nMatthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes\nTan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Va-\nsudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun\nYoon. 2017. In-Datacenter Performance Analysis of a Tensor Pro-\ncessing Unit. In Proceedings of the 44th Annual International Sym-\nposium on Computer Architecture (Toronto, ON, Canada) (ISCA ’17).\nAssociation for Computing Machinery, New York, NY, USA, 1–12.\nhttps://doi.org/10.1145/3079856.3080246\n[32] John H. Lau. 2021. 3D IC Integration and 3D IC Packaging. Springer\nSingapore, Singapore, 343–378. https://doi.org/10.1007/978-981-16-\n1376-0_7 https://doi.org/10.1007/978-981-16-1376-0_7.\n[33] John H. Lau. 2021. Chiplet Heterogeneous Integration. Springer Singa-\npore, Singapore, 413–439. https://doi.org/10.1007/978-981-16-1376-\n0_9 https://doi.org/10.1007/978-981-16-1376-0_9.\n[34] Dayeol Lee, Dongha Jung, Ian T. Fang, Chia che Tsai, and Raluca Ada\nPopa. 2020. An Off-Chip Attack on Hardware Enclaves via the Mem-\nory Bus. In 29th USENIX Security Symposium (USENIX Security 20).\nUSENIX Association, 487–504. https://www.usenix.org/conference/\nusenixsecurity20/presentation/lee-dayeol\n[35] Dayeol Lee, David Kohlbrenner, Shweta Shinde, Krste Asanovic, and\nDawn Song. 2020. Keystone: An Open Framework for Architecting\nTrusted Execution Environments. In Proceedings of the Fifteenth Euro-\npean Conference on Computer Systems (EuroSys ’20).\n[36] Sangho Lee, Youngsok Kim, Jangwoo Kim, and Jong Kim. 2014. Steal-\ning Webpages Rendered on Your Browser by Exploiting GPU Vul-\nnerabilities. In 2014 IEEE Symposium on Security and Privacy. 19–33.\nhttps://doi.org/10.1109/SP.2014.9\n[37] Daofu Liu, Tianshi Chen, Shaoli Liu, Jinhong Zhou, Shengyuan Zhou,\nOlivier Teman, Xiaobing Feng, Xuehai Zhou, and Yunji Chen. 2015.\nPuDianNao: A Polyvalent Machine Learning Accelerator. In Proceed-\nings of the Twentieth International Conference on Architectural Support\nfor Programming Languages and Operating Systems (Istanbul, Turkey)\n(ASPLOS ’15). Association for Computing Machinery, New York, NY,\nUSA, 369–381. https://doi.org/10.1145/2694344.2694358\n[38] Fangfei Liu, Yuval Yarom, Qian Ge, Gernot Heiser, and Ruby B. Lee.\n2015. Last-Level Cache Side-Channel Attacks Are Practical. In Proceed-\nings of the 2015 IEEE Symposium on Security and Privacy (SP ’15). IEEE\nComputer Society, USA, 605–622. https://doi.org/10.1109/SP.2015.43\n[39] Shaoli Liu, Zidong Du, Jinhua Tao, Dong Han, Tao Luo, Yuan Xie, Yunji\nChen, and Tianshi Chen. 2016. Cambricon: An Instruction Set Archi-\ntecture for Neural Networks. In Proceedings of the 43rd International\nSymposium on Computer Architecture (Seoul, Republic of Korea) (ISCA\n’16). IEEE Press, 393–405. https://doi.org/10.1109/ISCA.2016.42\n[40] Frank McKeen, Ilya Alexandrovich, Alex Berenzon, Carlos V. Rozas,\nHisham Shafi, Vedvyas Shanbhogue, and Uday R. Savagaonkar. 2013.\nInnovative Instructions and Software Model for Isolated Execution. In\nProceedings of the 2nd International Workshop on Hardware and Archi-\ntectural Support for Security and Privacy (Tel-Aviv, Israel) (HASP ’13).\nAssociation for Computing Machinery, New York, NY, USA, Article\n10, 1 pages. https://doi.org/10.1145/2487726.2488368\n[41] Marco Melis, Ambra Demontis, Battista Biggio, Gavin Brown, Gior-\ngio Fumera, and Fabio Roli. 2017.\nIs Deep Learning Safe for Ro-\nbot Vision? Adversarial Examples against the iCub Humanoid. In\nICCV 2017 Workshop on Vision in Practice on Autonomous Robots\n(ViPAR), Vol. 2017 IEEE International Conference on Computer Vi-\nsion Workshops (ICCVW). IEEE, IEEE, Venice, Italy, 751–759. https:\n//doi.org/10.1109/ICCVW.2017.94\n[42] Luis Muñoz González, Battista Biggio, Ambra Demontis, Andrea Pau-\ndice, Vasin Wongrassamee, Emil C. Lupu, and Fabio Roli. 2017. Towards\nPoisoning of Deep Learning Algorithms with Back-Gradient Optimiza-\ntion. Association for Computing Machinery, New York, NY, USA,\n27–38. https://doi.org/10.1145/3128572.3140451\n[43] Hoda Naghibijouybari, Ajaya Neupane, Zhiyun Qian, and Nael Abu-\nGhazaleh. 2018. Rendered Insecure: GPU Side Channel Attacks Are\nPractical. In Proceedings of the 2018 ACM SIGSAC Conference on Com-\nputer and Communications Security (Toronto, Canada) (CCS ’18). As-\nsociation for Computing Machinery, New York, NY, USA, 2139–2153.\nhttps://doi.org/10.1145/3243734.3243831\n[44] Nvidia. 2016.\nNvidia RISCV Story.\n4th RISC-V Workshop,\n7/2016.\nhttps://riscv.org/wp-content/uploads/2016/07/Tue1100_\nNvidia_RISCV_Story_V2.pdf.\n[45] Nvidia. 2017. RISCV in NVIDIA. 6th RISC-V Workshop, Shanghai,\nMay 2017. https://riscv.org/wp-content/uploads/2017/05/Tue1345pm-\nNVIDIA-Sijstermans.pdf.\n[46] Bharath Pichai, Lisa Hsu, and Abhishek Bhattacharjee. 2014. Architec-\ntural Support for Address Translation on GPUs: Designing Memory\nManagement Units for CPU/GPUs with Unified Address Spaces. In\nProceedings of the 19th International Conference on Architectural Sup-\nport for Programming Languages and Operating Systems (Salt Lake City,\nUtah, USA) (ASPLOS ’14). Association for Computing Machinery, New\nYork, NY, USA, 743–758. https://doi.org/10.1145/2541940.2541942\n[47] Roberto Di Pietro, Flavio Lombardi, and Antonio Villani. 2016. CUDA\nLeaks: A Detailed Hack for CUDA and a (Partial) Fix. ACM Trans.\nEmbed. Comput. Syst. 15, 1, Article 15 (jan 2016), 25 pages.\nhttps:\n//doi.org/10.1145/2801153\n[48] Jason Power, Mark D. Hill, and David A. Wood. 2014. Supporting\nx86-64 address translation for 100s of GPU lanes. In 2014 IEEE 20th\nInternational Symposium on High Performance Computer Architecture\n(HPCA). 568–578. https://doi.org/10.1109/HPCA.2014.6835965\n[49] Jiantao Qiu, Jie Wang, Song Yao, Kaiyuan Guo, Boxun Li, Erjin\nZhou, Jincheng Yu, Tianqi Tang, Ningyi Xu, Sen Song, Yu Wang, and\nHuazhong Yang. 2016. Going Deeper with Embedded FPGA Plat-\nform for Convolutional Neural Network. In Proceedings of the 2016\nACM/SIGDA International Symposium on Field-Programmable Gate\nArrays (Monterey, California, USA) (FPGA ’16). Association for Com-\nputing Machinery, New York, NY, USA, 26–35.\nhttps://doi.org/10.\n1145/2847263.2847265\n[50] PHIL ROGERS, JOE MACRI, and SASA MARINKOVIC. 2013. AMD\nhUMA heterogeneous Uniform Memory Access. AMD Confidential,\nunder embargo until Apr 30. https://events.csdn.net/AMD/130410%20-\n70\nSYSTOR ’22, June 13–15, 2022, Haifa, Israel\nJ. Zhu et al.\n%20hUMA_v6.6_FINAL.PDF.\n[51] Binxin Ru, Adam Cobb, Arno Blaas, and Yarin Gal. 2020. BayesOpt\nAdversarial Attack. In International Conference on Learning Represen-\ntations. https://openreview.net/forum?id=Hkem-lrtvH\n[52] Nikolay\nSakharnykh.\n2017.\nUnified\nmemory\non\npascal\nand volta.\nGPU Technology Conference, 5/10/2017, Nvidia.\nhttps://on-demand.gputechconf.com/gtc/2017/presentation/s7285-\nnikolay-sakharnykh-unified-memory-\\on-pascal-and-volta.pdf.\n[53] Nikolay Sakharnykh. 2018.\nEverything you need to know\nabout unified memory.\nGPU Technology Conference, 3/27/2018,\nNvidia. https://on-demand.gputechconf.com/gtc/2018/presentation/\ns8430-everything-you-need-to-know-about-\\unified-memory.pdf.\n[54] Michael Schwarz, Samuel Weiser, Daniel Gruss, Clémentine Maurice,\nand Stefan Mangard. 2017. Malware Guard Extension: Using SGX to\nConceal Cache Attacks. In Detection of Intrusions and Malware, and\nVulnerability Assessment, Michalis Polychronakis and Michael Meier\n(Eds.). Springer International Publishing, Cham, 3–24.\n[55] Yucheng Shi, Yahong Han, and Qi Tian. 2020. Polishing Decision-\nBased Adversarial Noise With a Customized Sampling. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR).\n[56] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Du-\nmitru Erhan, Ian Goodfellow, and Rob Fergus. 2014. Intriguing prop-\nerties of neural networks. In 2nd International Conference on Learning\nRepresentations, ICLR 2014.\nConference date: 14-04-2014 Through\n16-04-2014.\n[57] Florian Tramèr and Dan Boneh. 2018. Slalom: Fast, Verifiable and\nPrivate Execution of Neural Networks in Trusted Hardware. https:\n//doi.org/10.48550/ARXIV.1806.03287\n[58] Stavros Volos, Kapil Vaswani, and Rodrigo Bruno. 2018. Graviton:\nTrusted Execution Environments on GPUs. In 13th USENIX Symposium\non Operating Systems Design and Implementation (OSDI 18). USENIX As-\nsociation, Carlsbad, CA, 681–696. https://www.usenix.org/conference/\nosdi18/presentation/volos\n[59] Wenhao Wang, Guoxing Chen, Xiaorui Pan, Yinqian Zhang, XiaoFeng\nWang, Vincent Bindschaedler, Haixu Tang, and Carl A. Gunter. 2017.\nLeaky Cauldron on the Dark Land: Understanding Memory Side-\nChannel Hazards in SGX. In Proceedings of the 2017 ACM SIGSAC\nConference on Computer and Communications Security (Dallas, Texas,\nUSA) (CCS ’17). Association for Computing Machinery, New York, NY,\nUSA, 2421–2434. https://doi.org/10.1145/3133956.3134038\n[60] Xin Wang and Wei Zhang. 2019. Cracking Randomized Coalescing\nTechniques with An Efficient Profiling-Based Side-Channel Attack to\nGPU. In Proceedings of the 8th International Workshop on Hardware\nand Architectural Support for Security and Privacy (Phoenix, AZ, USA)\n(HASP ’19). Association for Computing Machinery, New York, NY, USA,\nArticle 2, 8 pages. https://doi.org/10.1145/3337167.3337169\n[61] Zhewei Yao, Amir Gholami, Peng Xu, Kurt Keutzer, and Michael W.\nMahoney. 2019. Trust Region Based Adversarial Attack on Neural\nNetworks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR).\n[62] Yuval Yarom and Katrina Falkner. 2014. FLUSH+RELOAD: A High\nResolution, Low Noise, L3 Cache Side-Channel Attack. In 23rd\nUSENIX Security Symposium (USENIX Security 14). USENIX Associa-\ntion, San Diego, CA, 719–732. https://www.usenix.org/conference/\nusenixsecurity14/technical-sessions/presentation/yarom\n[63] Shijin Zhang, Zidong Du, Lei Zhang, Huiying Lan, Shaoli Liu, Ling Li,\nQi Guo, Tianshi Chen, and Yunji Chen. 2016. Cambricon-x: An Accel-\nerator for Sparse Neural Networks. In The 49th Annual IEEE/ACM Inter-\nnational Symposium on Microarchitecture (Taipei, Taiwan) (MICRO-49).\nIEEE Press, Article 20, 12 pages.\n[64] Yinqian Zhang, Ari Juels, Michael K. Reiter, and Thomas Risten-\npart. 2014. Cross-Tenant Side-Channel Attacks in PaaS Clouds. In\nProceedings of the 2014 ACM SIGSAC Conference on Computer and\nCommunications Security (Scottsdale, Arizona, USA) (CCS ’14). As-\nsociation for Computing Machinery, New York, NY, USA, 990–1003.\nhttps://doi.org/10.1145/2660267.2660356\n[65] Tianhao Zheng, David Nellans, Arslan Zulfiqar, Mark Stephenson,\nand Stephen W. Keckler. 2016. Towards high performance paged\nmemory for GPUs. In 2016 IEEE International Symposium on High\nPerformance Computer Architecture (HPCA). 345–357. https://doi.org/\n10.1109/HPCA.2016.7446077\n[66] Zhe Zhou, Wenrui Diao, Xiangyu Liu, Zhou Li, Kehuan Zhang, and Rui\nLiu. 2017. Vulnerable GPU Memory Management: Towards Recovering\nRaw Data from GPU. Proceedings on Privacy Enhancing Technologies\n2017, 2 (2017), 57–73. https://doi.org/doi:10.1515/popets-2017-0016\n[67] Jianping Zhu, Rui Hou, XiaoFeng Wang, Wenhao Wang, Jiangfeng Cao,\nBoyan Zhao, Zhongpu Wang, Yuhui Zhang, Jiameng Ying, Lixin Zhang,\nand Dan Meng. 2020. Enabling Rack-scale Confidential Computing\nusing Heterogeneous Trusted Execution Environment. In 2020 IEEE\nSymposium on Security and Privacy (SP). 1450–1465. https://doi.org/\n10.1109/SP40000.2020.00054\n[68] Zhiting Zhu, Sangman Kim, Yuri Rozhanski, Yige Hu, Emmett Witchel,\nand Mark Silberstein. 2017. Understanding The Security of Discrete\nGPUs. In Proceedings of the General Purpose GPUs (Austin, TX, USA)\n(GPGPU-10). Association for Computing Machinery, New York, NY,\nUSA, 1–11. https://doi.org/10.1145/3038228.3038233\n71\n"
  }
]