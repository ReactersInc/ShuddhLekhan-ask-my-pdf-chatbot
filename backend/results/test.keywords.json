[
  {
    "section": "Unknown",
    "keywords": [
      "TACC",
      "accelerator",
      "AI",
      "workloads",
      "secure"
    ],
    "key_phrases": [
      "secure accelerator enclave",
      "trusted execution environment"
    ],
    "key_paragraph": "We present a Secure Accelerator Enclave design, which includes heterogeneous accelerator running AI workloads into the protection scope of Trusted Execution Environment, called TACC (Trusted Accelerator)."
  },
  {
    "section": "Jianping Zhu, Rui Hou∗, Dan Meng",
    "keywords": [
      "security",
      "information",
      "engineering"
    ],
    "key_phrases": [
      "Chinese Academy of Sciences",
      "Institute of Information Engineering",
      "School of Cyber Security"
    ],
    "key_paragraph": "State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences;"
  },
  {
    "section": "Abstract",
    "keywords": [
      "security",
      "accelerator",
      "enclave",
      "memory",
      "isolation",
      "AES",
      "FPGA",
      "AI",
      "workloads"
    ],
    "key_phrases": [
      "Trusted Execution Environment",
      "secure accelerator enclave",
      "physical isolation",
      "hardware AES-GCM",
      "on-chip buffer",
      "in-package memory",
      "off-package memory",
      "context clearing"
    ],
    "key_paragraph": "We present a Secure Accelerator Enclave design, which includes heterogeneous accelerator running AI workloads into the protection scope of Trusted Execution Environment, called TACC (Trusted Accelerator). TACC supports dynamic user switching and context clearing of accelerator enclave from the microarchitecture level; The physical isolation of in-package memory (3D chip package) and off-package memory is used to realize the full stack (from hardware to software) isolation of enclave internal running memory and external ciphertext memory."
  },
  {
    "section": "1 Introduction",
    "keywords": [
      "accelerator",
      "CPU",
      "GPU",
      "TEE",
      "security",
      "AI",
      "workloads",
      "memory",
      "isolation",
      "hardware",
      "software"
    ],
    "key_phrases": [
      "Trusted Execution Environment",
      "heterogeneous architecture",
      "secure enclave",
      "on-chip buffer",
      "cache hierarchy",
      "physical address space",
      "Trusted Computing Base",
      "device security controller",
      "memory access hierarchy"
    ],
    "key_paragraph": "In recent years, the field of architecture has presented a situation where CPU, GPU, and accelerator are three pillars, and the accelerator architecture for AI workloads will play an increasingly important role. As small as the on-chip integrated heterogeneous architecture of the mobile phone chip (for example, the Apple A15 Bionic chip [1] is composed of 6-core CPU + 5-core GPU + 16-core Neural Engine), as large as the distributed heterogeneous computing of the cloud platform/data center Architecture (for example, Google Cloud [21] supports three types of computing resources: CPU clusters, GPU clusters and TPU clusters). Trusted Execution Environment (TEE) only based on CPU architecture can no longer meet the security requirements of the heterogeneous architecture."
  },
  {
    "section": "Background",
    "keywords": [
      "security",
      "confidentiality",
      "integrity",
      "authentication",
      "TEE"
    ],
    "key_phrases": [
      "Trusted Execution Environment",
      "secure channels",
      "remote attestation"
    ],
    "key_paragraph": "Confidentiality: Secrets are invisible to the outside. The user’s sensitive data inside the enclave, as well as its calculation process, are not allowed to be observed or perceived by untrusted third parties (including privileged OS). And the communications between user and enclave are encrypted."
  },
  {
    "section": "2.2",
    "keywords": [
      "TEE",
      "CPU",
      "GPU",
      "accelerator",
      "memory",
      "security",
      "enclave",
      "hardware",
      "micro-architecture",
      "TLB",
      "MMU",
      "cache",
      "side-channels"
    ],
    "key_phrases": [
      "memory access hierarchy",
      "secure data interaction",
      "trusted computing",
      "per-enclave page tables",
      "unified virtual addressing memory",
      "device security controller",
      "hardware primitive support",
      "security context saving",
      "off-chip side channels",
      "encrypted address bus"
    ],
    "key_paragraph": "We believe that when building a heterogeneous TEE whose main goal is to protect AI workloads, it is necessary to add security primitive support from the underlying hardware to the emerging accelerator micro-architecture. The processor’s memory access hierarchy is the key underlying hardware for secure data interaction and trusted computing. Therefore, this paper will add an independent hardware Device Security Controller to the accelerator’s unique memory access hierarchy to provide security primitive support."
  },
  {
    "section": "Accelerator Core",
    "keywords": [
      "accelerator",
      "memory",
      "cache",
      "TLB",
      "buffers",
      "scheduler",
      "PE",
      "MMU",
      "virtualization",
      "OS",
      "security"
    ],
    "key_phrases": [
      "on-chip buffers",
      "double-buffering mechanism",
      "unified memory space",
      "address space virtualization",
      "on-demand paging",
      "physical isolation",
      "secure physical boundary"
    ],
    "key_paragraph": "Accelerators usually do not have complex multi-level caches, TLBs, instead they use on-chip buffers to cache input and output data. Although, some local MMU designs for accelerators have recently appeared providing a unified memory space similar to GPUs, supporting address space virtualization and on-demand paging on accelerators, so as to achieve programming convenience and improve resource utilization."
  },
  {
    "section": "2.3",
    "keywords": [
      "3D",
      "packaging",
      "memory",
      "chip",
      "security",
      "tamper",
      "snooping",
      "HBM",
      "SRAM"
    ],
    "key_phrases": [
      "3D IC packaging technology",
      "in-package memory",
      "off-package memory",
      "copper-based traces",
      "silicon interconnection",
      "performance bottleneck",
      "memory access efficiency"
    ],
    "key_paragraph": "With the advancement of 3D IC packaging technology, the integration of a considerable scale of in-package memory on heterogeneous processor chips in the foreseeable future will become the mainstream. In-package memory is different from off-package memory that uses copper-based traces on the PCB to connect to the CPU. The memory outside the package is easy to snoop and tamper with, but it is difficult for an attacker to open the chip package and snoop on the silicon interconnection between the accelerator and the stacked memory."
  },
  {
    "section": "TACC DESIGN",
    "keywords": [
      "TACC",
      "enclave",
      "accelerator",
      "security",
      "DSC",
      "AES-GCM",
      "memory",
      "MMIO",
      "xdma",
      "CPU",
      "OS",
      "key",
      "encryption"
    ],
    "key_phrases": [
      "secure enclave",
      "memory access hierarchy",
      "device security control",
      "encrypted communication channels",
      "plaintext running memory",
      "ciphertext communication memory",
      "Oblivious Relay",
      "three-party encrypted communication",
      "secure boot",
      "remote attestation"
    ],
    "key_paragraph": "Inside the TACC chip, we adopt the popular Accelerator Core. We add several critical modules for building secure enclave to the memory access hierarchy of the accelerator: DSC (device security control), Address Checker, and AES-GCM engine. Unlike the GPU unified memory management, TACC cuts off the MMIO path for the host CPU to access the accelerator."
  },
  {
    "section": "3.2",
    "keywords": [
      "threat",
      "model",
      "OS",
      "CPU",
      "accelerator",
      "memory",
      "security",
      "tamper",
      "snooping",
      "enclave",
      "attack",
      "privacy",
      "integrity"
    ],
    "key_phrases": [
      "strong adversary",
      "root privileged code",
      "ciphertext memory",
      "plaintext running memory",
      "physical leakage",
      "denial of service",
      "adversarial samples",
      "poisoning attacks",
      "evasion attacks",
      "platform as a service"
    ],
    "key_paragraph": "The strong adversary of this paper is the OS (and other root privileged code) running on the host CPU. The attacker will try to use the shared IOMMU mapped device xdma and shared PCIe Root Complex to snoop or even tamper with the memory and registers space of accelerator cards that connected to the CPU. Since the space exposed by our TACC to the host is only the ciphertext memory area outside the chip package, and the plaintext running memory area in the package is not visible to the host, TACC can prevent these attacks."
  },
  {
    "section": "3.3",
    "keywords": [
      "secure",
      "channels",
      "TACC",
      "enclave",
      "remote",
      "local",
      "command",
      "data",
      "code",
      "memory",
      "AES",
      "encryption",
      "buffers",
      "address",
      "checker",
      "allocation",
      "reclaim"
    ],
    "key_phrases": [
      "secure channels",
      "Remote User",
      "CPU enclave",
      "TACC enclave",
      "memory occupancy",
      "accelerator core",
      "communication AES keys",
      "sensitive information",
      "memory access",
      "Task-Code channel",
      "Task-Data channel",
      "encrypted packages",
      "data-oblivious"
    ],
    "key_paragraph": "The Remote-Command channel is used by the Remote User to send TACC enclave creation and destruction requests. It contains five steps. Only certified devices and runtime stacks are allowed to establish TACC enclaves, and then negotiate a three-party shared communication AES keys to construct a secure channel. Then the Remote User can transmit the code and data to the \"CPU+TACC\" joint enclave."
  },
  {
    "section": "3.4",
    "keywords": [
      "DSC",
      "Address",
      "Checker",
      "memory",
      "occupancy",
      "table",
      "allocation",
      "physical",
      "addressing",
      "chunks",
      "enclave",
      "ID",
      "exception",
      "flags"
    ],
    "key_phrases": [
      "memory access permission check",
      "physical addressing",
      "memory chunks",
      "task memory occupancy table",
      "enclave ID",
      "allocation exception",
      "occupied flag",
      "memory exception",
      "Accelerator Core"
    ],
    "key_paragraph": "TACC maintains per-Accelerator Core memory occupancy table of the current task in the Address Checker (the DSC is responsible for set/clear the table entries), to implement the memory access permission check of the Accelerator Core. Unlike the virtual addressing and complex memory paging mechanism in traditional CPU/GPU, TACC uses direct physical addressing and divides the on-chip memory into larger chunks."
  },
  {
    "section": "Enclave ID",
    "keywords": [
      "table",
      "address",
      "enclave",
      "ID"
    ],
    "key_phrases": [
      "task memory occupancy table",
      "physical address"
    ],
    "key_paragraph": "Task memory occupancy table physical address ... Enclave ID 1 1"
  },
  {
    "section": "Enclave ID",
    "keywords": [
      "DSC",
      "TACCmalloc",
      "TACCfree",
      "memory",
      "allocation",
      "exception",
      "enclave",
      "ID",
      "flags",
      "address",
      "checker"
    ],
    "key_phrases": [
      "memory chunk",
      "Idle state",
      "occupied flag",
      "allocation exception",
      "Response package",
      "physical memory chunk",
      "memory access requests",
      "enclave ID",
      "memory exception"
    ],
    "key_paragraph": "When the DSC executes a TACCmalloc instruction, it first finds the entry corresponding to the required memory chunk according to the index, and checks whether its occupied flag is in the Idle state. Only memory chunks in the Idle state are allowed to be allocated, the current enclave ID is recorded in the table entry, and then occupied flag bit is set to 1."
  },
  {
    "section": "3.5",
    "keywords": [
      "AES",
      "GCM",
      "encryption",
      "decryption",
      "integrity",
      "MAC",
      "replay",
      "rollback",
      "packets",
      "DMA",
      "engine",
      "security",
      "isolation"
    ],
    "key_phrases": [
      "AES-GCM engine",
      "ciphertext transfer queues",
      "encrypted packages",
      "integrity check",
      "replay attacks",
      "rollback attacks",
      "CPU enclave ID",
      "Command package",
      "Task package",
      "malicious software",
      "untrusted host OS"
    ],
    "key_paragraph": "TACC currently implements a 128-bit GCM mode AES hardware encryption and decryption module. The AES-GCM module completes the integrity check while decrypting the input packages, and generates the integrity MAC check code accompanying the ciphertext packages while encrypting the output packets. The unique package ID in the decrypted packet is used to prevent replay attacks and rollback attacks, and the bound CPU enclave ID is used to check whether the packet belongs to the current TACC enclave."
  },
  {
    "section": "IMPLEMENTATION",
    "keywords": [
      "FPGA",
      "memory",
      "PCIe",
      "chip",
      "ciphertext",
      "plaintext"
    ],
    "key_phrases": [
      "FPGA development board",
      "Xilinx xc7z100-ffg900-2 chip",
      "PCIe x8 end-point interface",
      "PL side memory",
      "in-package memory"
    ],
    "key_paragraph": "AX7Z100 is a FPGA development board similar to the Xilinx ZC706, which has a Xilinx xc7z100-ffg900-2 chip on board, and a PCIe x8 end-point interface can be connected to the host PCIe slots. Although the 1 GB PL side memory of AX7Z100 is all outside the chip package, we use it half for ciphertext area, and another half for plaintext (pretending that we have in-package memory)."
  },
  {
    "section": "4.2",
    "keywords": [
      "RepVGG",
      "CNN",
      "inference",
      "FPGA",
      "parameters",
      "accuracy",
      "convolution",
      "ReLU",
      "memory",
      "buffers",
      "multipliers",
      "SLIM",
      "FAT",
      "blockRAMs",
      "LUT",
      "FF",
      "DSP"
    ],
    "key_phrases": [
      "Convolutional Neural Network",
      "VGG-like inference-time body",
      "multi-branch complex networks",
      "network parameters",
      "ImageNet-2012 test set",
      "Top-1 accuracy",
      "fixed-point processing",
      "matrix multipliers",
      "Accelerator Cores",
      "on-chip buffers",
      "blockRAMs",
      "resource occupancy"
    ],
    "key_paragraph": "RepVGG [17] is a powerful architecture of Convolutional Neural Network, which has a VGG-like inference-time body composed only a stack of 3x3 convolution and ReLU. While, it can achieve the accuracy and speed of multi-branch complex networks. For simplicity (considering the limited resources on the FPGA), we choose RepVGG-A0, which has the least network parameters in a series of RepVGG configurations, as the AI workload to evaluate our prototype TACC on the FPGA. Even so, the RepVGG-A0 network still has considerable network size and recognition accuracy (it can classify 1000 classes of images in the ImageNet-2012 test set, with a Top-1 accuracy of 72.41%)."
  },
  {
    "section": "4.3",
    "keywords": [
      "instructions",
      "security",
      "memory",
      "buffers",
      "access",
      "calculation",
      "convolution",
      "pooling",
      "DMA",
      "registers",
      "opcode",
      "mask"
    ],
    "key_phrases": [
      "Security-related TACCmalloc",
      "TACCenclaveCreate",
      "Task instructions",
      "memory access",
      "calculation instructions",
      "tensor load",
      "tensor store",
      "tensor copy",
      "tensor clear",
      "base address",
      "immediate offset",
      "buffer controller",
      "execution unit",
      "physical address",
      "on-chip buffers"
    ],
    "key_paragraph": "We divide TACC task instructions into two types: memory access and calculation. Since we implemented multiple buffers that can be accessed simultaneously for SB, NBin and NBout in Accelerator Core, but not all instructions need to access all buffers. Therefore, on the basis of Cambricon’s instruction format, we add field indicating the buffers mask for selective access to multiple buffers. And, since the physical address of on-chip buffers is directly open to programmers and compilers, mask allows user to flexibly choose to concurrently access all buffers, or selectively access a buffer."
  },
  {
    "section": "4.4",
    "keywords": [
      "program",
      "memory",
      "enclave",
      "CPU",
      "device",
      "allocation",
      "encryption",
      "decryption",
      "ECALLs",
      "OCALLs",
      "relay",
      "cipher-text"
    ],
    "key_phrases": [
      "Unified Memory programming model",
      "on-chip memory",
      "Remote Commands packages",
      "Task Code packages",
      "Local Commands packages",
      "Oblivious-Relay",
      "off-chip memory",
      "ciphertext area"
    ],
    "key_paragraph": "Unlike the Unified Memory programming model of GPU, TACC does not support direct pointer sharing between host CPU and device, and TACC device on-chip memory must be managed explicitly. That is, all memory chunks to be accessed by the current task must be manually allocated by the programmer, and then the task kernel code can access them, and finally the memory chunks need to be manually freed by the programmer. If the capacity of memory chunks used by a task kernel exceeds the capacity of TACC on-chip memory, programmers also need to manually swap memory chunks to off-chip memory (memory chunks must be encrypted and decrypted when swapping out and in)."
  },
  {
    "section": "Evaluation",
    "keywords": [
      "security",
      "performance",
      "overhead",
      "encryption",
      "decryption",
      "batch",
      "latency",
      "throughput",
      "AES",
      "DSC",
      "Address",
      "Checker",
      "FPGA",
      "ImageNet"
    ],
    "key_phrases": [
      "security-related hardware",
      "baseline performance",
      "ciphertext relay",
      "plaintext data packets",
      "latency overhead",
      "throughput overhead",
      "batch size",
      "execution units",
      "encryption bandwidth"
    ],
    "key_paragraph": "In the following, we will show the impact of security-related hardware (such as AES-GCM, DSC, and Address Checker) on the execution performance of the Accelerator Core. For comparison, in the absence of AES-GCM, DSC, and Address Checker, we provide the LaunchPlianPacket( ) and GetPlainPacket( ) functions for Oblivious-Relay on the host, which can directly relay plaintext data packets to TACC on-chip memory, and thus get baseline performance. Compared with the performance obtained by the ciphertext relay functions LaunchCipherPacket( ) and GetCipherPacket( ), the increased overhead of security-related hardware can be obtained."
  },
  {
    "section": "Security Analysis",
    "keywords": [
      "memory",
      "ciphertext",
      "adversary",
      "encryption",
      "communication",
      "key",
      "data",
      "security",
      "TACC"
    ],
    "key_phrases": [
      "memory management",
      "privileged adversary",
      "symmetric secret key",
      "ciphertext area",
      "data-oblivious",
      "secure communication",
      "host OS",
      "remote user",
      "exception information"
    ],
    "key_paragraph": "Therefore, our TACC system can protect against the adversary’s attack described in our threat model."
  },
  {
    "section": "Related Work",
    "keywords": [
      "TEE",
      "CPU",
      "GPU",
      "security",
      "enclave",
      "hardware",
      "accelerator",
      "data",
      "integrity",
      "confidentiality"
    ],
    "key_phrases": [
      "CPU TEE",
      "GPU TEE",
      "data-oblivious streams",
      "secure channels",
      "hardware-independent Security Controller",
      "distributed heterogeneous computing",
      "trusted reconstruction",
      "AI pipeline",
      "network layer",
      "feature map",
      "hardware accelerators"
    ],
    "key_paragraph": "However, none of the above studies involved the trusted reconstruction of the emerging accelerator architecture itself."
  },
  {
    "section": "Conclusion",
    "keywords": [
      "TACC",
      "accelerator",
      "enclave",
      "memory",
      "security",
      "FPGA",
      "overhead",
      "design",
      "isolation"
    ],
    "key_phrases": [
      "secure accelerator enclave",
      "internal memory management",
      "on-chip memory",
      "hardware security mechanism",
      "remote user",
      "user switching",
      "context clearing",
      "FPGA platform"
    ],
    "key_paragraph": "This paper provides a secure accelerator enclave design called TACC. TACC isolates the internal memory management mechanism of the accelerator from the untrusted host OS to protect against privileged software attacks."
  },
  {
    "section": "ACKNOWLEDGMENTS",
    "keywords": [
      "comments",
      "funding",
      "support",
      "reviewers"
    ],
    "key_phrases": [
      "insightful comments",
      "Chinese National Science Foundation",
      "Distinguished Young Scholars"
    ],
    "key_paragraph": "We thank the shepherd Prof. Christian Wressnegger and the anonymous reviewers for their insightful comments."
  }
]