[
  {
    "section": "Unknown",
    "text": "High Performance Optimization at the Door of the Exascale\n"
  },
  {
    "section": "Claude Tadonki",
    "text": "MINES ParisTech - PSL\nDépartement Mathématiques et Systèmes\nCentre de Recherche en Informatique (CRI)\n35, rue Saint-Honoré\n"
  },
  {
    "section": "77305, Fontainebleau-Cedex",
    "text": "claude.tadonki@mines-paristech.fr\n"
  },
  {
    "section": "June 23, 2021",
    "text": "Abstract\nThe next frontier of high performance computing is the Exascale, and this will certainly\nstand as a noteworthy step in the quest for processing speed potential. In fact, we always\nget a fraction of the technically available computing power (so-called theoretical peak), and\nthe gap is likely to go hand-to-hand with the hardware complexity of the target system.\nAmong the key aspects of this complexity, we have: the heterogeneity of the computing units,\nthe memory hierarchy and partitioning including the non-uniform memory access (NUMA)\nconﬁguration, and the interconnect for data exchanges among the computing nodes. Scientiﬁc\ninvestigations and cutting-edge technical activities should ideally scale-up with respect to\nsustained performance. The special case of quantitative approaches for solving (large-scale)\nproblems deserves a special focus. Indeed, most of common real-life problems, even when\nconsidering the artiﬁcial intelligence paradigm, rely on optimization techniques for the main\nkernels of algorithmic solutions. Mathematical programming and pure combinatorial methods\nare not easy to implement eﬃciently on large-scale supercomputers because of irregular control\nﬂow, complex memory access patterns, heterogeneous kernels, numerical issues, to name a few.\nWe describe and examine our thoughts from the standpoint of large-scale supercomputers.\n"
  },
  {
    "section": "1",
    "text": "Scientiﬁc context\nThe most notorious computing challenges mainly come from combinatorial problems and their\napplications. As the power of supercomputers is increasing, large-scale scenarios of common prob-\nlems are under consideration and are expected to enter into routine. As previously stated, most\nof these problems can be expressed and solved in the standpoint of optimization, combinatorial\nand/or numerical. Serious eﬀorts are being made to derive more powerful techniques for com-\nbinatorial, numerical, and hybrid optimization. At this point, the word “powerful” refers to the\ncomplexity in terms of the number of basic steps or any relevant metric. When moving to an im-\nplementation on computers, targeting performance through eﬃciency turns to be a diﬃcult task,\nwhich is exacerbated by the speciﬁc complexity and constraints of modern computing systems.\nThe case of linear programming is particularly illustrative of what can appears as disconcerting.\nIndeed, the traditional simplex method, which is known to have an exponential (worst case) com-\nplexity yields more eﬃcient implementations than the polynomial ellipsoid method. We think\nthat similar facts will come up with large-scale optimization on exascale systems. Fundamental\nmethods for solving problems are computer agnostic, thus, implementation eﬀorts mainly try to\nmap an existing method onto a given computing system. A full and consistent optimization solu-\ntion is likely to be a mix of several distinct components from the computing standpoint. Beside\nlinear and non-linear algebra kernels, there are pure combinatorial modules, all orchestrated by at\na higher level following the rules of the global method being so implemented.\n1\narXiv:2106.11819v1  [cs.DC]  22 Jun 2021\n2\nTechnical context\nHigh Performance Computing (HPC) aims at providing powerful computing solutions to scientiﬁc\nand real life problems. Many eﬀorts have been made on the way to faster supercomputers, including\ngeneric and customized conﬁgurations. The advent of multicore architectures is noticeable in the\nHPC history, because it has brought the underlying parallel programming concept into common\nconsiderations. Based on multicore processors, probably enhanced with acceleration units, current\ngeneration of supercomputers is rated to deliver an increasing peak performance, the Exascale era\nbeing the current horizon. However, getting a high fraction of the available peak performance is\nmore and more diﬃcult. The Design of an eﬃcient code that scales well on a supercomputer is a\nnon-trivial task. Manycore processors are now common, and the scalability issue in this context\nis crucial. Code optimization requires advanced programming techniques, taking into account the\nspeciﬁcities and constraints of the target architecture. Many challenges are to be considered from\nthe standpoints of eﬃciency and expected performances. The current faster supercomputer, the\nSupercomputer Fugaku, has a peak of nearly 0.5 exaﬂops with 82% for the sustained performance\non LinPack, and the average sustained performance for the top 5 machines is 75%. We can see that\nthe increasing available power goes alongside with a better eﬃciency, most likely because of more\neﬃcient memory systems and a faster connection between the compute nodes. It is important\nto keep in mind that an eﬀective HPC solution comes from a skillful combination of methods,\nprogramming, and machines [117]. The topic of Optimization is a very nice illustration of this\nobservation because it has provided cutting-edge methods for solving (large-scale) problems, and\nthe question of their eﬃcient mapping onto large-scale supercomputers is crucial and challenging.\nWe now present an overview of the fundamental aspects of optimization, this part comes from our\nwork[117] and is provided here in the intention of a self-contained report.\n"
  },
  {
    "section": "3",
    "text": "Foundations and background\nOperations research is the science of decision making. The goal is to derive suitable mathematical\nmodels for practical problems and study eﬀective methods to solve them as eﬃcient as possible.\nFor this purpose, mathematical programming has emerged as a strong formalism for major prob-\nlems. Nowadays, due to the increasing size of the market and the pervasiveness of network services,\nindustrial productivity and customers services should scale up with a whooping need and a higher\nquality requirement. In addition, the interaction between business operators has reached a no-\nticeable level of complexity. Consequently, for well established companies, dealing with optimal\ndecisions is critical to survive, and the key to achieve this purpose is to exploit recent operation\nresearch advances. The objective is to give a quick and accurate answer to practical instances\nof critical decision problems. The role of operation research is also central in cutting-edge scien-\ntiﬁc investigations and technical achievements. A nice example is the application of the traveling\nsalesman problem (TSP) on logistics, genome sequencing, X-Ray crystallography, and microchips\nmanufacturing[5].\nMany other examples can be found in real-world applications[108].\nA nice\nintroduction of combinatorial optimization and complexity can be found in [105, 39].\nThe noteworthy increase of supercomputers capability has boosted the enthusiasm for solving\nlarge-scale combinatorial problems.\nHowever, we still need powerful methods to tackle those\nproblems, and afterward provide eﬃcient implementation on modern computing systems.\nWe\nreally need to seat far beyond brute force or had hoc (unless genius) approaches, as increasingly\nbigger instances are under genuine consideration.\nFigure 1 displays an overview of a typical\nworkﬂow when it comes to solving optimization problems.\n2\nFigure 1: Typical operation research workﬂow\nMost of common combinatorial problems can be written in the following form\n\n\n\nminimize\nF(x)\nsubject to\nP(x)\nx ∈S,\n(1)\nwhere F is a polynomial, P(x) a predicate, and S the working set, generally {0, 1}n or Zn. The\npredicate is generally referred to as feasibility constraint, while F is the known as the objective\nfunction.\nIn the case of a pure feasibility problem, F could be assumed to be constant.\nAn\nimportant class of optimization problem involves a linear objective function and linear constraints,\nthus the following generic formulation\n\n\n\nminimize\ncT x\nsubject to\nAx ≤b\nx ∈Zp × Rn−p,\n(2)\nwhere A ∈Rm×n, c ∈Rn, b ∈Rm, and p ∈{0, 1, · · · , n}. If p = 0 (resp. p = n), then we have a\nso-called linear program (resp. integer linear program), otherwise we have a mixed integer program.\nThe corresponding acronyms are LP, ILP, and MIP respectively. In most cases, integer variables\nare binary 0 −1 variables. Such variables are generally used to indicate a choice. Besides linear\nobjective functions, quadratic ones are also common, with a quadratic term proportional to xtQx.\nWe now state some illustrative examples.\nExample 1 The Knapsack Problem (KP)[79].\nThe Knapsack Problem is the problem of\nchoosing a subset of items such that the corresponding proﬁt sum is maximized without having\nthe weight sum to exceed a given capacity limit. For each item type i, either we are allowed to\npick up at most 1 (binary knapsack)[35], or at most mi (bounded knapsack), or whatever quantity\n(unbounded knapsack). The bounded case may be formulated as follows(3):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmaximize\nn\nX\ni=1\npixi\nsubject to\nn\nX\ni=1\nwixi ≤c\nxi ≤mi\ni = 1, 2, · · · , n\nx ∈{0, 1}n×n\n(3)\n3\nExample 2 The Traveling Salesman Problem (TSP)[5]. Given a valuated graph, the Trav-\neling Salesman Problem is to ﬁnd a minimum cost cycle that crosses each node exactly once (tour).\nWithout lost of generality, we can assume positive cost for every arc and a zero cost for every dis-\nconnected pair of vertices. We formulated the problem as selecting a set of arcs (i.e. xij ∈{0, 1})\nso as to have a tour with a minimum cost(4). Understanding how the way constraints are formu-\nlated implies a tour is left as an exercise for the reader.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nminimize\nn\nX\ni=1\nn\nX\nj=1\ncijxij\nsubject to\nn\nX\nj=1\nxij = 1\ni = 1, · · · , n, i ̸= j\nn\nX\ni=1\nxij = 1\nj = 1, · · · , n, i ̸= j\nx ∈{0, 1}n×n\n(4)\nThe TSP has an a priori n! complexity. Solving any instance with n = 25 using the current world\nfastest supercomputer (FUGAKU/0.5 exaﬂops) might require years of calculations.\nExample 3 The Airline Crew Pairing Problem (ACPP)[127]. The objective of the ACPP\nis to ﬁnd a minimum cost assignment of ﬂight crews to a given ﬂight schedule. The problem can\nbe formulated as a set partitioning problem(5).\n\n\n\nminimize\ncT x\nsubject to\nAx = 1\nx ∈{0, 1}n\n(5)\nIn equation (5), each row of A represents a ﬂight leg, while\neach column represents a feasible pairing. Thus, aij tells\nwhether or not ﬂight i belongs to pairing j.\nIn practice, the feasibility constraint is mostly the heart of the problem. This the case for the\nTSP, where the feasibility itself is a diﬃcult problem (the Hamiltonian cycle). However, there\nare also notorious cases where the dimension of the search space S (i.e. n) is too large to be\nhandled explicitly when evaluating the objective function. This is the case of the ACPP, where\nthe number of valid pairings is too large to be included into the objective function in one time.\nWe clearly see that we can either focus on the constraints or on components of the objective\nfunction. In both cases, the basic idea is to get rid of the part that makes the problem diﬃcult,\nand then reintroduce it progressively following a given strategy. Combined with the well known\nbranch-and-bound paradigm[26], these two approaches have led to well studied variants named\nbranch-and-cut[126] and branch-and-price[12] respectively.\n4\nFigure 3: Integer programming & LP\nFigure 4: Branch-and-bound & LP\nFigure 2: Branch-and-bound overview\nThe key ingredient of this connection between discrete and continuous optimization is linear\nprogramming (LP). Indeed, applying a linear relaxation on the exact formulation of a combinatorial\nproblem, which means assuming continuous variables in place of integer variables, generally leads\nto an LP formulation from which lower bounds can be obtained (upper bounds are obtained on\nfeasible guests, mostly obtained through heuristics). LP is also used to handle the set of constraints\nin a branch-and-cut, or to guide the choice of new components (columns) of the objective function\nin the branch-and-price scheme. Figure 3 depicts the linear relaxation of an IP conﬁguration, while\nFigure 4 provides a sample snapshot of an LP driven branch-and-bound.\nLinear programming has been intensively studied and has reached a very mature state, even\n5\nfrom the computing standpoint. Nowadays, very large scale LP can now be routinely solved using\nspecialized software packages like CPLEX[135] or MOSEK[138].\nBranch-and-bound and its variants can be applied to a mixed integer programming formulation\nby means of basic techniques like Bender decomposition[17] or Lagrangian relaxation[87]. Figure\n5 depicts the basic idea behind these two approaches.\nFigure 5: Bender decomposition & Langrangian relaxation\nThe later is likely to yield non-diﬀerentiable optimization (NDO) problems. Several approaches\nfor NDO are described in the literature[65], including an oracle-based approach[7], which we will\nlater describe in details as it illustrates our major contribution on that topic. Figure 6 gives an\noverview of an oracle based mechanism.\nFigure 6: Oracle based optimization workﬂow\nFrom the methodological point of view, optimization (both continuous and combinatorial) has\nbeen so far subject to intensive and fruitful investigations. New optimization paradigms or im-\nproved variants of classical techniques have reached an acceptable level of maturity, and have\nproved successful on number of notorious practical problems. However, in some cases, the ex-\npected level of performance can be achieved only through parallel implementation on large-scale\nsupercomputers, especially with intractable (but pertinent) combinatorial problems. The idea is\nto combine the advantages of mathematically powerful algorithms with the capability of machines\nthat have several processors. The existence of commercial multiprocessor computers has created\nsubstantial interest in exploring the use of parallel processing for solving optimization problems\neven for basic issues. The challenge is to ﬁnd a suitable way to implement the aforementioned\ntechniques (somehow irregular) on modern supercomputers (mostly tailored for regular computa-\ntions) with an acceptable eﬃciency. We now provide technical details on how this can be tackled\nand what has been done.\n6\n4\nParallel optimization\nProcessing with supercomputers is mainly parallel computing. Theoretical complexity studies the\nintrinsic diﬃculty of the optimization problems and classify them accordingly. There is an impor-\ntant set of common problems that can be solved or approximated in polynomial time. However,\nas some of them are (recursively) solved to get the solution of more diﬃcult problems, improve-\nments are still expected whenever there is a room for that. A good example of this is the shortest\npaths problem, which appears as a sub-problem for the multicommodity ﬂow problem[9]. Many\nother combinatorial problems are (known to be) diﬃcult, thus the basic expectation with super-\ncomputers is to be able to solve them in a reasonable time through an eﬃcient parallelization of\na chosen method. The main point with large-scale supercomputers (thus exascale ones) is the\nhuge number of computing units, which implies a larger and deeper parallelism. The global topic\nof Optimization has mainly two components: continuous optimization and discrete optimization.\nHowever, because pure combinatorial problems might be too diﬃcult to solve only from the combi-\nnatorial standpoint, many approaches have developed a bridge between the discrete universe and\nthe continuous universe through geometric, analytic, and algebraic techniques such as global opti-\nmization, semideﬁnite programming, and spectral theory. Mixed integer programming formulations\ninvolve diﬀerentiable or non-diﬀerentiable objective functions. Non-diﬀerentiable conﬁgurations\nmight come from the consideration of a Lagrangian relaxation approach, which moves a subset of\nthe constraints (usually the harder ones) into the objective function. The eﬀorts in the design\nof eﬃcient algorithms for common combinatorial problems has lead to useful connections among\nproblems (i.e. a solution for one can be used to construct a solution for another). As consequence,\nthere is a set of reference optimization problems for which improved solutions are continuously\ntracked by researchers. For this purpose, parallel computing applied to all previously mentioned\noptimization paradigms is clearly worth considering.\nAn important set of discrete optimization problems are NP- complete[61]; hence their time\ncomplexity increases exponentially for all known algorithms. Consequently, parallel processing\ncannot achieve polynomial complexity on these problems without using at least an exponential\nnumber of processors (not counting data exchanges). However, the average-time complexity of\nheuristics and sub-optimal algorithms for a wide range of problems are polynomial[82, 129]. Sig-\nniﬁcant advances have been made in the use of powerful heuristics and parallel processing to\nsolve large scale discrete optimization problems. Number of problem instances that were con-\nsidered computationally intractable on sequential machines are routinely solved on server-class\nsymmetric multi-processors and workstation clusters. In conjunction with the increasing power of\nsupercomputers, cutting-edge methods in optimization are expected to cope with very large-scale\nproblems.\nWe get a direct impact of parallel computing in numerical optimization through the advances\nin parallel numerical algebra[29, 52, 49], with some of them being implemented into eﬀective\nframeworks[3, 11, 2, 25, 124]. Encapsulating parallel linear algebra routines into optimization\ncodes [43, 114] is a nice way to provide their power to the users without additional eﬀorts. This is\nstill a very critical and challenging topic since parallelizing the linear algebra kernels of optimization\nalgorithms is not an easy task, and moving on the exascale era will made it more complex. For\ninstance, matrix factorization updating process in quasi-Newton methods or active set strategies\ninvolves vector-vector operations that are not easy to parallelize eﬃciently [41].\nAccording to\nSchnabel[112], parallel computing can be performed in numerical optimization through three levels:\n⋄parallelization of the function and/or the derivative evaluations;\n⋄parallelization of the linear algebra kernels;\n⋄modiﬁcations of the basic algorithms in order to increase the degree of parallelism.\nFor the ﬁrst two levels, several approaches have been developed [29, 106, 107, 54, 69] and we\nmight also expect some outputs for heterogeneous systems. For most of interior point (IP) meth-\nods in linear programming (LP), quadratic programming (QP), and nonlinear programming, the\n7\nkernel is the solution of a special linear system [4, 19]. As the iterates approach the boundary\nof the feasible set or the optimal solution, the system becomes more and more ill-conditioned.\nSuitable strategies have been developed within a modiﬁed Cholesky factorization framework and\nsuccessfully used in specialized codes as CPLEX[135], LOQO[137] and GUROBI[136].\nThus,\neﬃcient parallel versions of these strategies are highly desired, but challenging on large-scale su-\npercomputers, especially for sparse systems. The paper of Durazzi and Ruggiero [50] presents a\nparallel approximated IP method for QP, based on a preconditioned Conjugated Gradient algo-\nrithm. D’Apuzzo and Marino [41] have proposed a parallel Potential Reduction algorithm for the\nconvex quadratic bound constrained problem. A parallel decomposition approach is considered\nby Zanghirati and Zanni [132] for large scale QPs. Blomwall [23] has proposed a parallel imple-\nmentation of a Riccati-based primal IP algorithm for multistage stochastic programming. Most\nof these contributions together with more recent ones consider conventional parallel architectures,\nthe question is how well are they adaptable for complex systems (i.e. heterogeneous with NUMA\nmemory for instance).\nRegarding the third level, multi-directional search strategies [89] provide a high-level paral-\nlelism which can be exploited through a concurrent execution of the minimization processes. Ad-\nhoc or application speciﬁc algorithms are also concerned, particularly when large-scale instances\nare considered [31, 81]. Another case study in statistical model selection is analyzed by Gatu\nand Kontoghiorghes [62]. As many ﬁelds in numerical analysis, several algorithms in numerical\noptimization have been revisited because of parallelism considerations. In [56], approaches to ex-\npose parallelism through appropriate partitioning of mathematical programs are reported. Interior\npoint strategies, because of their direct possibility of parallel implementation [42, 73], have received\nmuch attention compare to active set algorithms, and have stimulated intensive researches in order\nto understand and overcome their weak scaling on large supercomputers. Developments in object\noriented software for coding and tuning linear algebra algorithms at a high level of abstraction are\nprovided in [125, 130]\nAs previously said, many techniques have so far been developed to provide a bridge between\ncontinuous and discrete formulations. Recent successes based on such approaches include IP meth-\nods for discrete problems, the Goemans-Williamson relaxation of the maximum cut problem, the\nChvatal cuts for the traveling salesman problem, and the Gilbert-Pollak’s conjecture, to name\na few. Parallel algorithms for discrete optimization problems can be obtained in many diﬀer-\nent ways including the classical domain decomposition. SPMD (Single Program Multiple Data)\nparallelization attempts to enlarge the exploration of the solution space by initiating multiple\nsimultaneous searches towards the optimal solution. These approaches are well implemented by\nclustering methods. Byrd et al. [31, 30] and Smith and Schnabel[115] have developed several\nparallel implementations of the clustering method. Parallelization of classical paradigms have also\nbeen explored: parallel dynamic programming[63], branch and bound[26, 45], tabu search, simu-\nlated annealing, and genetic algorithms[109]. In the paper of Clementi, Rolim, and Urland [37],\nrandomized parallel algorithms are studied for shortest paths, maximum ﬂows, maximum indepen-\ndent set, and matching problems. A survey of parallel search techniques for discrete optimization\nproblems are presented in [71]. The most active topics are those involved with searching over trees,\nmainly the depth-ﬁrst and the best-ﬁrst techniques and their variants. The use of parallel search\nalgorithms in games implementation has been particularly successful, the case of IBM’s Deep Blue\n[113] is illustrative. This topic is very active in Artiﬁcial Intelligence for which the question of\neﬃcient parallelization stands as one of the major HPC applications.\nWe now discuss what appears to us crucial points on the way to the Exascale when considering\neﬃcient implementations in both continuous optimization and combinatorial optimization.\n8\n5\nCritical Numerical and Performance Challenges\nLet ﬁrst point out and describe the main issues when seeking an eﬃcient implementation of the\naforementioned paradigms on large-scale supercomputers.\n⋄Computing unit: The generic compute node is likely to be a many-core processor. Seeking\neﬃciency and scalability with many-core processors is a hard task [116]. As with any shared-\nmemory system, the way to go is through the shared-memory paradigm. Thereby, we avoid\nexplicit data exchanges, but there is more pressure on main memory accesses with a heavy\nconcurrency that will be the main culprit of weak scalability. Vectorization is to be considered\nat the level of the linear algebra kernels, and this requires a suitable data organization.\n⋄Memory system: One critical point here is the management of shared variables. Optimization\ntechniques are likely to be iterative, so the access to these variables is repeated accordingly.\nFor read-only accesses, the performance will depend on how good we are with memory\ncaching, this aspect should be investigated deeply considering all iteration levels. For write\naccesses, the main issue is concurrency, with a special focus on iterative (in-place) updates.\nThe case of non uniform memory access (NUMA) needs a special attention as most of many-\ncore processors follow this speciﬁcity along with the corresponding packaging of the cores.\nIt is likely that exascale machines will be equipped with such processors.\n⋄Numerical sensitivity: A part from accuracy concerns, it is common to consider a lower\nprecision in order to reach a faster ﬂops through wider SIMD and also to speed-up the\nmemory accesses. Iterative methods usually consider this adaptation under a global mixed-\nprecision scheme. The main drawbacks with lower precision come from the potential lost of\naccuracy, which might led to wrong numerical results or slower convergence.\n⋄Heterogeneity: The tendency with top class supercomputers is heterogeneity.\nThe most\ncommon conﬁguration is the classical CPU-GPU conjunction. GPUs has reached enough\nmaturity to be considered for most of common computing tasks including those form linear\nalgebra. It is likely that this will be and remain the typical scenario of the GPU considera-\ntion for the implementation of optimization algorithms. However, the well-known problem of\nCPU/GPU data exchanges is to be seriously considered in the standpoint of an iterative pro-\ncess. In case of high-precision computation with GPU, there might be some concerns about\naccuracy. In addition, a trade-oﬀbetween accuracy and performance should be skillfully\nconsidered.\n⋄Synchronization: Considering the notorious case of the brand-and-bound, which also stands\nas a typical connection between continuous optimization and combinatorial optimization, all\nactive explorations running in parallel share some common variables (concurrent updates,\ncritical values, ...) and conditions (termination/pruning, numerical/structural, ...). Synchro-\nnize in the context of a large-scale supercomputer is costly and the eﬀect on the scalability\nis noteworthy.\n⋄Data exchanges: This is the main source of a serious time overhead with distributed memory\nparallelism, which also generally includes the aforementioned mechanism (synchronization).\nA more general optimization scheme has several levels of iteration (of diﬀerent natures),\nwhich yields a complex communication ﬂow and topology. This aspect is certainly the most\nhindering on the way to parallel eﬃciency, as it consumes the major part of the global\noverhead.\n⋄Load balance: Active subproblems might have diﬀerent complexities and numerical charac-\nteristics, thus yielding unequal loads for the corresponding tasks. Beside the computing load,\nthere is also some numerical characteristics that might impact the local runtime complexity\non the compute nodes. This aspect is hard to ﬁx without changing the way the computation\nis organized. The way a given (sub)problem is solved in optimization sometimes depends on\n9\nits speciﬁc structure, this makes diﬃcult to predict the choice that will be made at runtime\nand thus complicates any prediction.\nMany optimization problems are based on an objective function that is implicit or non-\ndiﬀerentiable. To solve them with gradients-based approaches, we need to deal with an oracle\nthat can return for a given point of the search space the evaluation of the objective function and\nthe corresponding derivatives. Oracle-based Optimization is a powerful tool for general purpose\noptimization. To make this approach successful from the performance and numerical standpoints,\nit is important to (i) keep the number of calls to the oracle as low as possible, especially if it\ninvolves solving a diﬃcult combinatorial problem; and (ii) take care of numerical issues that might\nextend the number of necessary iterations or lead to divergence. Oracle-based continuous opti-\nmization is better addressed with generic approaches so as to oﬀer the possibility to treat most\ncommon combinatorial problems. However, number of important aspects still need to be seriously\nconsidered. We list some of them.\n⋄The core of an oracle-based method in continuous optimization involves solving a linear\nsystem for the search direction used to get the next query point. It is crucial to have the\nsolution accurate enough to be meaningful and keep us to the track. As we get close to\nthe boundaries of the search space or to the optimal solution, the principal matrix of the\nlinear system becomes ill-conditioned, thus making diﬃcult the computation of the required\nsolution. This fact severely increases the associated computational cost, unless we chose to\nsacriﬁce the accuracy, which will extend the number of outer iterations towards the solution.\nThus, it is important to carefully address this issue, which belongs to the more general\ntopic of solving ill-conditioned linear systems. However, there is probably a way to exploit\nthe speciﬁc structure of the principal matrix in this case. The topic here is mainly that\nof linear systems solving, which has been extensively studied but remains diﬃcult to make\nit as scalable as desired, especially with sophisticated iterative methods. Inter-processors\ncommunication and global synchronization mechanisms are what we should care about for\nthis aspect on exascale systems.\n⋄The query point generator of the cutting planes method looks for a guess within the localiza-\ntion set that corresponds to the polyhedral deﬁned by the cuts accumulated so far. A good\nmanagement of these cuts is crucial and their number linearly increases with the number\nof iterations. If the dimension of the problem is huge, or if we have already performed a\nlarge number of iterations, then the required memory space necessary to keep all the cuts\nwill become signiﬁcant, and this might slowdown the global memory eﬃciency, especially\nwith complex memory systems like NUMA ones. One way to ﬁght against this problem is\nto eliminate redundant cuts, or to keep only the minimal set of the cuts that corresponds\nto the same (or equivalent) polyhedral of the localization set. Doing this is not trivial as\nthere are many valid selections. Another way is to aggregate the cuts instead of eliminating\nthem. We could also weight the cuts according to their importance within the localization\nset. All theses thoughts have to be studied deeply, even through an experimental approach.\nHowever, we need to be careful as we could destroy the coherence of the localization set,\nthus impacting the convergence.\n⋄Cutting planes methods are iterative, and the convergence is monitored by the calculation of\nthe gap between the best solution found so far and the estimated lower/upper bound (ideally\nthe optimal value of the objective function, but we don’t have it). The process converges\nif: (a) the gap is below the tolerance threshold; (b) we have reached the maximum number\nof iterations (over the expectation); (c) a null gradient is returned by the oracle; (d) an\nincoherent information is provided by the oracle or calculated internally; (e) an unexpected\ncritical issue (hardware/system or numerical) has occurred.\nThe main focus here is the\nlower/upper bound estimation. This is usually obtained from the localization set (the cuts\n+ the objectives), which might become heavy and numerically sensitive over the time (again,\nbecause of the large number of collected cuts and the global conﬁguration). If the estimation\n10\nof the lower/upper bound is good enough, then we will perform more additional iterations\nor never converge (even if we should, either because we are already at the optimum or there\nis no further improvement). It is therefore important to address this problem and look for\na robust approach. It makes sense to assign this calculation to single computing unit and\nbroadcast the result to the whole computing system.\n⋄Regarding the Newton linear system that is solved during inner iterations to get the search\ndirection for the next query point, updating the principal matrix takes a serious overhead.\nIndeed, at each iteration, the matrix of the generated cuts is updated from A to [A, u], where\nu is the new incoming cut, then we solve a linear system based on a principal matrix of the\nform\nA × diag(s2) × AT ,\n(6)\nwhere s is the vector of the so-called slack variables and s2 = (s2\ni ). It is quite frustrating to\nsolve this system from scratch at each iteration. Indeed, the principal matrix (6) seems to\nhave a suitable form for a direct Cholesky factorization. The dream here is to keep on the\ndesired factorization by means of eﬃcient updates, thus a quadratic complexity instead of the\ncubic one for the factorization restarted from scratch. The current state-of-the-art in matrix\ncomputation, to the best of our knowledge, does not provides the aforementioned Cholesky\nupdate, this remains to be investigated including the parallelization from the perspective of\nrunning on a (large-scale) supercomputer.\n⋄About the branch-and-bound, an important method for solving combinatorial problems\n(including approximations), very popular for MIP formulations, the main research direc-\ntion from the HPC standpoint is through an eﬃcient parallelization of the paradigm itself.\nBranch-and-bound is likely to yield an irregular computation scheme with an unpredictable\npath to the solution, thus making very challenging for eﬃcient parallelization, especially\non large-scale supercomputers. Among critical issues, we mention: heavy synchronization,\nirregular communication pattern, huge amount of memory to handle the generated cuts, load\nunbalanced and/or non-regular memory accesses. The management of the implicit recursion\nof the whole is diﬃcult to keep scalable as the number of processors increases. An on-the-ﬂy\nrescheduling of the tasks might be necessary at some points in order to adapt to branch-\ning mispredictions or severe load imbalance. In conjunction with continuous optimization,\nthere are eﬀective generic frameworks for the branch-and-bound associated with continuous\noptimization solvers [28], such frameworks should be made parallel at design time.\n"
  },
  {
    "section": "Conclusion",
    "text": "Optimisation is a central topic, which combines advances in applied mathematics and technical\ncomputing. Powerful methods have been developed and are still improved to solve diﬃcult but\nrelevant real-life problems. As the power of supercomputers is signiﬁcantly increasing, there is an\ninstinctive desire for being able to routinely solve large-scale problems. This raises the challenge of\neﬃcient implementations of cutting-edge optimization techniques on large-scale supercomputers.\nParallel optimization is the main topic involved in this context, and the main concern is scalability.\nIdeally, the most powerful optimization methods should be scalable enough to yield the most\neﬃcient solutions to the target problems. However, the global and internal structures of modern\nsupercomputers make them not easy to program eﬃciently, especially with too speciﬁc approaches\nlike the ones from optimization. On the way to the exascale, this will be exacerbated by the\ncomplexity of the systems, but the eﬀorts are worth it.\n"
  },
  {
    "section": "References",
    "text": "[1] C. J. Adcock and N. Meade, A simple algorithm to incorporate transaction costs in quadratic opti-\nmization, European Journal of Operational Research, 7, 85-94, 1994.\n11\n[2] E. Agullo, B. Hadri, H. Ltaief and J. Dongarra, Comparative study of one-sided factorizations with\nmultiple software packages on multi-core hardware, SC’09: International Conference for High Perfor-\nmance Computing, 2009\n[3] E. Agullo, J. Dongarra, B. Hadri, J. Kurzak, J. Langou, J. Langou, H. Ltaief, P. Luszczek, and A.\nYarKhan, PLASMA: Parallel Linear Algebra Software for Multicore Architectures, Users? Guide,\nhttp://icl.cs.utk.edu/plasma/, 2012.\n[4] E. D. Andersen, J. Gondzio, Cs. Mészáros, and X. Xu, Implementation of interior point methods for\nlardge scale linear programming, T. Terlaky (Ed), Interior-point Methods of Mathematical Program-\nming, Kluwer Academic Publishers, pp. 189-252, 1996.\n[5] Applegate, D.L., Bixby, R.E., Chvatal, V., Cook, W.J., The Traveling Salesman Problem - A Com-\nputational Study, Princeton Series in Applied Mathematics, 2006.\n[6] F. Babonneau and J.-P. Vial. ACCPM with a nonlinear constraint and an active set strategy to solve\nnonlinear multicommodity ﬂow problems. Technical report, Logilab, Hec, University of Geneva, June\n2005.\n[7] Babonneau, Frédéric and Beltran, Cesar and Haurie, Alain and Tadonki, Claude and Vial, Jean-\nPhilippe, Proximal-ACCPM: a versatile oracle based optimization method, 9, 69–92, 2007.\n[8] F. Babonneau, O. du Merle, and J.-P. Vial. Solving large scale linear multicommodity ﬂow problems\nwith an active set strategy and Proximal-ACCPM. Operations Research, 54(1):184–197, 2006.\n[9] F. Babonneau, Solving the multicommodity ﬂow problem with the analytic center cutting plane method,\nPhD thesis, University of Geneva, http://archive-ouverte.unige.ch/unige:396, 2006.\n[10] Bader, D.A., Hart, W.E., Phillips, C.A., Parallel Algorithm Design for Branch and Bound, In: Green-\nberg, H.J. (ed.) Tutorials on Emerging Methodologies and Applications in Operations Research, ch.\n5, Kluwer Academic Press, Dordrecht, 2004.\n[11] S. Balay, J. Brown, K. Buschelman, V. Eijkhout, W. Gropp, D. Kaushik, M. Knepley, L. Curfman\nMcInnes, B. Smith, and H. Zhang, PETSc Users Manual. Revision 3.2, Mathematics and Computer\nScience Division, Argonne National Laboratory, September 2011\n[12] Barnhart et. al. ,\nBranch and Price :\nColumn Generation for Solving Huge Integer Pro-\ngrams,Operation Research, Vol 46(3), 1998.\n[13] C. Barnhart, A. M. Cohn, E. L. Johnson, D. Klabjan, G. L. Nemhauser, P. H. Vance, Airline\nCrew Scheduling, Handbook of Transportation Science International Series in Operations Research\n& Management Science Volume 56, pp 517-560, 2003.\n[14] C. Barnhart, E.L. Johnson, G.L. Nemhauser, M.W.P. Savelsbergh, and P.H. Vance. Branch-and-\nprice: Column generation for solving huge integer programs. Operations Research, 46(3):316–329,\n1998.\n[15] C. Beltran, C. Tadonki, J.-Ph. Vial, Semi-Lagrangian relaxation , Computational Management Sci-\nence Conference and Workshop on Computational Econometrics and Statistics, Link, Neuchatel,\nSwitzerland, April 2004 .\n[16] C. Beltran, C. Tadonki, and J.-P. Vial. Semi-lagrangian relaxation. Technical report, Logilab, HEC,\nUniversity of Geneva, 2004.\n[17] J. F. Benders, Partitioning procedures for solving mixed-variables programming problems, Numerische\nMathematik 4(3), pp. 238?252, 1962.\n[18] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Computa-\ntional Management Science, 2:3–19, 2005. Initially appeared in Numerische Mathematik, 4: 238-252,\n1962.\n[19] H. Y. Benson, D. F. Shanno, R.J. Vanderbei, Interior-point methods for convex nonlinear program-\nming: jamming and comparative numerical testing, Op. Res. and Fin. Eng., ORFE-00-02-Princeton\nUniversity, 2000 .\n12\n[20] C. Berger, R. Dubois, A. Haurie, E. Lessard, R. Loulou, and J.-P. Waaub. Canadian MARKAL: An\nadvanced linear programming system for energy and environmental modelling. INFOR, 30(3):222–\n239, 1992.\n[21] D. Bienstock, Computational study of a family of mixed-integer quadratic programming problems,\nMath. Prog. 74, 121-140, 1996.\n[22] L. S. Blackford,\nJ. Choi,\nA. Cleary,\nE. D’Azevedo,\nJ. Demmel,\nI. Dhillon,\nJ. Dongarra,\nS. Hammarling,\nG. Henry,\nA. Petitet,\nK. Stanley,\nD. Walker,\nR. C. WhaleyScaLAPACK,\nhttp://www.netlib.org/scalapack, 2012.\n[23] J. Blomwall, A multistage stochastic programming algorithm suitable for parallel computing, Parallel\nComputing, 29, 2003.\n[24] R. A. Bosh and J.A. Smith, Separating Hyperplanes and the Authorship of the Disputed Federalist\nPapers, American Mathematical Monthly, Volume 105, No 7, pp. 601-608, 1995.\n[25] Bosilca, G., Bouteiller, A., Danalis, A., Faverge, M., Haidar, H., Herault, T., Kurzak, J., Langou,\nJ., Lemariner, P., Ltaief, H., Luszczek, P., YarKhan, A., Dongarra, J. Distributed Dense Numerical\nLinear Algebra Algorithms on Massively Parallel Architectures: DPLASMA, University of Tennessee\nComputer Science Technical Report, UT-CS-10-660, Sept. 15, 2010.\n[26] S. Boyd , J. Mattingley, Branch and Bound Methods, Notes for EE364b, Stanford University, Winter\n2006-07 (http://see.stanford.edu/materials/lsocoee364b/17-bb_notes.pdf).\n[27] O. Briant and D. Naddef. The optimal diversity management problem. Operations research, 52(4),\n2004.\n[28] O. Briant, C. Lemaréchal,K. Monneris,N. Perrot,C. Tadonki,F. Vanderbeck,J.-P. Vial,C. Beltran,P.\nMeurdesoif, Comparison of various approaches for column generation, Eigth Aussois Workshop on\nCombinatorial Optimization, 5-9 january 2004.\n[29] A. Buttari, J. Langou, J. Kurzak, and J. Dongarra, A class of parallel tiled linear algebra algorithms\nfor multicore architectures, Parallel Computing 35: 38-53, 2009.\n[30] R. H. Byrd, et al., Parallel global optimization for molecular conﬁguration problem, in Proceedings\nof the 6th SIAm Conference on Parallel Processing for ScientiﬁComputation, SIAM, Philadelphia,\n1993.\n[31] R. H. Byrd, et al., Parallel global optimization: numerical methods, dynamic scheduling methods,\nand application to molecular conﬁguration, in B. Ford and A. Fincham (Eds), Parallel Computation,\nOxford University Press, Oxford, pp. 187-207, 1993 .\n[32] CPLEX, http://www.ilog.com/products/cplex/\n[33] D. Carlson, A. Haurie, J.-P. Vial, and D.S. Zachary. Large scale convex optimization methods for\nair quality policy assessment. Automatica, 40:385–395, 2004.\n[34] T. J. Chang, N. Meade, J. E. Beasley, Y. M. Sharaiha, Heuristics for cardinality constrained portfolio\noptimization, Computers & Operation Research, 27, pp. 1271-1302, 2000.\n[35] V. Chvátal, Hard Knapsack Problems, Operations Research, Vol. 28(6),pp. 1402-1411,1980.\n[36] V. Chvatal, Linear Programming, W. H. Freeman Compagny, Series of Books in the Mathematical\nSciences, 1983.\n[37] A. Clementi, J. D. P. Rolim, and E. Urland, Randomized Parallel Algorithms, LLNCS, A. Ferreira\nand P. Pardalos (Eds), pp. 25-48, 1995.\n[38] Constantinides G. M. and Malliaris A.G., Portfolio theory, Finance ed R.A. Jarrow, V. Maksimovic\nand W. T. Ziemba, Elsevier-Amsterdam, 1-30, 1995.\n[39] W. J. Cook, W. H. Cunningham, W. R. Pulleyblank, A. Schrijver, Combinatorial Optimization, John\nWiley & Sons, 1998.\n13\n[40] T. Crainic, B. Le Cun, and C. Roucairol, Parallel Branch-and-Bound Algorithms, In: Talbi, E. (ed.)\nParallel Combinatorial Optimization, ch. 1, Wiley, Chichester 2006.\n[41] M. D’Apuzzo and M. Marino, Parallel computation issued of an interior point method for solving\nlarge bound-constrained quadratic programming problems, Parallel Computing, 29, 2003.\n[42] M. D’Apuzzo, et al., A parallel implementation of a potential reduction algorithm for box-constrained\nquadratic programming, in LLNCS pp. 839-848, Europar2000, Spriger-Verlag, Berlin, 2000.\n[43] M. D’Apuzzo, et al., Nonlinear optimization: a parallel linear algebra standpoint, Handbook of Par-\nallel Computing and Statistics, E. J. Kontoghiorges (Ed.), New-York, 2003.\n[44] M. D’Apuzzo, et al.,Parallel computing in bound constrained quadratic programming, Ann. Univ.\nFerrara-Sez VII-Sc. Mat. Supplemento al XLV pp. 479-491, 2000.\n[45] A. De Bruin, G. A. P. Kindervater, H. W. J. M. Trienekens , Towards and abstract parallel branch\nand bound mahcine, LLNCS, A. Ferreira and P. Pardalos (Eds), pp. 145-170, 1995.\n[46] Z. Degrave and M. Peeters, Benchmark Results for the Cutting Stock and Bin Packing Problem,\nResearch Report No 9820 of the Quantitative Methods Group, Louvain, Belgique, 1998.\n[47] L. Drouet, C. Beltran, N.R. Edwards, A. Haurie, J.-P. Vial, and D.S. Zachary. An oracle method to\ncouple climate and economic dynamics. In A. Haurie and L. Viguier, editors, Coupling climate and\neconomic dynamics. Kluwer (to appear), 2005.\n[48] L. Drouet, N.R. Edwards, and A. Haurie. Coupling climate and economic models in a cost-beneﬁt\nframework: A convex optimization approach. Environmental Modeling and Assessment, to appear\nin 2005.\n[49] I. S. Duﬀ, H. A. VanDer Vorst, Developments and trends in the parallel solution of linear systems,\nParallel Computing 25, pp. 13-14, 1999.\n[50] C. Durazzi and V. Ruggiero, Numerical solution of special linear and quadratic programs via a parallel\ninterior-point method, Parallel Computing, 29, 2003.\n[51] O. Du Merle and J.-P. Vial. Proximal ACCPM, a cutting plane method for column generation and\nLagrangian relaxation: application to the p-median problem. Technical report, Logilab, University\nof Geneva, 40 Bd du Pont d’Arve, CH-1211 Geneva, Switzerland, 2002.\n[52] Fengguang, S., Tomov, S., Dongarra, J., Eﬃcient Support for Matrix Computations on Heteroge-\nneous Multi-core and Multi-GPU Architectures, University of Tennessee Computer Science Technical\nReport, UT-CS-11-668, June 16, 2011.\n[53] A. Ferreira and P. M. Pardalos (Eds.), Solving Combinatorial Optimization Problems in Parallel,\nLLNCS-Springer 1054, 1995 .\n[54] A. Ferreira and P. M. Pardalos, Parallel Processing of Discrete Optimization Problems, DIMACS\nSeries Vol. 22, American Mathematical Society, 1995.\n[55] M. C. Ferris and T.S. Munson, Interior Point Methods for Massive Support Vector Machines,\nCours/SÈminaire du 3e cycle romand de recherche opÈrationnelle, Zinal, Switzerland, march 2001.\n[56] M. C. Ferris, J. D. Horn, Partitioning mathematical programs for parallel solution, Mathematical\nProgramming 80, PP. 35-61, 1998.\n[57] Ferris, M., GAMS: Condor and the grid: Solving hard optimization problems in parallel, Industrial\nand Systems Engineering, Lehigh University, 2006.\n[58] L. G. Fishbone and H. Abilock. MARKAL, a linear programming model for energy systems analysis:\nTechnical description of the BNL version. International Journal of Energy Research, 5:353–375, 1981.\n[59] E. Fragnière and A. Haurie. A stochastic programming model for energy/environment choices under\nuncertainty. International Journal of Environment and Pollution, 6(4-6):587–603, 1996.\n14\n[60] E. Fragnière and A. Haurie. MARKAL-Geneva: A model to assess energy-environment choices for\na Swiss Canton.\nIn C. Carraro and A. Haurie, editors, Operations Research and Environmental\nManagement, volume 5 of The FEEM/KLUWER International Series on Economics, Energy and\nEnvironment. Kluwer Academic Publishers, 1996.\n[61] M. R. Garey and D. S. Johnson, Computers and Intractability: A Guide to the Theory of NP-\nCompleteness, Morgan Kaufmann, 1979.\n[62] C. Gatu and E. J. Kontoghiorghes, Parallel aalgorithms for computing all possible subset regression\nmodels using the QR decomposition, Parallel Computing, 29, 2003.\n[63] M. Gengler, An introduction to parallel dynamic programming, LLNCS, A. Ferreira and P. Pardalos\n(Eds), pp. 86-114, 1995.\n[64] A. M. Geoﬀrion. Lagrangean relaxation for integer programming. Mathematical Programming Study,\n2:82–114, 1974.\n[65] J.-L. Goﬃn and J.-P. Vial. Convex nondiﬀerentiable optimization: A survey focussed on the analytic\ncenter cutting plane method. Optimization Methods and Software, 174:805–867, 2002.\n[66] J.-L. Goﬃn and J.-P. Vial. Shallow, deep and very deep cuts in the analytic center cutting plane\nmethod. Mathematical Programming, 84:89–103, 1999.\n[67] J. L. Goﬃn, A. Haurie, and J. P. Vial, Decomposition and nondiﬀerentiable optimization with the\nprojective algorithm Management Science, 37, 284-302.\n[68] J.-L. Goﬃn, Z. Q. Luo, and Y. Ye. Complexity analysis of an interior point cutting plane method\nfor convex feasibility problems. SIAM Journal on Optimization, 69:638–652, 1996.\n[69] J. Gondzio, A. Grothey, Parallel Interior Point Solver for Structured Quadratic Programs: Appli-\ncation to Financial Planning Problems, RR MS-03-001, School of Mathematics, University of Edin-\nburgh, 2003.\n[70] J. Gondzio, O. du Merle, R. Sarkissian and J.P. Vial, ACCPM - A Library for Convex Optimization\nBased on an Analytic Center Cutting Plane Method, European Journal of Operational Research, 94,\n206-211, 1996.\n[71] A. Grama and V. Kumar, State-of-the-Art in Parallel Search Techniques for Discretes Optimization\nProblems, Personnal communication, 1993 .\n[72] M. Guignard and S. Kim. Lagrangean decomposition: a model yielding stronger Lagrangean bounds.\nMathematical Programming, 39:215–228, 1987.\n[73] A. Gupta, G. Karypis, V. Kumar, A highly scalable parallel algorithm for sparse matrix factorization,\nIEEE TPDS 8(5), pp. 502-520.\n[74] N. H. Hakansson, Multi-period mean-variance analysis: Toward a theory of portfolio choice, Journal\nof Finance, 26, 857-884, 1971.\n[75] J. Han and M. Kamber, Data Mining: Concept and Techniques, Morgan Kaufmann Publishers, 2000.\n[76] P. Hansen, N. Mladenovic, and D. Perez-Brito. Variable neighborhood decomposition search. Journal\nof Heuristics, 7:335–350, 2001.\n[77] A. Haurie, J. Kübler, A. Clappier, and H. Van den Bergh. A metamodeling approach for integrated\nassessment of air quality policies. Environmental Modeling and Assessment, 9:1–122, 2004.\n[78] J. L. Houle, W. Cadigan, S. Henry, A. Pinnamanenib, S. Lundahlc, Database Mining in the Human\nGenome Initiative, http://www.biodatabases.com/whitepaper01.html\n[79] B. Hunsaker, C. A. Tovey, Simple lifted cover inequalities and hard knapsack problems, Discrete\nOptimization 2(3), pp. 219-228, 2005.\n15\n[80] N. J. Jobst, M. D. Horniman, C. A. Lucas, and G. Mitra, Computational aspects of alternative\nportfolio selection models in the presence of discrete asset choice constraints, Quantitative ﬁnance,\nVol. 1, p. 1-13, 2001.\n[81] L. Jooyounga, et al., Eﬃcient parallel algorithms in global optimization of potential energy functions\nfor peptides, proteins, and crystals, Computer Physics Communications 128 pp. 3999-411, 2000.\n[82] Judea Pearl, Heuristics-Intelligent Search Strategies for Computer Problem Solving, Addison-Wesley,\nReading, MA, 1984.\n[83] O. Kariv and L. Hakimi. An algorithmic approach to network location problems. ii: the p-medians.\nSIAM Journal of Applied Mathematics, 37(3):539–560, 1979.\n[84] J. Kepner, MatlabMPI, http://www.ll.mit.edu/MatlabMPI/\n[85] M. Kleinberg, C.H. Papadimitriou, and P. Raghavan, Segmentation Problems, ACM Symposium on\nTheory of Computing, 1998, pp. 473-482.\n[86] E. K. Lee and J. E. Mitchell , Computational experience of an interior-point SQP algorithm in a\nparallel branch-and-bound franmework, Proc. High Perf. Opt. Tech., 1997.\n[87] C. Lemaréchal, Lagrangian relaxation, M. Junger and D. Naddef (Eds.): Computat. Comb. Opti-\nmization, LNCS 2241, pp. 112?156, 2001.\nhttp://link.springer.com/content/pdf/10.1007%2F3-540-45586-8_4\n[88] C. Lemaréchal. Nondiﬀerentiable optimization. In G.L. Nemhauser, A.H.G Rinnooy Kan, and M.J.\nTodd, editors, Handbooks in Operations Research and Management Science, volume 1, pages 529–572.\nNorth-Holland, 1989.\n[89] R. M. Lewis and V. J. Torczon, Pattern search methods for linearly constrained minimization, SIAM\nJournal of Optimization, 10, pp. 971-941, 2000.\n[90] D. Li and W. L. Ng, Optimal dynamic portfolio selection: Multi-period mean-variance formulation,\nMath. Finance 10, 387-406, 2000.\n[91] MOSEK, http://www.mosek.com/.\n[92] O. L. Mangasarian, R. Setino, and W. Wolberg, Pattern Recognition via linear programming: Theory\nand Applications to Medical Diagnosis, 1990.\n[93] O. L. Mangasarian, W.N. Street, and W.H. Wolberg, Breast Cancer Diagnosis and prognosis via\nlinear programming, Operation research, Vol. 43, No. 4, July-August 1995, pp. 570-577.\n[94] O. L. Mangasarian, Linear and Non-linear Separation of Patterns by linear programming, Operations\nResearch, 13, pp. 444-452.\n[95] R. Mansini and M. G. Speranza, Heuristic algorithms for the portfolio selection poblem with minimum\ntransaction lots, Eur. Jour. Op. Res., 114, 219-223, 1999.\n[96] H. Markowitz, Portfolio Selection: Eﬃcient Diversiﬁcation of Investment, John Wiley & Sons, New-\nYork, 1959.\n[97] H. Markowitz, Portfolio Selection, The Journal of Finance 1, 77-91, 1952.\n[98] A. Migdalas, G. Toraldo, and V. Kumar, Nonlinear optimization and parallel computing, Parallel\nComputing 29, pp. 375-391, 2003.\n[99] J. Mossin, Optimal multiperiod portfolio policies, J. Business, 41, 215-229, 1968.\n[100] Y. Nesterov and A. Nemirovsky. Interior Point Polynomial Algorithms in Convex Programming:\nTheory and Applications. SIAM, Philadelphia, Penn., 1994.\n[101] Y. Nesterov and J.-P. Vial. Homogeneous analytic center cutting plane methods for convex problems\nand variational inequalities. SIAM Journal on Optimization, 9:707–728, 1999.\n16\n[102] Y. Nesterov. Complexity estimates of some cutting plane methods based on the analytic center.\nMathematical Programming, 69:149–176, 1995.\n[103] Y. Nesterov. Introductory Lectures on Convex Optimization, a Basic Course, volume 87 of Applied\nOptimization. Kluwer Academic Publishers, 2004.\n[104] P.S. Pacheco, Parallel Programming with MPI, Morgan Kaufmann, 1997.\n[105] C. H. Papadimitriou and K. Steiglitz, Combinatorial optimization:\nalgorithms and complexity,\nPrentice-Hall 1982.\n[106] P. M. Pardalos, A. T. Phillips, and J. B. Rosen, Topics in Parallel Computing in Mathematical\nProgramming, Science Press, 1993.\n[107] P. M. Pardalos, M. G. C. Resende, and K. G. Ramakrishinan (eds), Parallel Processing of Discrete\nOptimization Problems, DIMACS Series Vol. 22, American Mathematical Society, 1995.\n[108] Paul A. Jensen and Jonathan F. Bard, Operations Research - Models and Methods, John Wiley and\nSons , 2003.\n[109] Per. S. Lauren, Parallel heuristic search - Introduction and new approach, LLNCS, A. Ferreira and\nP. Pardalos (Eds), pp. 248-274, 1995.\n[110] G. Reinelt.\nTsplib, 2001.\nhttp://www.iwr.uni-heidelberg.de / groups / comopt / software /\nTSPLIB95.\n[111] P. A. Samuelson, Lifetime portfolio selection by dynamic stochastic programming, Rev. Econ. Stat.\n51, 239-246, 1969.\n[112] R. B. Schnabel, A view of the limitation, opportunities, and challenges in parallel nonlinear opti-\nmization, Parallel Computing 21(6), pp. 875-905, 1995.\n[113] Scott Hamilton and Lee Garber, Deep Blue’s hardware-software synergy, IEEE Computer, 30(10),\npp. 29-35, 1997.\n[114] Y. Shinano, T. Fujie, ParaLEX: A Parallel Extension for the CPLEX Mixed Integer Optimizer, Re-\ncent Advances in Parallel Virtual Machine and Message Passing Interface Lecture Notes in Computer\nScience Volume 4757, pp 97-106, 2007.\n[115] S. L. Smith, R. B. Schnabel, Centralized and distributed dynamic scheduling for adaptative paral-\nlel algorithms, in P. Mehrotra, J. Saltz, R. Voight (Eds), Unstructured Scientiﬁc Computation on\nScalable Multiprocessors, MIT Press, pp. 301-321, 1992.\n[116] C. Tadonki, Scalability on Manycore Machines\nhttps://www.cri.ensmp.fr/people/tadonki/talks/Scalability.pdf\n[117] C. Tadonki, High Performance Computing as Combination of Machines and Methods and Program-\nming\nHDR Thesis, Université Paris Sud-Paris XI, 2013.\n[118] C. Tadonki and J.-P. Vial, Eﬃcient algorithm for linear pattern separation, (to appear in) Interna-\ntional Conference on Computational Science, ICCS04 (LNCS/Springer), Krakow, Poland, June 2004\n.\n[119] C. Tadonki, C. Beltran and J.-P. Vial , Portfolio management with integrality constraints, Computa-\ntional Management Science Conference and Workshop on Computational Econometrics and Statistics,\nLink, Neuchatel, Switzerland, April 2004 .\n[120] C. Tadonki, J.-P. Vial, Eﬃcient Algorithm for Linear Pattern Separation, International Conference\non Computational Science, ICCS04 (LNCS/Springer), Krakow, Poland, June 2004.\n[121] C. Tadonki, A Recursive Method for Graph Scheduling, International Symposium on Parallel and\nDistributed Computing (SPDC), Iasi, Romania, July 2002\n17\n[122] C. Tadonki, Parallel Cholesky Factorization, Workshop on Parallel Matrix Algorithm and Applica-\ntions (PMAA), Neuchatel, Switzerland, August 2000.\n[123] E.-G. Talbi (Editor), Parallel Combinatorial Optimization,Wiley Series on Parallel and Distributed\nComputing,2006.\n[124] S. Tomov R. Nath P. Du J. Dongarra, MAGMA: Matrix Algebra on GPU and Multicore Architec-\ntures, http://icl.cs.utk.edu/magma, 2012.\n[125] R. A. Van de Geijn, Using PLAPACK, The MIT Press, 1997.\n[126] Vance et. al. , Using Branch-and-Price-and-Cut to solve Origin-Destination Integer Multi-commodity\nFlow problems,Operation Research, Vol 48(2), 2000.\n[127] P. H. Vance, A. Atamturk, C. Barnhart, E. Gelman, and E. L. Johnson, A. Krishna, D. Mahidhara,\nG. L. Nemhauser, and R. Rebello, A Heuristic Branch-and-Price Approach for the Airline Crew\nPairing Problem, 1997.\n[128] M.S. Viveros, J.P. Nearhos, M.J. Rothman, Applying Data Mining Techniques to a Health Insurance\nInformation System, 22nd VLDB Conference, Mumbai(Bombay), India,1996, pp. 286-294.\n[129] B. W. Wah, G.-J. Li, and C. F. Yu, Multiprocessing of combinatorial search problems, IEEE Com-\nputer, June 1985.\n[130] R. C. Whaley, et al., Automated empirical optimizations of software and the ATLAS project, Parallel\nComputing 27, pp. 3-35, 2001.\n[131] M. R. Young, A minimax portfolio selection rule with linear programming solution, Management\nScience 44, 673-683, 1992.\n[132] G. Zanghirati and L. Zanni, A parallel solver for large quadratic programs in training support vector\nmachines, Parallel Computing, 29, 2003.\n[133] T. Zariphoulou, Investment-consumption models with transactions costs and Marko chain parame-\nters, SIAM J. Control Optim 30, 613-636, 1992.\n[134] X. Y. Zhou and D. Li, Continuous-Time mean-variance portfolio selection: A stochastic LQ frame-\nwork, Appl. Math. Optim. 42, 19-33, 2000.\n[135] CPLEX, http://www.ilog.com/products/cplex\n[136] GUROBI, https://www.gurobi.com\n[137] LOGO, http://www.orfe.princeton.edu/ loqo/\n[138] MOSEK, http://www.mosek.com/\n18\n"
  }
]