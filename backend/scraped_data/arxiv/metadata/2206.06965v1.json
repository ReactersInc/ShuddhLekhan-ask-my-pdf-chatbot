{
  "arxiv_id": "2206.06965v1",
  "title": "Deep Reinforcement Learning for Exact Combinatorial Optimization:\n  Learning to Branch",
  "abstract": "Branch-and-bound is a systematic enumerative method for combinatorial\noptimization, where the performance highly relies on the variable selection\nstrategy. State-of-the-art handcrafted heuristic strategies suffer from\nrelatively slow inference time for each selection, while the current machine\nlearning methods require a significant amount of labeled data. We propose a new\napproach for solving the data labeling and inference latency issues in\ncombinatorial optimization based on the use of the reinforcement learning (RL)\nparadigm. We use imitation learning to bootstrap an RL agent and then use\nProximal Policy Optimization (PPO) to further explore global optimal actions.\nThen, a value network is used to run Monte-Carlo tree search (MCTS) to enhance\nthe policy network. We evaluate the performance of our method on four different\ncategories of combinatorial optimization problems and show that our approach\nperforms strongly compared to the state-of-the-art machine learning and\nheuristics based methods.",
  "authors": [
    "Tianyu Zhang",
    "Amin Banitalebi-Dehkordi",
    "Yong Zhang"
  ],
  "published": "2022-06-14T16:35:58Z",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO",
    "math.OC"
  ],
  "pdf_url": "http://arxiv.org/pdf/2206.06965v1",
  "url": "http://arxiv.org/abs/2206.06965v1",
  "download_date": "2025-08-16T05:36:29.003074",
  "pdf_path": "./scraped_data\\arxiv\\pdfs\\2206.06965v1.pdf",
  "file_size": 406718,
  "relevance_score": 103
}